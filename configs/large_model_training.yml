# Large Model Training Configuration
# ==================================
# Configuration for training large Hyena-GLT models
# with distributed training support

model:
  model_type: hyena_glt
  model_size: large
  vocab_size: 16384
  hidden_size: 1024
  num_layers: 24
  max_position_embeddings: 32768
  sequence_type: dna
  kmer_size: 8

training:
  data_path: processed_data/large_scale/
  output_dir: training_output/large_model/
  epochs: 5
  batch_size: 8  # Smaller batch for large model
  learning_rate: 5.0e-5
  max_length: 4096
  warmup_steps: 2000
  save_steps: 1000
  eval_steps: 500
  gradient_accumulation_steps: 4

data:
  train_file: processed_data/large_scale/train.hdf5
  val_file: processed_data/large_scale/val.hdf5
  tokenizer_path: processed_data/large_scale/tokenizer.json

optimization:
  optimizer: adamw
  weight_decay: 0.01
  gradient_clipping: 1.0
  mixed_precision: true
  use_fsdp: true  # For distributed training

system:
  device: auto
  num_workers: 8
  gradient_checkpointing: true
