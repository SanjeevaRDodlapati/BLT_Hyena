{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124a0586",
   "metadata": {},
   "source": [
    "# Genomic Tokenization and Data Preprocessing\n",
    "\n",
    "This notebook provides a comprehensive guide to genomic data preprocessing and tokenization with Hyena-GLT. Learn how to handle different sequence types, implement efficient tokenization strategies, and prepare data for downstream modeling tasks.\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Master different tokenization strategies for genomic sequences\n",
    "- Understand sequence encoding and decoding processes\n",
    "- Learn advanced preprocessing techniques\n",
    "- Implement custom tokenizers for specific genomic tasks\n",
    "- Handle various sequence formats and data sources\n",
    "- Optimize tokenization for performance\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "- Complete the Introduction notebook (`01_introduction_to_hyena_glt.ipynb`)\n",
    "- Basic understanding of genomic sequences\n",
    "- Familiarity with Python and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c54eaf",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599e0180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Hyena-GLT components\n",
    "from hyena_glt.tokenizers import (\n",
    "    DNATokenizer, RNATokenizer, ProteinTokenizer,\n",
    "    CodonTokenizer, KmerTokenizer\n",
    ")\n",
    "from hyena_glt.data import GenomicDataset, SequenceCollator\n",
    "from hyena_glt.utils import get_sequence_stats, analyze_composition\n",
    "\n",
    "# Example utilities\n",
    "from examples.utils.data_utils import (\n",
    "    generate_synthetic_genomic_data,\n",
    "    load_fasta_sequences,\n",
    "    create_classification_dataset\n",
    ")\n",
    "from examples.utils.visualization_utils import (\n",
    "    plot_tokenization_comparison,\n",
    "    plot_sequence_length_distribution,\n",
    "    plot_composition_analysis\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"ðŸ§¬ Genomic tokenization setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8f835b",
   "metadata": {},
   "source": [
    "## 2. Understanding Tokenization Strategies\n",
    "\n",
    "Genomic sequences can be tokenized in multiple ways. Let's explore different strategies and their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14deb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DNA sequence for demonstration\n",
    "sample_dna = \"ATCGATCGTAGCTAGCTAGCGATCGATCGTAGCTAGCATGAAATTTGGGCCCAAATTTCCCGGGATCGATCG\"\n",
    "\n",
    "print(f\"Sample DNA sequence ({len(sample_dna)} bp):\")\n",
    "print(sample_dna)\n",
    "print()\n",
    "\n",
    "# 1. Character-level tokenization (k=1)\n",
    "char_tokenizer = DNATokenizer(k=1)\n",
    "char_tokens = char_tokenizer.encode(sample_dna)\n",
    "print(f\"Character tokens (k=1): {char_tokens[:20]}... (total: {len(char_tokens)})\")\n",
    "print(f\"Vocabulary size: {char_tokenizer.vocab_size}\")\n",
    "\n",
    "# 2. K-mer tokenization (k=3)\n",
    "kmer_tokenizer = DNATokenizer(k=3)\n",
    "kmer_tokens = kmer_tokenizer.encode(sample_dna)\n",
    "print(f\"\\nK-mer tokens (k=3): {kmer_tokens[:20]}... (total: {len(kmer_tokens)})\")\n",
    "print(f\"Vocabulary size: {kmer_tokenizer.vocab_size}\")\n",
    "\n",
    "# 3. Overlapping vs non-overlapping\n",
    "overlap_tokenizer = DNATokenizer(k=3, overlap=True)\n",
    "overlap_tokens = overlap_tokenizer.encode(sample_dna)\n",
    "print(f\"\\nOverlapping k-mers: {overlap_tokens[:20]}... (total: {len(overlap_tokens)})\")\n",
    "\n",
    "non_overlap_tokenizer = DNATokenizer(k=3, overlap=False)\n",
    "non_overlap_tokens = non_overlap_tokenizer.encode(sample_dna)\n",
    "print(f\"Non-overlapping k-mers: {non_overlap_tokens[:20]}... (total: {len(non_overlap_tokens)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97bf1ca",
   "metadata": {},
   "source": [
    "### Visualizing Tokenization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different tokenization strategies\n",
    "strategies = {\n",
    "    'Character (k=1)': char_tokens,\n",
    "    'K-mer (k=3)': kmer_tokens,\n",
    "    'Overlapping k=3': overlap_tokens,\n",
    "    'Non-overlapping k=3': non_overlap_tokens\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plot_tokenization_comparison(sample_dna, strategies)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16857b0d",
   "metadata": {},
   "source": [
    "## 3. Specialized Tokenizers for Different Sequence Types\n",
    "\n",
    "Different genomic sequences require different tokenization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92516c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sequences for each type\n",
    "dna_seq = \"ATGAAATTTGGGCCCAAATTTCCCGGGATCGATCGTAGCTAGC\"\n",
    "rna_seq = \"AUGAAAUUUGGGCCCAAAUUUCCCGGGAUCGAUCGUAGCUAGC\"\n",
    "protein_seq = \"MKFGPKFPGIDRSRR\"\n",
    "\n",
    "print(\"Sample sequences:\")\n",
    "print(f\"DNA:     {dna_seq}\")\n",
    "print(f\"RNA:     {rna_seq}\")\n",
    "print(f\"Protein: {protein_seq}\")\n",
    "print()\n",
    "\n",
    "# Initialize different tokenizers\n",
    "tokenizers = {\n",
    "    'DNA': DNATokenizer(k=3),\n",
    "    'RNA': RNATokenizer(k=3),\n",
    "    'Protein': ProteinTokenizer(),\n",
    "    'Codon': CodonTokenizer(),\n",
    "    'K-mer': KmerTokenizer(k=6)\n",
    "}\n",
    "\n",
    "# Test tokenization\n",
    "sequences = {\n",
    "    'DNA': dna_seq,\n",
    "    'RNA': rna_seq,\n",
    "    'Protein': protein_seq,\n",
    "    'Codon': dna_seq,  # Can be used for DNA coding sequences\n",
    "    'K-mer': dna_seq\n",
    "}\n",
    "\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    if name in sequences:\n",
    "        seq = sequences[name]\n",
    "        try:\n",
    "            tokens = tokenizer.encode(seq)\n",
    "            print(f\"{name} tokenizer:\")\n",
    "            print(f\"  Input:  {seq}\")\n",
    "            print(f\"  Tokens: {tokens[:10]}... (total: {len(tokens)})\")\n",
    "            print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "            \n",
    "            # Test decoding\n",
    "            decoded = tokenizer.decode(tokens)\n",
    "            print(f\"  Decoded: {decoded}\")\n",
    "            print(f\"  Match: {decoded == seq}\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"{name} tokenizer error: {e}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de8cd2c",
   "metadata": {},
   "source": [
    "## 4. Advanced Tokenization Features\n",
    "\n",
    "Learn about special tokens, padding, truncation, and other advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7007ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer with special tokens\n",
    "tokenizer = DNATokenizer(\n",
    "    k=3,\n",
    "    add_special_tokens=True,\n",
    "    max_length=100,\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(\"Tokenizer with special tokens:\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"UNK token ID: {tokenizer.unk_token_id}\")\n",
    "print(f\"CLS token ID: {tokenizer.cls_token_id}\")\n",
    "print(f\"SEP token ID: {tokenizer.sep_token_id}\")\n",
    "print()\n",
    "\n",
    "# Test with sequences of different lengths\n",
    "test_sequences = [\n",
    "    \"ATCG\",  # Very short\n",
    "    \"ATCGATCGTAGCTAGCTAGCGATCGATCG\",  # Medium\n",
    "    \"ATCGATCG\" * 20,  # Long (will be truncated)\n",
    "    \"ATCGXYZ\"  # Contains unknown characters\n",
    "]\n",
    "\n",
    "for i, seq in enumerate(test_sequences):\n",
    "    print(f\"Test sequence {i+1} (length: {len(seq)}):\")\n",
    "    print(f\"  Input: {seq[:50]}{'...' if len(seq) > 50 else ''}\")\n",
    "    \n",
    "    # Encode with different options\n",
    "    tokens = tokenizer.encode(seq)\n",
    "    print(f\"  Tokens: {tokens[:15]}{'...' if len(tokens) > 15 else ''}\")\n",
    "    print(f\"  Token count: {len(tokens)}\")\n",
    "    \n",
    "    # Get attention mask\n",
    "    attention_mask = tokenizer.get_attention_mask(tokens)\n",
    "    print(f\"  Attention mask: {attention_mask[:15]}{'...' if len(attention_mask) > 15 else ''}\")\n",
    "    print(f\"  Valid tokens: {sum(attention_mask)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e4ebb",
   "metadata": {},
   "source": [
    "## 5. Batch Processing and Data Loading\n",
    "\n",
    "Learn how to efficiently process multiple sequences and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1ae81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset of sequences\n",
    "np.random.seed(42)\n",
    "sequences, labels = generate_synthetic_genomic_data(\n",
    "    n_samples=1000,\n",
    "    sequence_type='dna',\n",
    "    min_length=100,\n",
    "    max_length=500,\n",
    "    num_classes=5\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(sequences)} sequences\")\n",
    "print(f\"Sequence length range: {min(len(s) for s in sequences)} - {max(len(s) for s in sequences)}\")\n",
    "print(f\"Label distribution: {np.bincount(labels)}\")\n",
    "print()\n",
    "\n",
    "# Analyze sequence properties\n",
    "lengths = [len(seq) for seq in sequences]\n",
    "gc_contents = [(seq.count('G') + seq.count('C')) / len(seq) for seq in sequences]\n",
    "\n",
    "# Plot sequence statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Length distribution\n",
    "axes[0, 0].hist(lengths, bins=30, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_xlabel('Sequence Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Sequence Length Distribution')\n",
    "\n",
    "# GC content distribution\n",
    "axes[0, 1].hist(gc_contents, bins=30, alpha=0.7, color='lightcoral')\n",
    "axes[0, 1].set_xlabel('GC Content')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('GC Content Distribution')\n",
    "\n",
    "# Label distribution\n",
    "label_counts = np.bincount(labels)\n",
    "axes[1, 0].bar(range(len(label_counts)), label_counts, color='lightgreen')\n",
    "axes[1, 0].set_xlabel('Label')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Label Distribution')\n",
    "\n",
    "# Length vs GC content\n",
    "scatter = axes[1, 1].scatter(lengths, gc_contents, c=labels, alpha=0.6, cmap='viridis')\n",
    "axes[1, 1].set_xlabel('Sequence Length')\n",
    "axes[1, 1].set_ylabel('GC Content')\n",
    "axes[1, 1].set_title('Length vs GC Content (colored by label)')\n",
    "plt.colorbar(scatter, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35709d69",
   "metadata": {},
   "source": [
    "### Creating Genomic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer for dataset creation\n",
    "tokenizer = DNATokenizer(k=3, max_length=512, padding=True, truncation=True)\n",
    "\n",
    "# Create genomic dataset\n",
    "dataset = GenomicDataset(\n",
    "    sequences=sequences,\n",
    "    labels=labels,\n",
    "    tokenizer=tokenizer,\n",
    "    sequence_type='dna',\n",
    "    task_type='classification'\n",
    ")\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Sample data structure:\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "print()\n",
    "\n",
    "# Split dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits:\")\n",
    "print(f\"  Training: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deacf348",
   "metadata": {},
   "source": [
    "### Efficient Data Loading with Collators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a388c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence collator for efficient batching\n",
    "collator = SequenceCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    padding=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print()\n",
    "\n",
    "# Examine a batch\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batch structure:\")\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
    "        if key == 'input_ids':\n",
    "            print(f\"    Range: {value.min().item()} - {value.max().item()}\")\n",
    "        elif key == 'labels':\n",
    "            print(f\"    Unique labels: {torch.unique(value).tolist()}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value)}\")\n",
    "print()\n",
    "\n",
    "# Check attention masks\n",
    "attention_mask = batch['attention_mask']\n",
    "valid_tokens_per_sample = attention_mask.sum(dim=1)\n",
    "print(f\"Valid tokens per sample in batch:\")\n",
    "print(f\"  Min: {valid_tokens_per_sample.min().item()}\")\n",
    "print(f\"  Max: {valid_tokens_per_sample.max().item()}\")\n",
    "print(f\"  Mean: {valid_tokens_per_sample.float().mean().item():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2beb164",
   "metadata": {},
   "source": [
    "## 6. Handling Different Data Formats\n",
    "\n",
    "Learn how to work with common genomic file formats like FASTA, FASTQ, and VCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample FASTA-like data\n",
    "fasta_data = [\n",
    "    (\">seq1|gene_A|chromosome_1\", \"ATGAAATTTGGGCCCAAATTTCCCGGGATCGATCGTAGCTAGC\"),\n",
    "    (\">seq2|gene_B|chromosome_2\", \"CGATCGATCGTAGCTAGCTAGCGATCGATCGTAGCTAGCATG\"),\n",
    "    (\">seq3|gene_C|chromosome_3\", \"TAGCTAGCGATCGATCGTAGCTAGCATGAAATTTGGGCCCAAA\"),\n",
    "    (\">seq4|gene_D|chromosome_4\", \"AAATTTCCCGGGATCGATCGTAGCTAGCGATCGATCGTAGC\")\n",
    "]\n",
    "\n",
    "print(\"Sample FASTA-like data:\")\n",
    "for header, sequence in fasta_data:\n",
    "    print(f\"{header}\")\n",
    "    print(f\"{sequence}\")\n",
    "    print()\n",
    "\n",
    "# Parse and process FASTA data\n",
    "def parse_fasta_header(header):\n",
    "    \"\"\"Parse FASTA header to extract metadata.\"\"\"\n",
    "    parts = header.strip('> ').split('|')\n",
    "    return {\n",
    "        'seq_id': parts[0] if len(parts) > 0 else 'unknown',\n",
    "        'gene': parts[1] if len(parts) > 1 else 'unknown',\n",
    "        'chromosome': parts[2] if len(parts) > 2 else 'unknown'\n",
    "    }\n",
    "\n",
    "# Process FASTA data\n",
    "processed_data = []\n",
    "for header, sequence in fasta_data:\n",
    "    metadata = parse_fasta_header(header)\n",
    "    \n",
    "    # Calculate sequence statistics\n",
    "    stats = get_sequence_stats(sequence)\n",
    "    \n",
    "    processed_data.append({\n",
    "        'sequence': sequence,\n",
    "        'metadata': metadata,\n",
    "        'stats': stats\n",
    "    })\n",
    "\n",
    "# Display processed data\n",
    "print(\"Processed FASTA data:\")\n",
    "for i, data in enumerate(processed_data):\n",
    "    print(f\"Entry {i+1}:\")\n",
    "    print(f\"  Sequence: {data['sequence'][:30]}...\")\n",
    "    print(f\"  Metadata: {data['metadata']}\")\n",
    "    print(f\"  Stats: {data['stats']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae36fb",
   "metadata": {},
   "source": [
    "### Quality Score Handling (FASTQ-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87141814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate FASTQ-style data with quality scores\n",
    "fastq_data = [\n",
    "    {\n",
    "        'sequence': 'ATGAAATTTGGGCCCAAATTTCCCGGGATCGATCG',\n",
    "        'quality': '##############################!!!!!!',  # Phred+33 encoding\n",
    "        'header': '@read1'\n",
    "    },\n",
    "    {\n",
    "        'sequence': 'CGATCGATCGTAGCTAGCTAGCGATCGATCGTAG',\n",
    "        'quality': '!!!!!!##########!!!!!############',\n",
    "        'header': '@read2'\n",
    "    }\n",
    "]\n",
    "\n",
    "def parse_quality_scores(quality_string, encoding='phred33'):\n",
    "    \"\"\"Parse quality scores from FASTQ format.\"\"\"\n",
    "    if encoding == 'phred33':\n",
    "        return [ord(char) - 33 for char in quality_string]\n",
    "    elif encoding == 'phred64':\n",
    "        return [ord(char) - 64 for char in quality_string]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoding: {encoding}\")\n",
    "\n",
    "def filter_by_quality(sequence, quality_scores, min_quality=20, min_length=20):\n",
    "    \"\"\"Filter sequence based on quality scores.\"\"\"\n",
    "    # Filter positions with quality >= min_quality\n",
    "    high_quality_positions = [i for i, q in enumerate(quality_scores) if q >= min_quality]\n",
    "    \n",
    "    if len(high_quality_positions) < min_length:\n",
    "        return None, None  # Sequence too short after filtering\n",
    "    \n",
    "    # Extract high-quality subsequence\n",
    "    start_pos = min(high_quality_positions)\n",
    "    end_pos = max(high_quality_positions) + 1\n",
    "    \n",
    "    filtered_sequence = sequence[start_pos:end_pos]\n",
    "    filtered_quality = quality_scores[start_pos:end_pos]\n",
    "    \n",
    "    return filtered_sequence, filtered_quality\n",
    "\n",
    "# Process FASTQ data\n",
    "print(\"Processing FASTQ-style data:\")\n",
    "for data in fastq_data:\n",
    "    sequence = data['sequence']\n",
    "    quality = data['quality']\n",
    "    header = data['header']\n",
    "    \n",
    "    # Parse quality scores\n",
    "    quality_scores = parse_quality_scores(quality)\n",
    "    avg_quality = np.mean(quality_scores)\n",
    "    \n",
    "    # Filter by quality\n",
    "    filtered_seq, filtered_qual = filter_by_quality(sequence, quality_scores)\n",
    "    \n",
    "    print(f\"\\n{header}:\")\n",
    "    print(f\"  Original: {sequence}\")\n",
    "    print(f\"  Quality:  {quality}\")\n",
    "    print(f\"  Quality scores: {quality_scores}\")\n",
    "    print(f\"  Average quality: {avg_quality:.1f}\")\n",
    "    \n",
    "    if filtered_seq:\n",
    "        print(f\"  Filtered: {filtered_seq}\")\n",
    "        print(f\"  Filtered quality: {filtered_qual}\")\n",
    "    else:\n",
    "        print(f\"  Filtered: REMOVED (low quality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094cbf2b",
   "metadata": {},
   "source": [
    "## 7. Custom Tokenizer Development\n",
    "\n",
    "Learn how to create custom tokenizers for specialized genomic tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifAwareTokenizer:\n",
    "    \"\"\"Custom tokenizer that preserves important biological motifs.\"\"\"\n",
    "    \n",
    "    def __init__(self, motifs=None, k=3):\n",
    "        self.k = k\n",
    "        self.motifs = motifs or [\n",
    "            'TATAAA',  # TATA box\n",
    "            'CAAT',    # CAAT box\n",
    "            'GGCC',    # GC box\n",
    "            'ATG',     # Start codon\n",
    "            'TAA', 'TAG', 'TGA',  # Stop codons\n",
    "        ]\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self._build_vocabulary()\n",
    "    \n",
    "    def _build_vocabulary(self):\n",
    "        \"\"\"Build vocabulary including motifs and k-mers.\"\"\"\n",
    "        bases = ['A', 'T', 'C', 'G']\n",
    "        \n",
    "        # Start with special tokens\n",
    "        self.vocab = {\n",
    "            '[PAD]': 0,\n",
    "            '[UNK]': 1,\n",
    "            '[CLS]': 2,\n",
    "            '[SEP]': 3\n",
    "        }\n",
    "        \n",
    "        # Add motifs (higher priority)\n",
    "        for motif in self.motifs:\n",
    "            if motif not in self.vocab:\n",
    "                self.vocab[motif] = len(self.vocab)\n",
    "        \n",
    "        # Add regular k-mers\n",
    "        for i in range(4 ** self.k):\n",
    "            kmer = ''\n",
    "            val = i\n",
    "            for _ in range(self.k):\n",
    "                kmer = bases[val % 4] + kmer\n",
    "                val //= 4\n",
    "            \n",
    "            if kmer not in self.vocab:\n",
    "                self.vocab[kmer] = len(self.vocab)\n",
    "        \n",
    "        # Create reverse mapping\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    def _find_motifs(self, sequence):\n",
    "        \"\"\"Find motif positions in sequence.\"\"\"\n",
    "        motif_positions = []\n",
    "        for motif in self.motifs:\n",
    "            start = 0\n",
    "            while True:\n",
    "                pos = sequence.find(motif, start)\n",
    "                if pos == -1:\n",
    "                    break\n",
    "                motif_positions.append((pos, pos + len(motif), motif))\n",
    "                start = pos + 1\n",
    "        \n",
    "        # Sort by position\n",
    "        motif_positions.sort()\n",
    "        return motif_positions\n",
    "    \n",
    "    def encode(self, sequence):\n",
    "        \"\"\"Encode sequence preserving motifs.\"\"\"\n",
    "        tokens = []\n",
    "        sequence = sequence.upper()\n",
    "        \n",
    "        # Find motifs\n",
    "        motif_positions = self._find_motifs(sequence)\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(sequence):\n",
    "            # Check if current position starts a motif\n",
    "            motif_found = False\n",
    "            for start, end, motif in motif_positions:\n",
    "                if i == start:\n",
    "                    tokens.append(self.vocab[motif])\n",
    "                    i = end\n",
    "                    motif_found = True\n",
    "                    break\n",
    "            \n",
    "            if not motif_found:\n",
    "                # Regular k-mer tokenization\n",
    "                if i + self.k <= len(sequence):\n",
    "                    kmer = sequence[i:i+self.k]\n",
    "                    tokens.append(self.vocab.get(kmer, self.vocab['[UNK]']))\n",
    "                    i += self.k\n",
    "                else:\n",
    "                    # Handle remaining characters\n",
    "                    remaining = sequence[i:]\n",
    "                    tokens.append(self.vocab.get(remaining, self.vocab['[UNK]']))\n",
    "                    break\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Decode tokens back to sequence.\"\"\"\n",
    "        sequence = ''\n",
    "        for token_id in tokens:\n",
    "            if token_id in self.id_to_token:\n",
    "                token = self.id_to_token[token_id]\n",
    "                if token not in ['[PAD]', '[UNK]', '[CLS]', '[SEP]']:\n",
    "                    sequence += token\n",
    "        return sequence\n",
    "\n",
    "# Test custom tokenizer\n",
    "test_sequence = \"CGATCGTATAAATCGATCGATGATGAAATTTGGGCCCAAATAACGATCGTAGCTAGC\"\n",
    "print(f\"Test sequence: {test_sequence}\")\n",
    "print(f\"Length: {len(test_sequence)}\")\n",
    "print()\n",
    "\n",
    "# Initialize custom tokenizer\n",
    "motif_tokenizer = MotifAwareTokenizer(k=3)\n",
    "print(f\"Custom tokenizer vocabulary size: {motif_tokenizer.vocab_size}\")\n",
    "print(f\"Important motifs: {motif_tokenizer.motifs}\")\n",
    "print()\n",
    "\n",
    "# Encode with custom tokenizer\n",
    "custom_tokens = motif_tokenizer.encode(test_sequence)\n",
    "print(f\"Custom tokens: {custom_tokens}\")\n",
    "print(f\"Token count: {len(custom_tokens)}\")\n",
    "\n",
    "# Decode and verify\n",
    "decoded_sequence = motif_tokenizer.decode(custom_tokens)\n",
    "print(f\"Decoded: {decoded_sequence}\")\n",
    "print(f\"Match: {decoded_sequence == test_sequence}\")\n",
    "print()\n",
    "\n",
    "# Compare with regular tokenizer\n",
    "regular_tokenizer = DNATokenizer(k=3)\n",
    "regular_tokens = regular_tokenizer.encode(test_sequence)\n",
    "print(f\"Regular tokens: {regular_tokens}\")\n",
    "print(f\"Regular token count: {len(regular_tokens)}\")\n",
    "\n",
    "print(f\"\\nCompression comparison:\")\n",
    "print(f\"  Original length: {len(test_sequence)}\")\n",
    "print(f\"  Custom tokenizer: {len(custom_tokens)} tokens\")\n",
    "print(f\"  Regular tokenizer: {len(regular_tokens)} tokens\")\n",
    "print(f\"  Custom compression ratio: {len(test_sequence) / len(custom_tokens):.2f}\")\n",
    "print(f\"  Regular compression ratio: {len(test_sequence) / len(regular_tokens):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734cb11",
   "metadata": {},
   "source": [
    "## 8. Performance Optimization\n",
    "\n",
    "Learn techniques to optimize tokenization performance for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a62b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing import Pool\n",
    "import concurrent.futures\n",
    "\n",
    "# Generate test data for performance testing\n",
    "test_sequences = []\n",
    "for i in range(1000):\n",
    "    length = np.random.randint(100, 1000)\n",
    "    seq = ''.join(np.random.choice(['A', 'T', 'C', 'G'], length))\n",
    "    test_sequences.append(seq)\n",
    "\n",
    "print(f\"Generated {len(test_sequences)} test sequences for performance testing\")\n",
    "print(f\"Average length: {np.mean([len(s) for s in test_sequences]):.1f}\")\n",
    "print()\n",
    "\n",
    "# Test different tokenization approaches\n",
    "tokenizer = DNATokenizer(k=3)\n",
    "\n",
    "# 1. Sequential processing\n",
    "start_time = time.time()\n",
    "sequential_results = []\n",
    "for seq in test_sequences:\n",
    "    tokens = tokenizer.encode(seq)\n",
    "    sequential_results.append(tokens)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"Sequential processing: {sequential_time:.3f} seconds\")\n",
    "\n",
    "# 2. Batch processing function\n",
    "def process_batch(sequences):\n",
    "    \"\"\"Process a batch of sequences.\"\"\"\n",
    "    return [tokenizer.encode(seq) for seq in sequences]\n",
    "\n",
    "# 3. Parallel processing with multiprocessing\n",
    "def parallel_tokenize(sequences, num_workers=4, batch_size=100):\n",
    "    \"\"\"Tokenize sequences in parallel.\"\"\"\n",
    "    # Split into batches\n",
    "    batches = [sequences[i:i+batch_size] for i in range(0, len(sequences), batch_size)]\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        batch_results = list(executor.map(process_batch, batches))\n",
    "    \n",
    "    # Flatten results\n",
    "    results = []\n",
    "    for batch_result in batch_results:\n",
    "        results.extend(batch_result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test parallel processing\n",
    "start_time = time.time()\n",
    "parallel_results = parallel_tokenize(test_sequences, num_workers=2, batch_size=100)\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"Parallel processing: {parallel_time:.3f} seconds\")\n",
    "print(f\"Speedup: {sequential_time / parallel_time:.2f}x\")\n",
    "print()\n",
    "\n",
    "# Verify results are identical\n",
    "results_match = all(\n",
    "    seq_tokens == par_tokens \n",
    "    for seq_tokens, par_tokens in zip(sequential_results, parallel_results)\n",
    ")\n",
    "print(f\"Results match: {results_match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbb303",
   "metadata": {},
   "source": [
    "### Memory-Efficient Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03272571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientTokenizer:\n",
    "    \"\"\"Memory-efficient tokenizer for large datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_memory_mb=100):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_memory_mb = max_memory_mb\n",
    "    \n",
    "    def process_file_streaming(self, file_path, output_path):\n",
    "        \"\"\"Process file in streaming fashion to save memory.\"\"\"\n",
    "        import psutil\n",
    "        import gc\n",
    "        \n",
    "        process = psutil.Process()\n",
    "        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        with open(output_path, 'w') as outfile:\n",
    "            batch = []\n",
    "            batch_size = 0\n",
    "            \n",
    "            # Simulated file reading (replace with actual file reading)\n",
    "            for seq in test_sequences[:100]:  # Process subset for demo\n",
    "                batch.append(seq)\n",
    "                batch_size += len(seq)\n",
    "                \n",
    "                # Check memory usage\n",
    "                current_memory = process.memory_info().rss / 1024 / 1024\n",
    "                memory_used = current_memory - initial_memory\n",
    "                \n",
    "                # Process batch if memory limit reached or batch is large enough\n",
    "                if memory_used > self.max_memory_mb or len(batch) >= 50:\n",
    "                    # Process batch\n",
    "                    for seq in batch:\n",
    "                        tokens = self.tokenizer.encode(seq)\n",
    "                        outfile.write(f\"{','.join(map(str, tokens))}\\n\")\n",
    "                    \n",
    "                    # Clear batch and force garbage collection\n",
    "                    batch.clear()\n",
    "                    batch_size = 0\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"Processed batch, memory usage: {memory_used:.1f} MB\")\n",
    "            \n",
    "            # Process remaining sequences\n",
    "            if batch:\n",
    "                for seq in batch:\n",
    "                    tokens = self.tokenizer.encode(seq)\n",
    "                    outfile.write(f\"{','.join(map(str, tokens))}\\n\")\n",
    "        \n",
    "        final_memory = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage: {initial_memory:.1f} -> {final_memory:.1f} MB\")\n",
    "\n",
    "# Test memory-efficient processing\n",
    "efficient_tokenizer = MemoryEfficientTokenizer(tokenizer, max_memory_mb=50)\n",
    "output_file = \"/tmp/tokenized_sequences.txt\"\n",
    "\n",
    "print(\"Testing memory-efficient processing:\")\n",
    "efficient_tokenizer.process_file_streaming(None, output_file)\n",
    "\n",
    "# Verify output\n",
    "with open(output_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    print(f\"\\nOutput file contains {len(lines)} tokenized sequences\")\n",
    "    print(f\"First few lines:\")\n",
    "    for i, line in enumerate(lines[:3]):\n",
    "        tokens = list(map(int, line.strip().split(',')))\n",
    "        print(f\"  Line {i+1}: {tokens[:10]}... ({len(tokens)} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff156c",
   "metadata": {},
   "source": [
    "## 9. Tokenization Best Practices\n",
    "\n",
    "Summary of key best practices for genomic tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_recommendations(sequence_type, task_type, sequence_length):\n",
    "    \"\"\"Provide tokenization recommendations based on task characteristics.\"\"\"\n",
    "    recommendations = {\n",
    "        'tokenizer_type': '',\n",
    "        'k_value': 1,\n",
    "        'overlap': True,\n",
    "        'special_tokens': True,\n",
    "        'max_length': 512,\n",
    "        'reasoning': []\n",
    "    }\n",
    "    \n",
    "    # Sequence type recommendations\n",
    "    if sequence_type == 'dna':\n",
    "        recommendations['tokenizer_type'] = 'DNATokenizer'\n",
    "        if task_type == 'classification':\n",
    "            recommendations['k_value'] = 3\n",
    "            recommendations['reasoning'].append(\"k=3 captures codon-like patterns for DNA classification\")\n",
    "        elif task_type == 'generation':\n",
    "            recommendations['k_value'] = 1\n",
    "            recommendations['reasoning'].append(\"k=1 provides flexibility for sequence generation\")\n",
    "    \n",
    "    elif sequence_type == 'rna':\n",
    "        recommendations['tokenizer_type'] = 'RNATokenizer'\n",
    "        recommendations['k_value'] = 3\n",
    "        recommendations['reasoning'].append(\"k=3 captures codon structure in RNA\")\n",
    "    \n",
    "    elif sequence_type == 'protein':\n",
    "        recommendations['tokenizer_type'] = 'ProteinTokenizer'\n",
    "        recommendations['k_value'] = 1\n",
    "        recommendations['overlap'] = False\n",
    "        recommendations['reasoning'].append(\"Amino acids are natural tokens for proteins\")\n",
    "    \n",
    "    # Task type adjustments\n",
    "    if task_type in ['classification', 'regression']:\n",
    "        recommendations['special_tokens'] = True\n",
    "        recommendations['reasoning'].append(\"Special tokens help with sequence classification\")\n",
    "    \n",
    "    # Sequence length adjustments\n",
    "    if sequence_length < 100:\n",
    "        recommendations['max_length'] = 128\n",
    "        recommendations['reasoning'].append(\"Short sequences need smaller max_length\")\n",
    "    elif sequence_length > 1000:\n",
    "        recommendations['max_length'] = 1024\n",
    "        recommendations['k_value'] = min(recommendations['k_value'] + 1, 6)\n",
    "        recommendations['reasoning'].append(\"Long sequences benefit from larger k-mers and max_length\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test recommendations for different scenarios\n",
    "scenarios = [\n",
    "    ('dna', 'classification', 500),\n",
    "    ('rna', 'structure_prediction', 200),\n",
    "    ('protein', 'function_prediction', 300),\n",
    "    ('dna', 'generation', 1500),\n",
    "    ('dna', 'variant_prediction', 50)\n",
    "]\n",
    "\n",
    "print(\"Tokenization Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for seq_type, task_type, seq_len in scenarios:\n",
    "    rec = tokenization_recommendations(seq_type, task_type, seq_len)\n",
    "    \n",
    "    print(f\"\\nScenario: {seq_type.upper()} {task_type} (length ~{seq_len})\")\n",
    "    print(f\"  Tokenizer: {rec['tokenizer_type']}\")\n",
    "    print(f\"  K-value: {rec['k_value']}\")\n",
    "    print(f\"  Overlap: {rec['overlap']}\")\n",
    "    print(f\"  Special tokens: {rec['special_tokens']}\")\n",
    "    print(f\"  Max length: {rec['max_length']}\")\n",
    "    print(f\"  Reasoning:\")\n",
    "    for reason in rec['reasoning']:\n",
    "        print(f\"    - {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d854f5",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "Recap of key concepts and next steps for advanced genomic tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c750b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ“ Genomic Tokenization Summary\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "\n",
    "print(\"âœ… Key Concepts Learned:\")\n",
    "concepts = [\n",
    "    \"Different tokenization strategies (character, k-mer, overlapping)\",\n",
    "    \"Specialized tokenizers for DNA, RNA, and proteins\",\n",
    "    \"Advanced features: special tokens, padding, truncation\",\n",
    "    \"Batch processing and efficient data loading\",\n",
    "    \"Handling different genomic file formats\",\n",
    "    \"Custom tokenizer development for specific tasks\",\n",
    "    \"Performance optimization techniques\",\n",
    "    \"Memory-efficient processing for large datasets\",\n",
    "    \"Best practices and recommendations\"\n",
    "]\n",
    "\n",
    "for i, concept in enumerate(concepts, 1):\n",
    "    print(f\"{i:2d}. {concept}\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸš€ Next Steps:\")\n",
    "next_steps = [\n",
    "    \"Explore model training with your tokenized data (notebook 04)\",\n",
    "    \"Learn about model architectures (notebook 03)\",\n",
    "    \"Apply tokenization to real genomic datasets\",\n",
    "    \"Experiment with custom tokenizers for your specific tasks\",\n",
    "    \"Optimize tokenization for your computational resources\",\n",
    "    \"Integrate with existing genomic analysis pipelines\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print()\n",
    "print(\"ðŸ“š Additional Resources:\")\n",
    "resources = [\n",
    "    \"Hyena-GLT documentation: docs/\",\n",
    "    \"Example scripts: examples/\",\n",
    "    \"API reference: docs/API.md\",\n",
    "    \"Performance optimization guide: docs/OPTIMIZATION.md\"\n",
    "]\n",
    "\n",
    "for resource in resources:\n",
    "    print(f\"  â€¢ {resource}\")\n",
    "\n",
    "print()\n",
    "print(\"Happy tokenizing! ðŸ§¬ðŸ”¤\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
