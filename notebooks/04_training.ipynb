{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c33862",
   "metadata": {},
   "source": [
    "# Training Strategies and Optimization\n",
    "\n",
    "This notebook covers training strategies, optimization techniques, and best practices for the Hyena-GLT framework.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Training Configuration\n",
    "2. Optimization Strategies\n",
    "3. Learning Rate Scheduling\n",
    "4. Distributed Training\n",
    "5. Memory Optimization\n",
    "6. Monitoring and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from hyena_glt.config import TrainingConfig\n",
    "from hyena_glt.optimization import OptimizationConfig\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68700c0",
   "metadata": {},
   "source": [
    "## 1. Training Configuration\n",
    "\n",
    "Let's start by setting up a comprehensive training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9065eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic training configuration\n",
    "training_config = TrainingConfig(\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    num_epochs=10,\n",
    "    warmup_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    gradient_clip_norm=1.0,\n",
    "    save_every=1000,\n",
    "    eval_every=500,\n",
    "    mixed_precision=True,\n",
    "    gradient_accumulation_steps=4\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"Batch Size: {training_config.batch_size}\")\n",
    "print(f\"Learning Rate: {training_config.learning_rate}\")\n",
    "print(f\"Epochs: {training_config.num_epochs}\")\n",
    "print(f\"Mixed Precision: {training_config.mixed_precision}\")\n",
    "print(f\"Gradient Accumulation Steps: {training_config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f371d31",
   "metadata": {},
   "source": [
    "## 2. Optimization Strategies\n",
    "\n",
    "Different optimization strategies for various training scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization configuration for different scenarios\n",
    "optimization_configs = {\n",
    "    \"conservative\": OptimizationConfig(\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1\n",
    "    ),\n",
    "    \"aggressive\": OptimizationConfig(\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.1,\n",
    "        beta1=0.9,\n",
    "        beta2=0.95,\n",
    "        scheduler_type=\"linear\",\n",
    "        warmup_ratio=0.05\n",
    "    ),\n",
    "    \"fine_tuning\": OptimizationConfig(\n",
    "        optimizer_type=\"adamw\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        scheduler_type=\"polynomial\",\n",
    "        warmup_ratio=0.06\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, config in optimization_configs.items():\n",
    "    print(f\"\\n{name.upper()} Strategy:\")\n",
    "    print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "    print(f\"  Weight Decay: {config.weight_decay}\")\n",
    "    print(f\"  Scheduler: {config.scheduler_type}\")\n",
    "    print(f\"  Warmup Ratio: {config.warmup_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533a8f9e",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Scheduling\n",
    "\n",
    "Visualizing different learning rate schedules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_lr_schedule(schedule_type, base_lr=1e-4, total_steps=10000, warmup_steps=1000):\n",
    "    \"\"\"Simulate learning rate schedule.\"\"\"\n",
    "    steps = np.arange(total_steps)\n",
    "    lrs = []\n",
    "\n",
    "    for step in steps:\n",
    "        if step < warmup_steps:\n",
    "            # Linear warmup\n",
    "            lr = base_lr * (step / warmup_steps)\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "\n",
    "            if schedule_type == \"cosine\":\n",
    "                lr = base_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "            elif schedule_type == \"linear\":\n",
    "                lr = base_lr * (1 - progress)\n",
    "            elif schedule_type == \"polynomial\":\n",
    "                lr = base_lr * ((1 - progress) ** 2)\n",
    "            else:  # constant\n",
    "                lr = base_lr\n",
    "\n",
    "        lrs.append(lr)\n",
    "\n",
    "    return steps, lrs\n",
    "\n",
    "# Plot different schedules\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "schedules = [\"cosine\", \"linear\", \"polynomial\", \"constant\"]\n",
    "colors = [\"blue\", \"red\", \"green\", \"orange\"]\n",
    "\n",
    "for i, (schedule, color) in enumerate(zip(schedules, colors, strict=False)):\n",
    "    steps, lrs = simulate_lr_schedule(schedule)\n",
    "    axes[i].plot(steps, lrs, color=color, linewidth=2)\n",
    "    axes[i].set_title(f\"{schedule.title()} Schedule\")\n",
    "    axes[i].set_xlabel(\"Training Steps\")\n",
    "    axes[i].set_ylabel(\"Learning Rate\")\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_ylim(0, 1.1e-4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“ˆ Learning rate schedules visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9830c56",
   "metadata": {},
   "source": [
    "## 4. Training Loop Example\n",
    "\n",
    "A comprehensive training loop with monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d7d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    \"\"\"Monitor training progress and metrics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"learning_rate\": [],\n",
    "            \"gradient_norm\": [],\n",
    "            \"step_time\": []\n",
    "        }\n",
    "        self.step = 0\n",
    "\n",
    "    def log_metrics(self, **kwargs):\n",
    "        \"\"\"Log training metrics.\"\"\"\n",
    "        self.step += 1\n",
    "        for key, value in kwargs.items():\n",
    "            if key in self.metrics:\n",
    "                self.metrics[key].append(value)\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot training metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "        # Loss curves\n",
    "        if self.metrics[\"train_loss\"]:\n",
    "            axes[0, 0].plot(self.metrics[\"train_loss\"], label=\"Train Loss\", color=\"blue\")\n",
    "        if self.metrics[\"val_loss\"]:\n",
    "            axes[0, 0].plot(self.metrics[\"val_loss\"], label=\"Val Loss\", color=\"red\")\n",
    "        axes[0, 0].set_title(\"Training Loss\")\n",
    "        axes[0, 0].set_xlabel(\"Steps\")\n",
    "        axes[0, 0].set_ylabel(\"Loss\")\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Learning rate\n",
    "        if self.metrics[\"learning_rate\"]:\n",
    "            axes[0, 1].plot(self.metrics[\"learning_rate\"], color=\"green\")\n",
    "        axes[0, 1].set_title(\"Learning Rate\")\n",
    "        axes[0, 1].set_xlabel(\"Steps\")\n",
    "        axes[0, 1].set_ylabel(\"LR\")\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Gradient norm\n",
    "        if self.metrics[\"gradient_norm\"]:\n",
    "            axes[1, 0].plot(self.metrics[\"gradient_norm\"], color=\"purple\")\n",
    "        axes[1, 0].set_title(\"Gradient Norm\")\n",
    "        axes[1, 0].set_xlabel(\"Steps\")\n",
    "        axes[1, 0].set_ylabel(\"Norm\")\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Step time\n",
    "        if self.metrics[\"step_time\"]:\n",
    "            axes[1, 1].plot(self.metrics[\"step_time\"], color=\"orange\")\n",
    "        axes[1, 1].set_title(\"Step Time\")\n",
    "        axes[1, 1].set_xlabel(\"Steps\")\n",
    "        axes[1, 1].set_ylabel(\"Time (s)\")\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "monitor = TrainingMonitor()\n",
    "\n",
    "# Simulate training metrics\n",
    "for i in range(100):\n",
    "    # Simulate decreasing loss\n",
    "    train_loss = 2.0 * np.exp(-i/50) + 0.1 + np.random.normal(0, 0.05)\n",
    "    val_loss = 2.2 * np.exp(-i/60) + 0.15 + np.random.normal(0, 0.03)\n",
    "\n",
    "    # Simulate learning rate schedule\n",
    "    if i < 20:\n",
    "        lr = 1e-4 * (i / 20)\n",
    "    else:\n",
    "        lr = 1e-4 * 0.5 * (1 + np.cos(np.pi * (i-20) / 80))\n",
    "\n",
    "    # Simulate gradient norm\n",
    "    grad_norm = np.random.lognormal(0, 0.5)\n",
    "\n",
    "    # Simulate step time\n",
    "    step_time = 0.5 + np.random.normal(0, 0.1)\n",
    "\n",
    "    monitor.log_metrics(\n",
    "        train_loss=train_loss,\n",
    "        val_loss=val_loss,\n",
    "        learning_rate=lr,\n",
    "        gradient_norm=grad_norm,\n",
    "        step_time=step_time\n",
    "    )\n",
    "\n",
    "monitor.plot_metrics()\n",
    "print(\"ðŸ“Š Training metrics visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7258f",
   "metadata": {},
   "source": [
    "## 5. Memory Optimization Techniques\n",
    "\n",
    "Strategies for efficient memory usage during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1587beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryOptimizer:\n",
    "    \"\"\"Memory optimization utilities.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def estimate_memory_usage(model, batch_size, sequence_length):\n",
    "        \"\"\"Estimate GPU memory usage.\"\"\"\n",
    "        # Count parameters\n",
    "        param_count = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        # Estimate memory (rough approximation)\n",
    "        param_memory = param_count * 4  # 4 bytes per float32\n",
    "        activation_memory = batch_size * sequence_length * model.config.hidden_size * 4\n",
    "        gradient_memory = param_memory  # Same as parameters\n",
    "        optimizer_memory = param_memory * 2  # AdamW stores momentum and variance\n",
    "\n",
    "        total_memory = param_memory + activation_memory + gradient_memory + optimizer_memory\n",
    "\n",
    "        return {\n",
    "            \"parameters\": param_memory / 1e9,  # GB\n",
    "            \"activations\": activation_memory / 1e9,\n",
    "            \"gradients\": gradient_memory / 1e9,\n",
    "            \"optimizer\": optimizer_memory / 1e9,\n",
    "            \"total\": total_memory / 1e9\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_memory_tips(memory_gb):\n",
    "        \"\"\"Get memory optimization tips based on usage.\"\"\"\n",
    "        tips = []\n",
    "\n",
    "        if memory_gb > 16:\n",
    "            tips.extend([\n",
    "                \"Consider gradient checkpointing\",\n",
    "                \"Use gradient accumulation\",\n",
    "                \"Enable mixed precision training\",\n",
    "                \"Reduce batch size\"\n",
    "            ])\n",
    "\n",
    "        if memory_gb > 24:\n",
    "            tips.extend([\n",
    "                \"Use model parallelism\",\n",
    "                \"Consider ZeRO optimizer\",\n",
    "                \"Use activation recomputation\"\n",
    "            ])\n",
    "\n",
    "        return tips\n",
    "\n",
    "# Example memory estimation\n",
    "print(\"Memory Optimization Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Mock model config for estimation\n",
    "class MockConfig:\n",
    "    def __init__(self):\n",
    "        self.hidden_size = 1024\n",
    "        self.num_layers = 12\n",
    "        self.vocab_size = 8000\n",
    "\n",
    "class MockModel:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # Estimate parameter count\n",
    "        self._param_count = (\n",
    "            config.vocab_size * config.hidden_size +  # Embedding\n",
    "            config.num_layers * config.hidden_size * config.hidden_size * 4 +  # Layers\n",
    "            config.hidden_size * config.vocab_size  # Output\n",
    "        )\n",
    "\n",
    "    def parameters(self):\n",
    "        class MockParam:\n",
    "            def __init__(self, numel):\n",
    "                self._numel = numel\n",
    "            def numel(self):\n",
    "                return self._numel\n",
    "        return [MockParam(self._param_count)]\n",
    "\n",
    "config = MockConfig()\n",
    "model = MockModel(config)\n",
    "optimizer = MemoryOptimizer()\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64]\n",
    "sequence_length = 2048\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    memory = optimizer.estimate_memory_usage(model, batch_size, sequence_length)\n",
    "    tips = optimizer.get_memory_tips(memory[\"total\"])\n",
    "\n",
    "    print(f\"\\nBatch Size: {batch_size}\")\n",
    "    print(f\"  Parameters: {memory['parameters']:.2f} GB\")\n",
    "    print(f\"  Activations: {memory['activations']:.2f} GB\")\n",
    "    print(f\"  Gradients: {memory['gradients']:.2f} GB\")\n",
    "    print(f\"  Optimizer: {memory['optimizer']:.2f} GB\")\n",
    "    print(f\"  Total: {memory['total']:.2f} GB\")\n",
    "\n",
    "    if tips:\n",
    "        print(f\"  ðŸ’¡ Tips: {', '.join(tips)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2842f3",
   "metadata": {},
   "source": [
    "## 6. Distributed Training Setup\n",
    "\n",
    "Configuration for multi-GPU and multi-node training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d994f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedTrainingConfig:\n",
    "    \"\"\"Configuration for distributed training.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.strategies = {\n",
    "            \"data_parallel\": {\n",
    "                \"description\": \"Replicate model on each GPU, split batch\",\n",
    "                \"pros\": [\"Simple to implement\", \"Good for large batches\"],\n",
    "                \"cons\": [\"Memory overhead\", \"Communication overhead\"],\n",
    "                \"best_for\": \"Models that fit on single GPU\"\n",
    "            },\n",
    "            \"model_parallel\": {\n",
    "                \"description\": \"Split model layers across GPUs\",\n",
    "                \"pros\": [\"Handles large models\", \"Memory efficient\"],\n",
    "                \"cons\": [\"Pipeline bubbles\", \"Complex implementation\"],\n",
    "                \"best_for\": \"Very large models\"\n",
    "            },\n",
    "            \"pipeline_parallel\": {\n",
    "                \"description\": \"Pipeline model layers with micro-batches\",\n",
    "                \"pros\": [\"Good throughput\", \"Handles large models\"],\n",
    "                \"cons\": [\"Latency overhead\", \"Memory for pipeline\"],\n",
    "                \"best_for\": \"Large models with many layers\"\n",
    "            },\n",
    "            \"zero\": {\n",
    "                \"description\": \"ZeRO optimizer state partitioning\",\n",
    "                \"pros\": [\"Memory efficient\", \"Scales well\"],\n",
    "                \"cons\": [\"Communication overhead\", \"Implementation complexity\"],\n",
    "                \"best_for\": \"Large scale training\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def recommend_strategy(self, model_size_gb, num_gpus, gpu_memory_gb):\n",
    "        \"\"\"Recommend distributed training strategy.\"\"\"\n",
    "        recommendations = []\n",
    "\n",
    "        if model_size_gb < gpu_memory_gb * 0.5:\n",
    "            recommendations.append((\"data_parallel\", \"Model fits comfortably on single GPU\"))\n",
    "\n",
    "        if model_size_gb > gpu_memory_gb:\n",
    "            recommendations.append((\"model_parallel\", \"Model too large for single GPU\"))\n",
    "            recommendations.append((\"pipeline_parallel\", \"Alternative for large models\"))\n",
    "\n",
    "        if num_gpus >= 8:\n",
    "            recommendations.append((\"zero\", \"Large scale training with many GPUs\"))\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def print_strategies(self):\n",
    "        \"\"\"Print all available strategies.\"\"\"\n",
    "        print(\"Distributed Training Strategies\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for name, info in self.strategies.items():\n",
    "            print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
    "            print(f\"  Description: {info['description']}\")\n",
    "            print(f\"  Pros: {', '.join(info['pros'])}\")\n",
    "            print(f\"  Cons: {', '.join(info['cons'])}\")\n",
    "            print(f\"  Best for: {info['best_for']}\")\n",
    "\n",
    "# Example usage\n",
    "dist_config = DistributedTrainingConfig()\n",
    "dist_config.print_strategies()\n",
    "\n",
    "# Example recommendations\n",
    "scenarios = [\n",
    "    (\"Small model\", 2, 4, 24),  # 2GB model, 4 GPUs, 24GB each\n",
    "    (\"Medium model\", 8, 8, 24),  # 8GB model, 8 GPUs, 24GB each\n",
    "    (\"Large model\", 30, 8, 24)   # 30GB model, 8 GPUs, 24GB each\n",
    "]\n",
    "\n",
    "print(\"\\n\\nRecommendations for Different Scenarios:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, model_size, num_gpus, gpu_memory in scenarios:\n",
    "    print(f\"\\n{name}: {model_size}GB model, {num_gpus} GPUs ({gpu_memory}GB each)\")\n",
    "    recommendations = dist_config.recommend_strategy(model_size, num_gpus, gpu_memory)\n",
    "\n",
    "    for strategy, reason in recommendations:\n",
    "        print(f\"  âœ… {strategy}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155db70",
   "metadata": {},
   "source": [
    "## 7. Best Practices Summary\n",
    "\n",
    "Key takeaways for effective training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea9414",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_best_practices = {\n",
    "    \"Learning Rate\": [\n",
    "        \"Start with learning rate finder\",\n",
    "        \"Use warmup for large batch sizes\",\n",
    "        \"Cosine annealing often works well\",\n",
    "        \"Monitor gradient norm for instability\"\n",
    "    ],\n",
    "    \"Batch Size\": [\n",
    "        \"Larger batches â†’ more stable gradients\",\n",
    "        \"Use gradient accumulation for memory constraints\",\n",
    "        \"Scale learning rate with batch size\",\n",
    "        \"Monitor training dynamics\"\n",
    "    ],\n",
    "    \"Regularization\": [\n",
    "        \"Weight decay for parameter regularization\",\n",
    "        \"Dropout for overfitting prevention\",\n",
    "        \"Gradient clipping for stability\",\n",
    "        \"Early stopping based on validation\"\n",
    "    ],\n",
    "    \"Memory Optimization\": [\n",
    "        \"Use mixed precision (FP16/BF16)\",\n",
    "        \"Gradient checkpointing for large models\",\n",
    "        \"Optimize data loading pipeline\",\n",
    "        \"Monitor GPU memory usage\"\n",
    "    ],\n",
    "    \"Monitoring\": [\n",
    "        \"Track loss curves\",\n",
    "        \"Monitor learning rate schedule\",\n",
    "        \"Watch gradient norms\",\n",
    "        \"Log training metrics regularly\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"ðŸŽ¯ Training Best Practices\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for category, practices in training_best_practices.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"  â€¢ {practice}\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready to train your Hyena-GLT model!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
