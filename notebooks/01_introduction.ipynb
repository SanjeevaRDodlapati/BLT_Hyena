{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21425f37",
   "metadata": {},
   "source": [
    "# Introduction to Hyena-GLT Framework\n",
    "\n",
    "Welcome to the Hyena-GLT (Genome Language Transformer) framework! This notebook provides an introduction to the framework and demonstrates its key capabilities for genomic sequence modeling.\n",
    "\n",
    "## What is Hyena-GLT?\n",
    "\n",
    "Hyena-GLT is a hybrid architecture that combines:\n",
    "- **BLT's Byte Latent Tokenization**: Efficient compression and tokenization\n",
    "- **Savanna's Striped Hyena blocks**: Linear complexity convolutions\n",
    "- **Genomic-specific adaptations**: Specialized for biological sequences\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- 🧬 **Multi-modal genomic support**: DNA, RNA, and protein sequences\n",
    "- ⚡ **Efficient processing**: Linear O(n) complexity vs quadratic attention\n",
    "- 🔄 **Dynamic tokenization**: Adaptive sequence compression\n",
    "- 📊 **Multi-task capable**: Classification, generation, and analysis\n",
    "- 🎯 **Fine-tuning ready**: Easy adaptation to downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a18d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Hyena-GLT imports\n",
    "from hyena_glt import HyenaGLT, HyenaGLTConfig\n",
    "from hyena_glt.data import GenomicTokenizer\n",
    "from hyena_glt.utils import analyze_tokenization, compute_sequence_statistics\n",
    "\n",
    "print(\"🧬 Hyena-GLT Framework Loaded Successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d82b8",
   "metadata": {},
   "source": [
    "## Quick Start Example\n",
    "\n",
    "Let's start with a simple example to demonstrate the basic workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80641111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create configuration\n",
    "config = HyenaGLTConfig(\n",
    "    vocab_size=4096,\n",
    "    hidden_size=256,  # Smaller for demo\n",
    "    num_layers=4,     # Fewer layers for demo\n",
    "    num_heads=8,\n",
    "    sequence_length=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"Configuration created:\")\n",
    "print(f\"  - Vocabulary size: {config.vocab_size:,}\")\n",
    "print(f\"  - Hidden size: {config.hidden_size}\")\n",
    "print(f\"  - Number of layers: {config.num_layers}\")\n",
    "print(f\"  - Sequence length: {config.sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16acff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize tokenizer\n",
    "tokenizer = GenomicTokenizer(\n",
    "    sequence_type=\"dna\",\n",
    "    vocab_size=config.vocab_size,\n",
    "    max_length=config.sequence_length\n",
    ")\n",
    "\n",
    "print(f\"Tokenizer initialized for {tokenizer.sequence_type.upper()} sequences\")\n",
    "print(f\"Vocabulary size: {len(tokenizer.vocab) if hasattr(tokenizer, 'vocab') else 'Dynamic'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create model\n",
    "model = HyenaGLT(config)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Model created with {total_params:,} parameters\")\n",
    "print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB (FP32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e4f810",
   "metadata": {},
   "source": [
    "## Working with Genomic Sequences\n",
    "\n",
    "Let's explore how Hyena-GLT processes different types of genomic sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02428b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example genomic sequences\n",
    "sequences = {\n",
    "    \"Gene fragment\": \"ATGCGATCGATCGATCGAATTCGCTAGCTAGCTAGCTAGCTAGCTAGCTAG\",\n",
    "    \"Promoter region\": \"TATAATGGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGCGC\",\n",
    "    \"Coding sequence\": \"ATGAAACGTTTCGACGACGACGACGACGACGACGACGACGACGACGACTAG\",\n",
    "    \"Repetitive DNA\": \"ATATATATATATATATATATATATATATATATATATATATATATATATAT\"\n",
    "}\n",
    "\n",
    "print(\"Example genomic sequences:\")\n",
    "for name, seq in sequences.items():\n",
    "    print(f\"  {name}: {seq[:30]}... ({len(seq)} bp)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fd131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sequences\n",
    "tokenized_sequences = {}\n",
    "for name, seq in sequences.items():\n",
    "    tokens = tokenizer.encode(seq)\n",
    "    tokenized_sequences[name] = tokens\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Original length: {len(seq)} bp\")\n",
    "    print(f\"  Tokenized length: {len(tokens)} tokens\")\n",
    "    print(f\"  Compression ratio: {len(seq) / len(tokens):.2f}x\")\n",
    "    print(f\"  Sample tokens: {tokens[:10]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b39344",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "Now let's run inference on our genomic sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "model.eval()\n",
    "inference_results = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, tokens in tokenized_sequences.items():\n",
    "        # Prepare input\n",
    "        input_ids = torch.tensor([tokens])  # Add batch dimension\n",
    "\n",
    "        # Run model\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        # Store results\n",
    "        inference_results[name] = {\n",
    "            'logits_shape': outputs.logits.shape,\n",
    "            'logits_mean': outputs.logits.mean().item(),\n",
    "            'logits_std': outputs.logits.std().item(),\n",
    "            'hidden_states_shape': outputs.hidden_states[-1].shape if outputs.hidden_states else 'N/A'\n",
    "        }\n",
    "\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Output logits shape: {outputs.logits.shape}\")\n",
    "        print(f\"  Logits statistics: mean={outputs.logits.mean():.4f}, std={outputs.logits.std():.4f}\")\n",
    "        if outputs.hidden_states:\n",
    "            print(f\"  Hidden states shape: {outputs.hidden_states[-1].shape}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da0c31",
   "metadata": {},
   "source": [
    "## Sequence Analysis\n",
    "\n",
    "Let's analyze the properties of our genomic sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequences\n",
    "sequence_list = list(sequences.values())\n",
    "stats = compute_sequence_statistics(sequence_list)\n",
    "\n",
    "print(\"Sequence Statistics:\")\n",
    "print(f\"  Average length: {stats['avg_length']:.1f} bp\")\n",
    "print(f\"  Length std: {stats['length_std']:.1f} bp\")\n",
    "print(f\"  Average GC content: {stats['avg_gc_content']:.3f}\")\n",
    "print(f\"  GC content std: {stats['gc_content_std']:.3f}\")\n",
    "\n",
    "if 'base_composition' in stats:\n",
    "    print(\"\\nBase composition:\")\n",
    "    for base, fraction in stats['base_composition'].items():\n",
    "        print(f\"  {base}: {fraction:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa48c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization analysis\n",
    "token_stats = analyze_tokenization(tokenizer, sequence_list)\n",
    "\n",
    "print(\"Tokenization Analysis:\")\n",
    "print(f\"  Average tokens per sequence: {token_stats['avg_tokens']:.1f}\")\n",
    "print(f\"  Token count std: {token_stats['token_std']:.1f}\")\n",
    "print(f\"  Average compression ratio: {token_stats['compression_ratio']:.2f}x\")\n",
    "print(f\"  Compression std: {token_stats['compression_std']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b419ac9",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create some visualizations to better understand our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dda71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Sequence lengths\n",
    "names = list(sequences.keys())\n",
    "lengths = [len(seq) for seq in sequences.values()]\n",
    "\n",
    "axes[0, 0].bar(range(len(names)), lengths, color='lightblue')\n",
    "axes[0, 0].set_title('Sequence Lengths')\n",
    "axes[0, 0].set_xlabel('Sequence')\n",
    "axes[0, 0].set_ylabel('Length (bp)')\n",
    "axes[0, 0].set_xticks(range(len(names)))\n",
    "axes[0, 0].set_xticklabels(names, rotation=45, ha='right')\n",
    "\n",
    "# Plot 2: Token counts\n",
    "token_counts = [len(tokens) for tokens in tokenized_sequences.values()]\n",
    "\n",
    "axes[0, 1].bar(range(len(names)), token_counts, color='lightgreen')\n",
    "axes[0, 1].set_title('Token Counts')\n",
    "axes[0, 1].set_xlabel('Sequence')\n",
    "axes[0, 1].set_ylabel('Tokens')\n",
    "axes[0, 1].set_xticks(range(len(names)))\n",
    "axes[0, 1].set_xticklabels(names, rotation=45, ha='right')\n",
    "\n",
    "# Plot 3: Compression ratios\n",
    "compression_ratios = [lengths[i] / token_counts[i] for i in range(len(lengths))]\n",
    "\n",
    "axes[1, 0].bar(range(len(names)), compression_ratios, color='orange')\n",
    "axes[1, 0].set_title('Compression Ratios')\n",
    "axes[1, 0].set_xlabel('Sequence')\n",
    "axes[1, 0].set_ylabel('Compression Ratio')\n",
    "axes[1, 0].set_xticks(range(len(names)))\n",
    "axes[1, 0].set_xticklabels(names, rotation=45, ha='right')\n",
    "\n",
    "# Plot 4: GC content\n",
    "gc_contents = []\n",
    "for seq in sequences.values():\n",
    "    gc_count = seq.count('G') + seq.count('C')\n",
    "    gc_content = gc_count / len(seq) if len(seq) > 0 else 0\n",
    "    gc_contents.append(gc_content)\n",
    "\n",
    "axes[1, 1].bar(range(len(names)), gc_contents, color='red', alpha=0.7)\n",
    "axes[1, 1].set_title('GC Content')\n",
    "axes[1, 1].set_xlabel('Sequence')\n",
    "axes[1, 1].set_ylabel('GC Content')\n",
    "axes[1, 1].set_xticks(range(len(names)))\n",
    "axes[1, 1].set_xticklabels(names, rotation=45, ha='right')\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955999c",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This introduction showed you the basics of Hyena-GLT. Here's what to explore next:\n",
    "\n",
    "1. **📊 Tokenization Deep Dive**: `02_tokenization.ipynb` - Learn about BLT tokenization\n",
    "2. **🏗️ Model Architecture**: `03_model_architecture.ipynb` - Understand Hyena blocks\n",
    "3. **🎯 Training**: `04_training.ipynb` - Train your own models\n",
    "4. **📈 Evaluation**: `05_evaluation.ipynb` - Assess model performance\n",
    "5. **🔧 Fine-tuning**: `06_fine_tuning.ipynb` - Adapt to specific tasks\n",
    "6. **🧬 Generation**: `07_generation.ipynb` - Generate new sequences\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **Documentation**: See `docs/` folder for comprehensive guides\n",
    "- **Examples**: Check `examples/` for complete scripts\n",
    "- **API Reference**: Full API documentation in `docs/API.md`\n",
    "- **User Guide**: Step-by-step guide in `docs/USER_GUIDE.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 Introduction to Hyena-GLT completed!\")\n",
    "print(\"Happy genomic modeling! 🧬\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
