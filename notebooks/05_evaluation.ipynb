{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea932d6",
   "metadata": {},
   "source": [
    "# Model Evaluation Techniques\n",
    "\n",
    "This notebook covers comprehensive evaluation strategies for genomic language models using the Hyena-GLT framework.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Evaluation Metrics\n",
    "2. Benchmarking Strategies\n",
    "3. Statistical Analysis\n",
    "4. Visualization\n",
    "5. Performance Profiling\n",
    "6. Comparison with Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97970e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96bd3c",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics Overview\n",
    "\n",
    "Different metrics for different genomic tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicMetrics:\n",
    "    \"\"\"Comprehensive metrics for genomic sequence tasks.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sequence_classification_metrics(y_true, y_pred, y_scores=None):\n",
    "        \"\"\"Calculate classification metrics.\"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='weighted'\n",
    "        )\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "        if y_scores is not None:\n",
    "            if len(np.unique(y_true)) == 2:  # Binary classification\n",
    "                metrics['auc_roc'] = roc_auc_score(y_true, y_scores)\n",
    "                metrics['auc_pr'] = average_precision_score(y_true, y_scores)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def sequence_generation_metrics(generated_seqs, reference_seqs):\n",
    "        \"\"\"Calculate generation quality metrics.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        # GC content similarity\n",
    "        gc_content_gen = [GenomicMetrics._gc_content(seq) for seq in generated_seqs]\n",
    "        gc_content_ref = [GenomicMetrics._gc_content(seq) for seq in reference_seqs]\n",
    "        metrics['gc_content_mse'] = np.mean((np.array(gc_content_gen) - np.array(gc_content_ref))**2)\n",
    "\n",
    "        # K-mer distribution similarity\n",
    "        for k in [3, 4, 5]:\n",
    "            kmer_sim = GenomicMetrics._kmer_similarity(generated_seqs, reference_seqs, k)\n",
    "            metrics[f'{k}_mer_similarity'] = kmer_sim\n",
    "\n",
    "        # Sequence diversity\n",
    "        metrics['diversity'] = GenomicMetrics._sequence_diversity(generated_seqs)\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _gc_content(sequence):\n",
    "        \"\"\"Calculate GC content of a sequence.\"\"\"\n",
    "        sequence = sequence.upper()\n",
    "        gc_count = sequence.count('G') + sequence.count('C')\n",
    "        return gc_count / len(sequence) if len(sequence) > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _kmer_similarity(seqs1, seqs2, k):\n",
    "        \"\"\"Calculate k-mer distribution similarity.\"\"\"\n",
    "        def get_kmer_freq(sequences, k):\n",
    "            kmer_counts = {}\n",
    "            total_kmers = 0\n",
    "            for seq in sequences:\n",
    "                for i in range(len(seq) - k + 1):\n",
    "                    kmer = seq[i:i+k]\n",
    "                    kmer_counts[kmer] = kmer_counts.get(kmer, 0) + 1\n",
    "                    total_kmers += 1\n",
    "            return {kmer: count/total_kmers for kmer, count in kmer_counts.items()}\n",
    "\n",
    "        freq1 = get_kmer_freq(seqs1, k)\n",
    "        freq2 = get_kmer_freq(seqs2, k)\n",
    "\n",
    "        # Jensen-Shannon divergence\n",
    "        all_kmers = set(freq1.keys()) | set(freq2.keys())\n",
    "        p = np.array([freq1.get(kmer, 0) for kmer in all_kmers])\n",
    "        q = np.array([freq2.get(kmer, 0) for kmer in all_kmers])\n",
    "\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        p = p + 1e-10\n",
    "        q = q + 1e-10\n",
    "        p = p / p.sum()\n",
    "        q = q / q.sum()\n",
    "\n",
    "        m = (p + q) / 2\n",
    "        js_div = 0.5 * np.sum(p * np.log(p / m)) + 0.5 * np.sum(q * np.log(q / m))\n",
    "        return 1 - js_div  # Convert to similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def _sequence_diversity(sequences):\n",
    "        \"\"\"Calculate sequence diversity.\"\"\"\n",
    "        if len(sequences) <= 1:\n",
    "            return 0\n",
    "\n",
    "        # Pairwise edit distance diversity\n",
    "        total_distance = 0\n",
    "        comparisons = 0\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            for j in range(i+1, min(i+100, len(sequences))):  # Limit comparisons\n",
    "                distance = GenomicMetrics._edit_distance(sequences[i], sequences[j])\n",
    "                total_distance += distance\n",
    "                comparisons += 1\n",
    "\n",
    "        return total_distance / comparisons if comparisons > 0 else 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _edit_distance(s1, s2):\n",
    "        \"\"\"Calculate edit distance between two sequences.\"\"\"\n",
    "        if len(s1) > len(s2):\n",
    "            s1, s2 = s2, s1\n",
    "\n",
    "        distances = range(len(s1) + 1)\n",
    "        for i2, c2 in enumerate(s2):\n",
    "            new_distances = [i2 + 1]\n",
    "            for i1, c1 in enumerate(s1):\n",
    "                if c1 == c2:\n",
    "                    new_distances.append(distances[i1])\n",
    "                else:\n",
    "                    new_distances.append(1 + min(distances[i1], distances[i1 + 1], new_distances[-1]))\n",
    "            distances = new_distances\n",
    "        return distances[-1]\n",
    "\n",
    "# Example metrics calculation\n",
    "print(\"ðŸ”¬ Genomic Metrics Overview\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate classification results\n",
    "np.random.seed(42)\n",
    "y_true = np.random.choice([0, 1], size=1000, p=[0.6, 0.4])\n",
    "y_scores = np.random.beta(2, 5, size=1000)\n",
    "y_pred = (y_scores > 0.3).astype(int)\n",
    "\n",
    "metrics = GenomicMetrics()\n",
    "classification_metrics = metrics.sequence_classification_metrics(y_true, y_pred, y_scores)\n",
    "\n",
    "print(\"Classification Metrics:\")\n",
    "for metric, value in classification_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Simulate generation results\n",
    "generated = ['ATCGATCG' * 10, 'GCTAGCTA' * 10, 'TTAAGGCC' * 10]\n",
    "reference = ['ATCGATCG' * 10, 'GCTAGCTA' * 10, 'AATTGGCC' * 10]\n",
    "\n",
    "generation_metrics = metrics.sequence_generation_metrics(generated, reference)\n",
    "\n",
    "print(\"\\nGeneration Metrics:\")\n",
    "for metric, value in generation_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd074e25",
   "metadata": {},
   "source": [
    "## 2. Performance Benchmarking\n",
    "\n",
    "Comprehensive benchmarking suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab142227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBenchmark:\n",
    "    \"\"\"Comprehensive model benchmarking.\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device='cuda'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.results = {}\n",
    "\n",
    "    def inference_speed_test(self, sequence_lengths, batch_sizes, num_runs=10):\n",
    "        \"\"\"Test inference speed across different configurations.\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for seq_len in sequence_lengths:\n",
    "            for batch_size in batch_sizes:\n",
    "                # Generate dummy data\n",
    "                dummy_input = torch.randint(0, 100, (batch_size, seq_len)).to(self.device)\n",
    "\n",
    "                # Warmup\n",
    "                for _ in range(3):\n",
    "                    with torch.no_grad():\n",
    "                        _ = self.model(dummy_input)\n",
    "\n",
    "                # Actual timing\n",
    "                times = []\n",
    "                for _ in range(num_runs):\n",
    "                    torch.cuda.synchronize() if self.device == 'cuda' else None\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        _ = self.model(dummy_input)\n",
    "\n",
    "                    torch.cuda.synchronize() if self.device == 'cuda' else None\n",
    "                    end_time = time.time()\n",
    "                    times.append(end_time - start_time)\n",
    "\n",
    "                avg_time = np.mean(times)\n",
    "                std_time = np.std(times)\n",
    "                throughput = batch_size / avg_time\n",
    "\n",
    "                results.append({\n",
    "                    'sequence_length': seq_len,\n",
    "                    'batch_size': batch_size,\n",
    "                    'avg_time': avg_time,\n",
    "                    'std_time': std_time,\n",
    "                    'throughput': throughput\n",
    "                })\n",
    "\n",
    "        self.results['inference_speed'] = pd.DataFrame(results)\n",
    "        return self.results['inference_speed']\n",
    "\n",
    "    def memory_usage_test(self, sequence_lengths, batch_sizes):\n",
    "        \"\"\"Test memory usage across different configurations.\"\"\"\n",
    "        if self.device != 'cuda':\n",
    "            print(\"Memory testing only available for CUDA\")\n",
    "            return None\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for seq_len in sequence_lengths:\n",
    "            for batch_size in batch_sizes:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "                dummy_input = torch.randint(0, 100, (batch_size, seq_len)).to(self.device)\n",
    "\n",
    "                # Forward pass\n",
    "                output = self.model(dummy_input)\n",
    "\n",
    "                current_memory = torch.cuda.memory_allocated() / 1024**3  # GB\n",
    "                peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB\n",
    "\n",
    "                results.append({\n",
    "                    'sequence_length': seq_len,\n",
    "                    'batch_size': batch_size,\n",
    "                    'current_memory_gb': current_memory,\n",
    "                    'peak_memory_gb': peak_memory\n",
    "                })\n",
    "\n",
    "                del dummy_input, output\n",
    "\n",
    "        self.results['memory_usage'] = pd.DataFrame(results)\n",
    "        return self.results['memory_usage']\n",
    "\n",
    "    def plot_benchmarks(self):\n",
    "        \"\"\"Plot benchmark results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "        if 'inference_speed' in self.results:\n",
    "            df = self.results['inference_speed']\n",
    "\n",
    "            # Throughput heatmap\n",
    "            pivot_throughput = df.pivot(index='sequence_length', columns='batch_size', values='throughput')\n",
    "            sns.heatmap(pivot_throughput, annot=True, fmt='.1f', ax=axes[0,0], cmap='viridis')\n",
    "            axes[0,0].set_title('Throughput (sequences/sec)')\n",
    "\n",
    "            # Latency heatmap\n",
    "            pivot_latency = df.pivot(index='sequence_length', columns='batch_size', values='avg_time')\n",
    "            sns.heatmap(pivot_latency, annot=True, fmt='.3f', ax=axes[0,1], cmap='plasma')\n",
    "            axes[0,1].set_title('Average Latency (seconds)')\n",
    "\n",
    "        if 'memory_usage' in self.results:\n",
    "            df = self.results['memory_usage']\n",
    "\n",
    "            # Memory usage heatmap\n",
    "            pivot_memory = df.pivot(index='sequence_length', columns='batch_size', values='peak_memory_gb')\n",
    "            sns.heatmap(pivot_memory, annot=True, fmt='.2f', ax=axes[1,0], cmap='Reds')\n",
    "            axes[1,0].set_title('Peak Memory Usage (GB)')\n",
    "\n",
    "            # Memory efficiency (sequences per GB)\n",
    "            df['memory_efficiency'] = (df['sequence_length'] * df['batch_size']) / df['peak_memory_gb']\n",
    "            pivot_efficiency = df.pivot(index='sequence_length', columns='batch_size', values='memory_efficiency')\n",
    "            sns.heatmap(pivot_efficiency, annot=True, fmt='.0f', ax=axes[1,1], cmap='Blues')\n",
    "            axes[1,1].set_title('Memory Efficiency (tokens/GB)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Mock benchmark (since we don't have actual model)\n",
    "print(\"ðŸš€ Model Benchmarking Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate benchmark results\n",
    "sequence_lengths = [512, 1024, 2048]\n",
    "batch_sizes = [8, 16, 32]\n",
    "\n",
    "# Create mock results\n",
    "mock_results = []\n",
    "for seq_len in sequence_lengths:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Simulate realistic performance characteristics\n",
    "        base_time = (seq_len / 1000) * (batch_size / 10) * 0.1\n",
    "        avg_time = base_time + np.random.normal(0, base_time * 0.1)\n",
    "        throughput = batch_size / avg_time\n",
    "        memory_gb = (seq_len * batch_size * 4) / (1024**3) * 2  # Rough estimate\n",
    "\n",
    "        mock_results.append({\n",
    "            'sequence_length': seq_len,\n",
    "            'batch_size': batch_size,\n",
    "            'avg_time': avg_time,\n",
    "            'throughput': throughput,\n",
    "            'peak_memory_gb': memory_gb\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(mock_results)\n",
    "print(\"Benchmark Results:\")\n",
    "print(df_results.round(3))\n",
    "\n",
    "# Plot mock results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Throughput\n",
    "pivot_throughput = df_results.pivot(index='sequence_length', columns='batch_size', values='throughput')\n",
    "sns.heatmap(pivot_throughput, annot=True, fmt='.1f', ax=axes[0], cmap='viridis')\n",
    "axes[0].set_title('Throughput (seq/sec)')\n",
    "\n",
    "# Latency\n",
    "pivot_latency = df_results.pivot(index='sequence_length', columns='batch_size', values='avg_time')\n",
    "sns.heatmap(pivot_latency, annot=True, fmt='.3f', ax=axes[1], cmap='plasma')\n",
    "axes[1].set_title('Latency (seconds)')\n",
    "\n",
    "# Memory\n",
    "pivot_memory = df_results.pivot(index='sequence_length', columns='batch_size', values='peak_memory_gb')\n",
    "sns.heatmap(pivot_memory, annot=True, fmt='.3f', ax=axes[2], cmap='Reds')\n",
    "axes[2].set_title('Memory (GB)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d06357",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis\n",
    "\n",
    "Statistical significance testing and confidence intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ee239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import bootstrap, mannwhitneyu, ttest_ind\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class StatisticalAnalysis:\n",
    "    \"\"\"Statistical analysis tools for model evaluation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compare_models(model1_scores, model2_scores, metric_name=\"accuracy\", alpha=0.05):\n",
    "        \"\"\"Compare two models statistically.\"\"\"\n",
    "        # Basic statistics\n",
    "        mean1, std1 = np.mean(model1_scores), np.std(model1_scores)\n",
    "        mean2, std2 = np.mean(model2_scores), np.std(model2_scores)\n",
    "\n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt(((len(model1_scores) - 1) * std1**2 +\n",
    "                              (len(model2_scores) - 1) * std2**2) /\n",
    "                             (len(model1_scores) + len(model2_scores) - 2))\n",
    "        cohens_d = (mean1 - mean2) / pooled_std\n",
    "\n",
    "        # Normality tests\n",
    "        _, p_norm1 = stats.shapiro(model1_scores)\n",
    "        _, p_norm2 = stats.shapiro(model2_scores)\n",
    "\n",
    "        # Choose appropriate test\n",
    "        if p_norm1 > alpha and p_norm2 > alpha:\n",
    "            # Both normal: use t-test\n",
    "            stat, p_value = ttest_ind(model1_scores, model2_scores)\n",
    "            test_used = \"t-test\"\n",
    "        else:\n",
    "            # Non-normal: use Mann-Whitney U\n",
    "            stat, p_value = mannwhitneyu(model1_scores, model2_scores, alternative='two-sided')\n",
    "            test_used = \"Mann-Whitney U\"\n",
    "\n",
    "        # Confidence intervals\n",
    "        ci1 = stats.t.interval(1-alpha, len(model1_scores)-1, mean1, stats.sem(model1_scores))\n",
    "        ci2 = stats.t.interval(1-alpha, len(model2_scores)-1, mean2, stats.sem(model2_scores))\n",
    "\n",
    "        return {\n",
    "            'model1_mean': mean1,\n",
    "            'model1_std': std1,\n",
    "            'model1_ci': ci1,\n",
    "            'model2_mean': mean2,\n",
    "            'model2_std': std2,\n",
    "            'model2_ci': ci2,\n",
    "            'effect_size': cohens_d,\n",
    "            'test_statistic': stat,\n",
    "            'p_value': p_value,\n",
    "            'test_used': test_used,\n",
    "            'significant': p_value < alpha\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def bootstrap_confidence_interval(data, statistic=np.mean, confidence_level=0.95, n_bootstrap=1000):\n",
    "        \"\"\"Calculate bootstrap confidence interval.\"\"\"\n",
    "        def bootstrap_statistic(x):\n",
    "            return statistic(x)\n",
    "\n",
    "        # Reshape data for scipy.stats.bootstrap\n",
    "        data_reshaped = (data,)\n",
    "\n",
    "        # Perform bootstrap\n",
    "        result = bootstrap(data_reshaped, bootstrap_statistic,\n",
    "                          n_resamples=n_bootstrap, confidence_level=confidence_level,\n",
    "                          random_state=42)\n",
    "\n",
    "        return {\n",
    "            'statistic': statistic(data),\n",
    "            'confidence_interval': result.confidence_interval,\n",
    "            'confidence_level': confidence_level\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def multiple_comparisons_correction(p_values, method='bonferroni'):\n",
    "        \"\"\"Apply multiple comparisons correction.\"\"\"\n",
    "        p_values = np.array(p_values)\n",
    "\n",
    "        if method == 'bonferroni':\n",
    "            corrected = p_values * len(p_values)\n",
    "            corrected = np.minimum(corrected, 1.0)\n",
    "        elif method == 'holm':\n",
    "            # Holm's step-down method\n",
    "            sorted_indices = np.argsort(p_values)\n",
    "            sorted_p = p_values[sorted_indices]\n",
    "            corrected = np.zeros_like(p_values)\n",
    "\n",
    "            for i, idx in enumerate(sorted_indices):\n",
    "                correction_factor = len(p_values) - i\n",
    "                corrected[idx] = min(1.0, sorted_p[i] * correction_factor)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        return corrected\n",
    "\n",
    "# Example statistical analysis\n",
    "print(\"ðŸ“Š Statistical Analysis Demo\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simulate model comparison data\n",
    "np.random.seed(42)\n",
    "model_a_scores = np.random.normal(0.85, 0.05, 30)  # Mean: 0.85, std: 0.05\n",
    "model_b_scores = np.random.normal(0.88, 0.04, 30)  # Mean: 0.88, std: 0.04\n",
    "model_c_scores = np.random.normal(0.87, 0.06, 30)  # Mean: 0.87, std: 0.06\n",
    "\n",
    "stats_analyzer = StatisticalAnalysis()\n",
    "\n",
    "# Compare Model A vs Model B\n",
    "comparison_ab = stats_analyzer.compare_models(model_a_scores, model_b_scores, \"F1-Score\")\n",
    "\n",
    "print(\"Model A vs Model B Comparison:\")\n",
    "print(f\"  Model A: {comparison_ab['model1_mean']:.4f} Â± {comparison_ab['model1_std']:.4f}\")\n",
    "print(f\"  Model B: {comparison_ab['model2_mean']:.4f} Â± {comparison_ab['model2_std']:.4f}\")\n",
    "print(f\"  Effect Size (Cohen's d): {comparison_ab['effect_size']:.4f}\")\n",
    "print(f\"  Test Used: {comparison_ab['test_used']}\")\n",
    "print(f\"  P-value: {comparison_ab['p_value']:.6f}\")\n",
    "print(f\"  Significant: {comparison_ab['significant']}\")\n",
    "\n",
    "# Bootstrap confidence intervals\n",
    "bootstrap_a = stats_analyzer.bootstrap_confidence_interval(model_a_scores)\n",
    "bootstrap_b = stats_analyzer.bootstrap_confidence_interval(model_b_scores)\n",
    "\n",
    "print(\"\\nBootstrap 95% CI:\")\n",
    "print(f\"  Model A: [{bootstrap_a['confidence_interval'].low:.4f}, {bootstrap_a['confidence_interval'].high:.4f}]\")\n",
    "print(f\"  Model B: [{bootstrap_b['confidence_interval'].low:.4f}, {bootstrap_b['confidence_interval'].high:.4f}]\")\n",
    "\n",
    "# Multiple comparisons\n",
    "comparisons = [\n",
    "    stats_analyzer.compare_models(model_a_scores, model_b_scores),\n",
    "    stats_analyzer.compare_models(model_a_scores, model_c_scores),\n",
    "    stats_analyzer.compare_models(model_b_scores, model_c_scores)\n",
    "]\n",
    "\n",
    "p_values = [comp['p_value'] for comp in comparisons]\n",
    "corrected_p = stats_analyzer.multiple_comparisons_correction(p_values, 'bonferroni')\n",
    "\n",
    "print(\"\\nMultiple Comparisons (Bonferroni correction):\")\n",
    "comparison_names = ['A vs B', 'A vs C', 'B vs C']\n",
    "for _i, (name, orig_p, corr_p) in enumerate(zip(comparison_names, p_values, corrected_p, strict=False)):\n",
    "    print(f\"  {name}: p={orig_p:.6f} â†’ corrected p={corr_p:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60032837",
   "metadata": {},
   "source": [
    "## 4. Evaluation Visualization\n",
    "\n",
    "Comprehensive visualization of evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationVisualizer:\n",
    "    \"\"\"Visualization tools for model evaluation.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, class_names=None, normalize=False):\n",
    "        \"\"\"Plot confusion matrix.\"\"\"\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            fmt = '.2f'\n",
    "        else:\n",
    "            fmt = 'd'\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n",
    "                    xticklabels=class_names, yticklabels=class_names)\n",
    "        plt.title('Confusion Matrix' + (' (Normalized)' if normalize else ''))\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curves(models_data, title=\"ROC Curves\"):\n",
    "        \"\"\"Plot ROC curves for multiple models.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        for model_name, (y_true, y_scores) in models_data.items():\n",
    "            fpr, tpr, _ = stats.roc_curve(y_true, y_scores)\n",
    "            auc = roc_auc_score(y_true, y_scores)\n",
    "            plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.3f})', linewidth=2)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_precision_recall_curves(models_data, title=\"Precision-Recall Curves\"):\n",
    "        \"\"\"Plot precision-recall curves for multiple models.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        for model_name, (y_true, y_scores) in models_data.items():\n",
    "            precision, recall, _ = stats.precision_recall_curve(y_true, y_scores)\n",
    "            ap = average_precision_score(y_true, y_scores)\n",
    "            plt.plot(recall, precision, label=f'{model_name} (AP = {ap:.3f})', linewidth=2)\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metric_comparison(models_metrics, metrics_to_plot=None):\n",
    "        \"\"\"Plot comparison of multiple metrics across models.\"\"\"\n",
    "        if metrics_to_plot is None:\n",
    "            metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "        df = pd.DataFrame(models_metrics).T\n",
    "        df = df[metrics_to_plot]\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Bar plot\n",
    "        df.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "        axes[0].set_title('Model Performance Comparison')\n",
    "        axes[0].set_ylabel('Score')\n",
    "        axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Radar plot\n",
    "        angles = np.linspace(0, 2*np.pi, len(metrics_to_plot), endpoint=False)\n",
    "        angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
    "\n",
    "        ax = axes[1]\n",
    "        ax.set_theta_offset(np.pi / 2)\n",
    "        ax.set_theta_direction(-1)\n",
    "        ax = plt.subplot(122, projection='polar')\n",
    "\n",
    "        for model_name, values in df.iterrows():\n",
    "            values_list = values.tolist()\n",
    "            values_list += [values_list[0]]  # Complete the circle\n",
    "            ax.plot(angles, values_list, 'o-', linewidth=2, label=model_name)\n",
    "            ax.fill(angles, values_list, alpha=0.25)\n",
    "\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(metrics_to_plot)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('Model Performance Radar')\n",
    "        ax.legend(bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example visualizations\n",
    "print(\"ðŸ“ˆ Evaluation Visualizations\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "visualizer = EvaluationVisualizer()\n",
    "\n",
    "# Simulate classification data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "y_true = np.random.choice([0, 1], size=n_samples, p=[0.6, 0.4])\n",
    "\n",
    "# Simulate three models with different performance\n",
    "models_data = {\n",
    "    'Hyena-GLT': (y_true, np.random.beta(2, 3, n_samples)),\n",
    "    'Transformer': (y_true, np.random.beta(1.8, 3.2, n_samples)),\n",
    "    'CNN': (y_true, np.random.beta(1.5, 4, n_samples))\n",
    "}\n",
    "\n",
    "# Plot ROC curves\n",
    "visualizer.plot_roc_curves(models_data, \"Model Comparison: ROC Curves\")\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "visualizer.plot_precision_recall_curves(models_data, \"Model Comparison: Precision-Recall\")\n",
    "\n",
    "# Confusion matrix for best model\n",
    "best_model_scores = models_data['Hyena-GLT'][1]\n",
    "y_pred = (best_model_scores > 0.3).astype(int)\n",
    "visualizer.plot_confusion_matrix(y_true, y_pred, ['Negative', 'Positive'], normalize=True)\n",
    "\n",
    "# Model metrics comparison\n",
    "models_metrics = {}\n",
    "for model_name, (y_true_model, y_scores_model) in models_data.items():\n",
    "    y_pred_model = (y_scores_model > 0.3).astype(int)\n",
    "    metrics = GenomicMetrics.sequence_classification_metrics(y_true_model, y_pred_model, y_scores_model)\n",
    "    models_metrics[model_name] = metrics\n",
    "\n",
    "visualizer.plot_metric_comparison(models_metrics)\n",
    "\n",
    "print(\"\\nâœ… All visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cf067e",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Evaluation Report\n",
    "\n",
    "Generate a complete evaluation report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationReport:\n",
    "    \"\"\"Generate comprehensive evaluation reports.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.report_data = {}\n",
    "\n",
    "    def add_model_results(self, model_name, metrics, benchmark_results=None):\n",
    "        \"\"\"Add results for a model.\"\"\"\n",
    "        self.report_data[model_name] = {\n",
    "            'metrics': metrics,\n",
    "            'benchmark': benchmark_results\n",
    "        }\n",
    "\n",
    "    def generate_summary(self):\n",
    "        \"\"\"Generate evaluation summary.\"\"\"\n",
    "        summary = {\n",
    "            'num_models': len(self.report_data),\n",
    "            'best_model': None,\n",
    "            'best_metric': 0,\n",
    "            'metric_comparison': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "\n",
    "        # Find best model (by F1 score)\n",
    "        for model_name, data in self.report_data.items():\n",
    "            f1_score = data['metrics'].get('f1', 0)\n",
    "            if f1_score > summary['best_metric']:\n",
    "                summary['best_metric'] = f1_score\n",
    "                summary['best_model'] = model_name\n",
    "\n",
    "        # Metric comparison\n",
    "        for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "            summary['metric_comparison'][metric] = {\n",
    "                model: data['metrics'].get(metric, 0)\n",
    "                for model, data in self.report_data.items()\n",
    "            }\n",
    "\n",
    "        # Generate recommendations\n",
    "        if summary['best_model']:\n",
    "            best_data = self.report_data[summary['best_model']]\n",
    "\n",
    "            if best_data['metrics'].get('precision', 0) > 0.9:\n",
    "                summary['recommendations'].append(\"High precision - good for critical applications\")\n",
    "\n",
    "            if best_data['metrics'].get('recall', 0) > 0.9:\n",
    "                summary['recommendations'].append(\"High recall - good for comprehensive detection\")\n",
    "\n",
    "            if best_data['metrics'].get('f1', 0) > 0.85:\n",
    "                summary['recommendations'].append(\"Balanced performance - suitable for production\")\n",
    "            else:\n",
    "                summary['recommendations'].append(\"Consider further optimization or data augmentation\")\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def print_report(self):\n",
    "        \"\"\"Print comprehensive evaluation report.\"\"\"\n",
    "        print(\"ðŸ” COMPREHENSIVE EVALUATION REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        summary = self.generate_summary()\n",
    "\n",
    "        print(\"\\nðŸ“Š SUMMARY\")\n",
    "        print(f\"  Models Evaluated: {summary['num_models']}\")\n",
    "        print(f\"  Best Model: {summary['best_model']} (F1: {summary['best_metric']:.4f})\")\n",
    "\n",
    "        print(\"\\nðŸ“ˆ DETAILED METRICS\")\n",
    "        for model_name, data in self.report_data.items():\n",
    "            print(f\"\\n  {model_name.upper()}:\")\n",
    "            for metric, value in data['metrics'].items():\n",
    "                print(f\"    {metric.title()}: {value:.4f}\")\n",
    "\n",
    "        print(\"\\nðŸŽ¯ RECOMMENDATIONS\")\n",
    "        for i, rec in enumerate(summary['recommendations'], 1):\n",
    "            print(f\"  {i}. {rec}\")\n",
    "\n",
    "        # Performance ranking\n",
    "        print(\"\\nðŸ† PERFORMANCE RANKING (by F1 Score)\")\n",
    "        ranked_models = sorted(\n",
    "            self.report_data.items(),\n",
    "            key=lambda x: x[1]['metrics'].get('f1', 0),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        for i, (model_name, data) in enumerate(ranked_models, 1):\n",
    "            f1 = data['metrics'].get('f1', 0)\n",
    "            acc = data['metrics'].get('accuracy', 0)\n",
    "            print(f\"  {i}. {model_name}: F1={f1:.4f}, Accuracy={acc:.4f}\")\n",
    "\n",
    "# Example evaluation report\n",
    "print(\"ðŸ“‹ Evaluation Report Generation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create report with simulated data\n",
    "report = EvaluationReport()\n",
    "\n",
    "# Add model results\n",
    "for model_name, (y_true_model, y_scores_model) in models_data.items():\n",
    "    y_pred_model = (y_scores_model > 0.3).astype(int)\n",
    "    metrics = GenomicMetrics.sequence_classification_metrics(y_true_model, y_pred_model, y_scores_model)\n",
    "    report.add_model_results(model_name, metrics)\n",
    "\n",
    "# Generate and print report\n",
    "report.print_report()\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete! Ready for model deployment decision.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
