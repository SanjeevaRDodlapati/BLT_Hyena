{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e61c30f",
   "metadata": {},
   "source": [
    "# Text and Sequence Generation with Hyena-GLT\n",
    "\n",
    "This notebook demonstrates various text and sequence generation techniques using the Hyena-GLT framework. We'll explore different sampling strategies, conditioning approaches, and generation quality metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e55888a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hyena_glt.models import HyenaGLT\n",
    "from hyena_glt.tokenizers import GenomicTokenizer\n",
    "from hyena_glt.utils import (\n",
    "    plot_attention_maps,\n",
    "    analyze_model_predictions,\n",
    "    validate_sequence\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e48b9",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2991873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GenomicTokenizer(\n",
    "    vocab_size=8192,\n",
    "    special_tokens=['<PAD>', '<UNK>', '<BOS>', '<EOS>', '<MASK>']\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'hidden_dim': 512,\n",
    "    'num_layers': 12,\n",
    "    'max_seq_len': 2048,\n",
    "    'hyena_order': 3,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "model = HyenaGLT(**model_config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4517a67",
   "metadata": {},
   "source": [
    "## Generation Strategies\n",
    "\n",
    "### 1. Greedy Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_generate(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 100,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using greedy decoding (always pick most likely next token).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Get model predictions\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs['logits']  # [batch_size, seq_len, vocab_size]\n",
    "            \n",
    "            # Get next token (greedy)\n",
    "            next_token_logits = logits[0, -1, :]  # Last position\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            # Add to sequence\n",
    "            generated_ids.append(next_token_id)\n",
    "            \n",
    "            # Update input for next iteration\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor,\n",
    "                torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "            ], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Test greedy generation\n",
    "prompt = \"ATGCGATCGAT\"\n",
    "generated_text = greedy_generate(model, tokenizer, prompt, max_length=50, device=device)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf3075",
   "metadata": {},
   "source": [
    "### 2. Nucleus (Top-p) Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sample(logits: torch.Tensor, p: float = 0.9) -> int:\n",
    "    \"\"\"\n",
    "    Sample from top-p (nucleus) distribution.\n",
    "    \"\"\"\n",
    "    # Sort logits in descending order\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    \n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    \n",
    "    # Find cutoff index\n",
    "    sorted_indices_to_remove = cumulative_probs > p\n",
    "    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "    sorted_indices_to_remove[0] = False\n",
    "    \n",
    "    # Set logits to -inf for tokens outside nucleus\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "    # Sample from filtered distribution\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "    return next_token.item()\n",
    "\n",
    "def nucleus_generate(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 100,\n",
    "    p: float = 0.9,\n",
    "    temperature: float = 1.0,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using nucleus (top-p) sampling.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs['logits'][0, -1, :] / temperature\n",
    "            \n",
    "            next_token_id = top_p_sample(logits, p)\n",
    "            \n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated_ids.append(next_token_id)\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor,\n",
    "                torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "            ], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Test nucleus sampling with different parameters\n",
    "prompt = \"ATGCGATCGAT\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for p_val in [0.5, 0.7, 0.9]:\n",
    "    for temp in [0.8, 1.0, 1.2]:\n",
    "        generated = nucleus_generate(\n",
    "            model, tokenizer, prompt, \n",
    "            max_length=30, p=p_val, temperature=temp, device=device\n",
    "        )\n",
    "        print(f\"p={p_val}, temp={temp}: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38860e73",
   "metadata": {},
   "source": [
    "### 3. Top-k Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d91ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(logits: torch.Tensor, k: int = 50) -> int:\n",
    "    \"\"\"\n",
    "    Sample from top-k distribution.\n",
    "    \"\"\"\n",
    "    # Get top-k logits and indices\n",
    "    top_k_logits, top_k_indices = torch.topk(logits, k)\n",
    "    \n",
    "    # Sample from top-k distribution\n",
    "    probs = F.softmax(top_k_logits, dim=-1)\n",
    "    next_token_idx = torch.multinomial(probs, num_samples=1)\n",
    "    next_token = top_k_indices[next_token_idx]\n",
    "    \n",
    "    return next_token.item()\n",
    "\n",
    "def top_k_generate(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 100,\n",
    "    k: int = 50,\n",
    "    temperature: float = 1.0,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate text using top-k sampling.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs['logits'][0, -1, :] / temperature\n",
    "            \n",
    "            next_token_id = top_k_sample(logits, k)\n",
    "            \n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated_ids.append(next_token_id)\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor,\n",
    "                torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "            ], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Test top-k sampling\n",
    "prompt = \"ATGCGATCGAT\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for k_val in [10, 25, 50]:\n",
    "    generated = top_k_generate(\n",
    "        model, tokenizer, prompt, \n",
    "        max_length=30, k=k_val, temperature=1.0, device=device\n",
    "    )\n",
    "    print(f\"k={k_val}: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8766ea",
   "metadata": {},
   "source": [
    "## Conditional Generation\n",
    "\n",
    "### 1. Sequence Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dab73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sequence(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    partial_sequence: str,\n",
    "    target_length: int,\n",
    "    method: str = 'nucleus',\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Complete a partial genomic sequence to target length.\n",
    "    \"\"\"\n",
    "    current_length = len(partial_sequence)\n",
    "    remaining_length = target_length - current_length\n",
    "    \n",
    "    if remaining_length <= 0:\n",
    "        return partial_sequence[:target_length]\n",
    "    \n",
    "    generation_methods = {\n",
    "        'greedy': greedy_generate,\n",
    "        'nucleus': nucleus_generate,\n",
    "        'top_k': top_k_generate\n",
    "    }\n",
    "    \n",
    "    generate_fn = generation_methods.get(method, nucleus_generate)\n",
    "    \n",
    "    completed = generate_fn(\n",
    "        model, tokenizer, partial_sequence,\n",
    "        max_length=remaining_length,\n",
    "        device=device,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return completed[:target_length]\n",
    "\n",
    "# Test sequence completion\n",
    "partial_sequences = [\n",
    "    \"ATGCGATCG\",\n",
    "    \"AAATTTGGGCCC\",\n",
    "    \"CGTACGTACGTA\"\n",
    "]\n",
    "\n",
    "target_length = 50\n",
    "\n",
    "print(\"Sequence Completion Examples:\\n\")\n",
    "for seq in partial_sequences:\n",
    "    completed = complete_sequence(\n",
    "        model, tokenizer, seq, target_length,\n",
    "        method='nucleus', p=0.8, temperature=0.9\n",
    "    )\n",
    "    print(f\"Partial:   {seq}\")\n",
    "    print(f\"Completed: {completed}\")\n",
    "    print(f\"Length:    {len(completed)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cedd21",
   "metadata": {},
   "source": [
    "### 2. Motif-Conditioned Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_motif(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    motif: str,\n",
    "    context_length: int = 100,\n",
    "    method: str = 'nucleus',\n",
    "    **kwargs\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate sequence containing a specific motif.\n",
    "    \"\"\"\n",
    "    # Generate prefix\n",
    "    prefix_length = context_length // 2\n",
    "    prefix_prompt = \"N\" * 10  # Generic start\n",
    "    \n",
    "    prefix = nucleus_generate(\n",
    "        model, tokenizer, prefix_prompt,\n",
    "        max_length=prefix_length, device=device, **kwargs\n",
    "    )\n",
    "    \n",
    "    # Insert motif\n",
    "    sequence_with_motif = prefix + motif\n",
    "    \n",
    "    # Generate suffix\n",
    "    suffix_length = context_length - len(sequence_with_motif)\n",
    "    if suffix_length > 0:\n",
    "        complete_sequence_result = nucleus_generate(\n",
    "            model, tokenizer, sequence_with_motif,\n",
    "            max_length=suffix_length, device=device, **kwargs\n",
    "        )\n",
    "        return complete_sequence_result\n",
    "    \n",
    "    return sequence_with_motif\n",
    "\n",
    "# Test motif-conditioned generation\n",
    "motifs = [\"TATAAA\", \"CCAAT\", \"GGGCGG\"]\n",
    "\n",
    "print(\"Motif-Conditioned Generation:\\n\")\n",
    "for motif in motifs:\n",
    "    generated = generate_with_motif(\n",
    "        model, tokenizer, motif,\n",
    "        context_length=80, p=0.8, temperature=0.9\n",
    "    )\n",
    "    print(f\"Motif: {motif}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(f\"Motif present: {motif in generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af1b23",
   "metadata": {},
   "source": [
    "## Generation Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ab9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_generation_quality(\n",
    "    generated_sequences: List[str],\n",
    "    reference_sequences: Optional[List[str]] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Assess quality of generated sequences.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Basic sequence statistics\n",
    "    lengths = [len(seq) for seq in generated_sequences]\n",
    "    metrics['avg_length'] = np.mean(lengths)\n",
    "    metrics['std_length'] = np.std(lengths)\n",
    "    \n",
    "    # Nucleotide composition\n",
    "    all_nucleotides = ''.join(generated_sequences)\n",
    "    total_chars = len(all_nucleotides)\n",
    "    \n",
    "    if total_chars > 0:\n",
    "        metrics['gc_content'] = (all_nucleotides.count('G') + all_nucleotides.count('C')) / total_chars\n",
    "        metrics['at_content'] = (all_nucleotides.count('A') + all_nucleotides.count('T')) / total_chars\n",
    "    \n",
    "    # Complexity (entropy)\n",
    "    entropies = []\n",
    "    for seq in generated_sequences:\n",
    "        if len(seq) > 0:\n",
    "            char_counts = {}\n",
    "            for char in seq:\n",
    "                char_counts[char] = char_counts.get(char, 0) + 1\n",
    "            \n",
    "            probs = np.array(list(char_counts.values())) / len(seq)\n",
    "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "            entropies.append(entropy)\n",
    "    \n",
    "    metrics['avg_entropy'] = np.mean(entropies) if entropies else 0\n",
    "    \n",
    "    # Repetitiveness (simple metric)\n",
    "    repetition_scores = []\n",
    "    for seq in generated_sequences:\n",
    "        if len(seq) >= 4:\n",
    "            # Count 4-mer repeats\n",
    "            kmers = {}\n",
    "            for i in range(len(seq) - 3):\n",
    "                kmer = seq[i:i+4]\n",
    "                kmers[kmer] = kmers.get(kmer, 0) + 1\n",
    "            \n",
    "            total_kmers = len(seq) - 3\n",
    "            unique_kmers = len(kmers)\n",
    "            repetition = 1 - (unique_kmers / total_kmers) if total_kmers > 0 else 0\n",
    "            repetition_scores.append(repetition)\n",
    "    \n",
    "    metrics['avg_repetition'] = np.mean(repetition_scores) if repetition_scores else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Generate multiple sequences for quality assessment\n",
    "prompt = \"ATGCGATCGAT\"\n",
    "num_sequences = 10\n",
    "generated_sequences = []\n",
    "\n",
    "print(\"Generating sequences for quality assessment...\")\n",
    "for i in tqdm(range(num_sequences)):\n",
    "    seq = nucleus_generate(\n",
    "        model, tokenizer, prompt,\n",
    "        max_length=100, p=0.8, temperature=1.0, device=device\n",
    "    )\n",
    "    generated_sequences.append(seq)\n",
    "\n",
    "# Assess quality\n",
    "quality_metrics = assess_generation_quality(generated_sequences)\n",
    "\n",
    "print(\"\\nGeneration Quality Metrics:\")\n",
    "for metric, value in quality_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3aba78",
   "metadata": {},
   "source": [
    "## Visualization of Generation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815db942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generation_analysis(generated_sequences: List[str]):\n",
    "    \"\"\"\n",
    "    Create visualizations for generation analysis.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Length distribution\n",
    "    lengths = [len(seq) for seq in generated_sequences]\n",
    "    axes[0, 0].hist(lengths, bins=20, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Sequence Length Distribution')\n",
    "    axes[0, 0].set_xlabel('Length')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # 2. Nucleotide composition\n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    compositions = []\n",
    "    \n",
    "    for nt in nucleotides:\n",
    "        total_count = sum(seq.count(nt) for seq in generated_sequences)\n",
    "        total_length = sum(len(seq) for seq in generated_sequences)\n",
    "        compositions.append(total_count / total_length if total_length > 0 else 0)\n",
    "    \n",
    "    axes[0, 1].bar(nucleotides, compositions, color=['red', 'blue', 'green', 'orange'])\n",
    "    axes[0, 1].set_title('Nucleotide Composition')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. GC content distribution\n",
    "    gc_contents = []\n",
    "    for seq in generated_sequences:\n",
    "        if len(seq) > 0:\n",
    "            gc_count = seq.count('G') + seq.count('C')\n",
    "            gc_content = gc_count / len(seq)\n",
    "            gc_contents.append(gc_content)\n",
    "    \n",
    "    axes[1, 0].hist(gc_contents, bins=15, alpha=0.7, color='lightgreen')\n",
    "    axes[1, 0].set_title('GC Content Distribution')\n",
    "    axes[1, 0].set_xlabel('GC Content')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # 4. Sequence complexity (entropy)\n",
    "    entropies = []\n",
    "    for seq in generated_sequences:\n",
    "        if len(seq) > 0:\n",
    "            char_counts = {}\n",
    "            for char in seq:\n",
    "                char_counts[char] = char_counts.get(char, 0) + 1\n",
    "            \n",
    "            probs = np.array(list(char_counts.values())) / len(seq)\n",
    "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "            entropies.append(entropy)\n",
    "    \n",
    "    axes[1, 1].hist(entropies, bins=15, alpha=0.7, color='coral')\n",
    "    axes[1, 1].set_title('Sequence Complexity (Entropy)')\n",
    "    axes[1, 1].set_xlabel('Entropy (bits)')\n",
    "    axes[1, 1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "plot_generation_analysis(generated_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae731732",
   "metadata": {},
   "source": [
    "## Advanced Generation Techniques\n",
    "\n",
    "### 1. Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_generate(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    prompt: str,\n",
    "    max_length: int = 50,\n",
    "    beam_size: int = 5,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Generate text using beam search.\n",
    "    Returns list of (sequence, score) tuples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Initialize beams: (sequence, score)\n",
    "    beams = [(input_ids, 0.0)]\n",
    "    completed_beams = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step in range(max_length):\n",
    "            candidates = []\n",
    "            \n",
    "            for sequence, score in beams:\n",
    "                if len(sequence) >= max_length or sequence[-1] == tokenizer.eos_token_id:\n",
    "                    completed_beams.append((sequence, score))\n",
    "                    continue\n",
    "                \n",
    "                # Get model predictions\n",
    "                input_tensor = torch.tensor([sequence], dtype=torch.long).to(device)\n",
    "                outputs = model(input_tensor)\n",
    "                logits = outputs['logits'][0, -1, :]\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "                \n",
    "                # Get top-k candidates\n",
    "                top_log_probs, top_indices = torch.topk(log_probs, beam_size)\n",
    "                \n",
    "                for i in range(beam_size):\n",
    "                    new_sequence = sequence + [top_indices[i].item()]\n",
    "                    new_score = score + top_log_probs[i].item()\n",
    "                    candidates.append((new_sequence, new_score))\n",
    "            \n",
    "            # Select top beam_size candidates\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            beams = candidates[:beam_size]\n",
    "            \n",
    "            if not beams:\n",
    "                break\n",
    "    \n",
    "    # Add remaining beams to completed\n",
    "    completed_beams.extend(beams)\n",
    "    \n",
    "    # Sort by score and decode\n",
    "    completed_beams.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for sequence, score in completed_beams[:beam_size]:\n",
    "        decoded = tokenizer.decode(sequence)\n",
    "        results.append((decoded, score))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test beam search\n",
    "prompt = \"ATGCGATCGAT\"\n",
    "beam_results = beam_search_generate(\n",
    "    model, tokenizer, prompt,\n",
    "    max_length=30, beam_size=5, device=device\n",
    ")\n",
    "\n",
    "print(f\"Beam Search Results for prompt: {prompt}\\n\")\n",
    "for i, (sequence, score) in enumerate(beam_results):\n",
    "    print(f\"Beam {i+1} (score: {score:.3f}): {sequence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9ad676",
   "metadata": {},
   "source": [
    "### 2. Constrained Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrained_generate(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    prompt: str,\n",
    "    constraints: Dict[str, any],\n",
    "    max_length: int = 100,\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate sequence with constraints.\n",
    "    \n",
    "    Constraints can include:\n",
    "    - 'forbidden_patterns': List of patterns to avoid\n",
    "    - 'required_patterns': List of patterns that must be included\n",
    "    - 'gc_content_range': (min, max) GC content\n",
    "    - 'max_repeats': Maximum consecutive repeats of same nucleotide\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.copy()\n",
    "    \n",
    "    forbidden_patterns = constraints.get('forbidden_patterns', [])\n",
    "    required_patterns = constraints.get('required_patterns', [])\n",
    "    gc_range = constraints.get('gc_content_range', (0.0, 1.0))\n",
    "    max_repeats = constraints.get('max_repeats', float('inf'))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(input_tensor)\n",
    "            logits = outputs['logits'][0, -1, :]\n",
    "            \n",
    "            # Apply constraints by modifying logits\n",
    "            for token_id in range(len(logits)):\n",
    "                token = tokenizer.decode([token_id])\n",
    "                candidate_sequence = tokenizer.decode(generated_ids + [token_id])\n",
    "                \n",
    "                # Check forbidden patterns\n",
    "                valid = True\n",
    "                for pattern in forbidden_patterns:\n",
    "                    if pattern in candidate_sequence:\n",
    "                        logits[token_id] = float('-inf')\n",
    "                        valid = False\n",
    "                        break\n",
    "                \n",
    "                if not valid:\n",
    "                    continue\n",
    "                \n",
    "                # Check repeat constraint\n",
    "                if len(generated_ids) > 0:\n",
    "                    last_token = tokenizer.decode([generated_ids[-1]])\n",
    "                    if token == last_token:\n",
    "                        # Count consecutive repeats\n",
    "                        repeat_count = 1\n",
    "                        for i in range(len(generated_ids) - 2, -1, -1):\n",
    "                            if tokenizer.decode([generated_ids[i]]) == token:\n",
    "                                repeat_count += 1\n",
    "                            else:\n",
    "                                break\n",
    "                        \n",
    "                        if repeat_count >= max_repeats:\n",
    "                            logits[token_id] = float('-inf')\n",
    "                            continue\n",
    "            \n",
    "            # Sample from constrained distribution\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "            \n",
    "            generated_ids.append(next_token_id)\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor,\n",
    "                torch.tensor([[next_token_id]], dtype=torch.long).to(device)\n",
    "            ], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(generated_ids)\n",
    "\n",
    "# Test constrained generation\n",
    "prompt = \"ATGCGATCGAT\"\n",
    "constraints = {\n",
    "    'forbidden_patterns': ['AAAA', 'TTTT'],\n",
    "    'max_repeats': 3\n",
    "}\n",
    "\n",
    "constrained_seq = constrained_generate(\n",
    "    model, tokenizer, prompt, constraints,\n",
    "    max_length=50, device=device\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Constraints: {constraints}\")\n",
    "print(f\"Generated: {constrained_seq}\")\n",
    "\n",
    "# Verify constraints\n",
    "print(\"\\nConstraint Verification:\")\n",
    "print(f\"Contains AAAA: {'AAAA' in constrained_seq}\")\n",
    "print(f\"Contains TTTT: {'TTTT' in constrained_seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927965d",
   "metadata": {},
   "source": [
    "## Generation Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb763424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generation_methods(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer: GenomicTokenizer,\n",
    "    prompts: List[str],\n",
    "    methods: List[str],\n",
    "    device: torch.device = torch.device('cpu')\n",
    ") -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Compare different generation methods.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        method_results = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            if method == 'greedy':\n",
    "                generated = greedy_generate(\n",
    "                    model, tokenizer, prompt, max_length=50, device=device\n",
    "                )\n",
    "            elif method == 'nucleus':\n",
    "                generated = nucleus_generate(\n",
    "                    model, tokenizer, prompt, max_length=50,\n",
    "                    p=0.8, temperature=1.0, device=device\n",
    "                )\n",
    "            elif method == 'top_k':\n",
    "                generated = top_k_generate(\n",
    "                    model, tokenizer, prompt, max_length=50,\n",
    "                    k=50, temperature=1.0, device=device\n",
    "                )\n",
    "            \n",
    "            method_results.append(generated)\n",
    "        \n",
    "        # Compute metrics for this method\n",
    "        metrics = assess_generation_quality(method_results)\n",
    "        results[method] = {\n",
    "            'sequences': method_results,\n",
    "            'metrics': metrics\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare generation methods\n",
    "test_prompts = [\n",
    "    \"ATGCGATCGAT\",\n",
    "    \"AAATTTGGGCCC\",\n",
    "    \"CGTACGTACGTA\",\n",
    "    \"TATATATATAT\",\n",
    "    \"GCGCGCGCGCG\"\n",
    "]\n",
    "\n",
    "methods_to_compare = ['greedy', 'nucleus', 'top_k']\n",
    "\n",
    "print(\"Comparing generation methods...\")\n",
    "comparison_results = compare_generation_methods(\n",
    "    model, tokenizer, test_prompts, methods_to_compare, device\n",
    ")\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\nGeneration Method Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for method, data in comparison_results.items():\n",
    "    print(f\"\\n{method.upper()} METHOD:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    metrics = data['metrics']\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nSample generations:\")\n",
    "    for i, seq in enumerate(data['sequences'][:3]):\n",
    "        print(f\"  {i+1}. {seq[:60]}...\" if len(seq) > 60 else f\"  {i+1}. {seq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25df76",
   "metadata": {},
   "source": [
    "## Interactive Generation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_generation_demo():\n",
    "    \"\"\"\n",
    "    Interactive demo for trying different generation parameters.\n",
    "    \"\"\"\n",
    "    print(\"Interactive Generation Demo\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Default parameters\n",
    "    params = {\n",
    "        'prompt': 'ATGCGATCGAT',\n",
    "        'method': 'nucleus',\n",
    "        'max_length': 50,\n",
    "        'temperature': 1.0,\n",
    "        'p': 0.8,\n",
    "        'k': 50\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nCurrent parameters:\")\n",
    "    for key, value in params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Generate with current parameters\n",
    "    if params['method'] == 'nucleus':\n",
    "        generated = nucleus_generate(\n",
    "            model, tokenizer, params['prompt'],\n",
    "            max_length=params['max_length'],\n",
    "            p=params['p'],\n",
    "            temperature=params['temperature'],\n",
    "            device=device\n",
    "        )\n",
    "    elif params['method'] == 'top_k':\n",
    "        generated = top_k_generate(\n",
    "            model, tokenizer, params['prompt'],\n",
    "            max_length=params['max_length'],\n",
    "            k=params['k'],\n",
    "            temperature=params['temperature'],\n",
    "            device=device\n",
    "        )\n",
    "    else:  # greedy\n",
    "        generated = greedy_generate(\n",
    "            model, tokenizer, params['prompt'],\n",
    "            max_length=params['max_length'],\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nGenerated sequence:\")\n",
    "    print(f\"  {generated}\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    quality = assess_generation_quality([generated])\n",
    "    print(f\"\\nQuality metrics:\")\n",
    "    for metric, value in quality.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Run interactive demo\n",
    "interactive_generation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a226d98",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Method Selection**: Choose generation method based on requirements:\n",
    "   - **Greedy**: Fast, deterministic, but may be repetitive\n",
    "   - **Nucleus (Top-p)**: Good balance of quality and diversity\n",
    "   - **Top-k**: Controllable diversity\n",
    "   - **Beam Search**: Higher quality but computationally expensive\n",
    "\n",
    "2. **Parameter Tuning**:\n",
    "   - Lower temperature (0.6-0.8) for more focused generation\n",
    "   - Higher temperature (1.0-1.2) for more diverse generation\n",
    "   - p=0.8-0.9 for nucleus sampling\n",
    "   - k=25-50 for top-k sampling\n",
    "\n",
    "3. **Quality Assessment**:\n",
    "   - Monitor GC content for biological realism\n",
    "   - Check sequence complexity (entropy)\n",
    "   - Avoid excessive repetition\n",
    "   - Validate against known biological constraints\n",
    "\n",
    "4. **Constraints and Control**:\n",
    "   - Use constrained generation for specific requirements\n",
    "   - Implement post-processing filters\n",
    "   - Consider domain-specific constraints\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - Compare multiple generation methods\n",
    "   - Use diverse evaluation metrics\n",
    "   - Consider downstream task performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e1b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generation Tutorial Complete!\")\n",
    "print(\"\\nThis notebook covered:\")\n",
    "print(\"- Greedy, nucleus, and top-k sampling\")\n",
    "print(\"- Beam search generation\")\n",
    "print(\"- Constrained generation\")\n",
    "print(\"- Quality assessment and comparison\")\n",
    "print(\"- Interactive generation demo\")\n",
    "print(\"\\nFor more advanced techniques, see the Hyena-GLT documentation.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
