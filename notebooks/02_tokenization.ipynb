{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e64fd642",
   "metadata": {},
   "source": [
    "# Tutorial 02: Tokenization Deep Dive\n",
    "\n",
    "This notebook provides an in-depth exploration of the tokenization methods used in the Hyena-GLT framework, focusing on BLT (Byte Latent Transformer) tokenization and specialized genomic tokenizers.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand BLT tokenization principles and advantages\n",
    "- Compare different genomic tokenizers (DNA, RNA, Protein)\n",
    "- Explore tokenization strategies for different sequence types\n",
    "- Analyze tokenization efficiency and compression ratios\n",
    "- Implement custom tokenization workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from hyena_glt.tokenizers import (\n",
    "    BLTTokenizer,\n",
    "    DNATokenizer,\n",
    "    ProteinTokenizer,\n",
    "    RNATokenizer,\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Hyena-GLT Tokenization Deep Dive Tutorial\")\n",
    "print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b3f40",
   "metadata": {},
   "source": [
    "## 1. BLT Tokenization Overview\n",
    "\n",
    "BLT (Byte Latent Transformer) tokenization offers several advantages for genomic sequences:\n",
    "- **Efficiency**: Operates directly on byte representations\n",
    "- **Universality**: Handles any sequence type without vocabulary limitations\n",
    "- **Compression**: Learns optimal byte-level representations\n",
    "- **Flexibility**: Adaptable to different genomic data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BLT tokenizer\n",
    "blt_tokenizer = BLTTokenizer(vocab_size=8192, latent_dim=256)\n",
    "\n",
    "# Create sample genomic sequences\n",
    "dna_sequences = [\n",
    "    \"ATCGATCGATCGATCGATCGATCGATCG\",\n",
    "    \"GCTAGCTAGCTAGCTAGCTAGCTAGCTA\",\n",
    "    \"TTAACCGGTTAACCGGTTAACCGGTTAA\",\n",
    "    \"CGATCGATCGATCGATCGATCGATCGAT\"\n",
    "]\n",
    "\n",
    "print(\"Sample DNA sequences:\")\n",
    "for i, seq in enumerate(dna_sequences[:2]):\n",
    "    print(f\"Sequence {i+1}: {seq}\")\n",
    "\n",
    "# Demonstrate BLT tokenization\n",
    "print(\"\\n=== BLT Tokenization ===\")\n",
    "for i, sequence in enumerate(dna_sequences[:2]):\n",
    "    # Encode sequence\n",
    "    tokens = blt_tokenizer.encode(sequence)\n",
    "\n",
    "    # Decode back to verify\n",
    "    decoded = blt_tokenizer.decode(tokens)\n",
    "\n",
    "    print(f\"\\nSequence {i+1}:\")\n",
    "    print(f\"Original:  {sequence}\")\n",
    "    print(f\"Tokens:    {tokens[:10]}... ({len(tokens)} total)\")\n",
    "    print(f\"Decoded:   {decoded}\")\n",
    "    print(f\"Match:     {sequence == decoded}\")\n",
    "    print(f\"Compression: {len(sequence)}/{len(tokens)} = {len(sequence)/len(tokens):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7d9aa",
   "metadata": {},
   "source": [
    "## 2. Specialized Genomic Tokenizers\n",
    "\n",
    "Different genomic sequence types benefit from specialized tokenization approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize specialized tokenizers\n",
    "dna_tokenizer = DNATokenizer()\n",
    "rna_tokenizer = RNATokenizer()\n",
    "protein_tokenizer = ProteinTokenizer()\n",
    "\n",
    "# Sample sequences for each type\n",
    "sample_sequences = {\n",
    "    'DNA': \"ATCGATCGATCGATCGATCGATCGATCG\",\n",
    "    'RNA': \"AUCGAUCGAUCGAUCGAUCGAUCGAUCG\",\n",
    "    'Protein': \"MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG\"\n",
    "}\n",
    "\n",
    "tokenizers = {\n",
    "    'DNA': dna_tokenizer,\n",
    "    'RNA': rna_tokenizer,\n",
    "    'Protein': protein_tokenizer\n",
    "}\n",
    "\n",
    "print(\"=== Specialized Tokenizer Comparison ===\")\n",
    "tokenization_results = {}\n",
    "\n",
    "for seq_type, sequence in sample_sequences.items():\n",
    "    tokenizer = tokenizers[seq_type]\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.encode(sequence)\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "\n",
    "    # Store results\n",
    "    tokenization_results[seq_type] = {\n",
    "        'original_length': len(sequence),\n",
    "        'token_count': len(tokens),\n",
    "        'vocab_size': tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'N/A',\n",
    "        'compression_ratio': len(sequence) / len(tokens)\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{seq_type} Tokenization:\")\n",
    "    print(f\"  Original:  {sequence[:50]}{'...' if len(sequence) > 50 else ''}\")\n",
    "    print(f\"  Tokens:    {tokens[:10]}... ({len(tokens)} total)\")\n",
    "    print(f\"  Vocab size: {tokenization_results[seq_type]['vocab_size']}\")\n",
    "    print(f\"  Compression: {tokenization_results[seq_type]['compression_ratio']:.2f}x\")\n",
    "    print(f\"  Perfect reconstruction: {sequence == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867a487",
   "metadata": {},
   "source": [
    "## 3. Tokenization Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "def benchmark_tokenizer(tokenizer, sequences, tokenizer_name):\n",
    "    \"\"\"Benchmark tokenization performance.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    total_tokens = 0\n",
    "    total_chars = 0\n",
    "\n",
    "    for sequence in sequences:\n",
    "        tokens = tokenizer.encode(sequence)\n",
    "        total_tokens += len(tokens)\n",
    "        total_chars += len(sequence)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return {\n",
    "        'name': tokenizer_name,\n",
    "        'time': end_time - start_time,\n",
    "        'total_chars': total_chars,\n",
    "        'total_tokens': total_tokens,\n",
    "        'compression_ratio': total_chars / total_tokens,\n",
    "        'chars_per_second': total_chars / (end_time - start_time)\n",
    "    }\n",
    "\n",
    "# Generate longer test sequences\n",
    "test_sequences = []\n",
    "for _ in range(100):\n",
    "    seq = ''.join(np.random.choice(['A', 'T', 'C', 'G'], size=1000))\n",
    "    test_sequences.append(seq)\n",
    "\n",
    "# Benchmark all tokenizers\n",
    "benchmark_results = []\n",
    "benchmark_results.append(benchmark_tokenizer(blt_tokenizer, test_sequences, 'BLT'))\n",
    "benchmark_results.append(benchmark_tokenizer(dna_tokenizer, test_sequences, 'DNA'))\n",
    "\n",
    "print(\"=== Tokenization Performance Benchmark ===\")\n",
    "print(f\"Test data: {len(test_sequences)} sequences, {benchmark_results[0]['total_chars']:,} total characters\")\n",
    "print()\n",
    "\n",
    "for result in benchmark_results:\n",
    "    print(f\"{result['name']} Tokenizer:\")\n",
    "    print(f\"  Time: {result['time']:.3f}s\")\n",
    "    print(f\"  Tokens generated: {result['total_tokens']:,}\")\n",
    "    print(f\"  Compression ratio: {result['compression_ratio']:.2f}x\")\n",
    "    print(f\"  Speed: {result['chars_per_second']:,.0f} chars/sec\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea1903e",
   "metadata": {},
   "source": [
    "## 4. Visualization of Tokenization Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3175c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token distribution patterns\n",
    "def analyze_token_patterns(tokenizer, sequences, tokenizer_name):\n",
    "    \"\"\"Analyze and visualize token distribution patterns.\"\"\"\n",
    "    all_tokens = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        tokens = tokenizer.encode(sequence)\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "    # Count token frequencies\n",
    "    token_counts = Counter(all_tokens)\n",
    "\n",
    "    return {\n",
    "        'name': tokenizer_name,\n",
    "        'tokens': all_tokens,\n",
    "        'unique_tokens': len(token_counts),\n",
    "        'most_common': token_counts.most_common(10),\n",
    "        'token_counts': token_counts\n",
    "    }\n",
    "\n",
    "# Analyze patterns for different tokenizers\n",
    "pattern_analyses = []\n",
    "pattern_analyses.append(analyze_token_patterns(blt_tokenizer, test_sequences[:10], 'BLT'))\n",
    "pattern_analyses.append(analyze_token_patterns(dna_tokenizer, test_sequences[:10], 'DNA'))\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Tokenization Pattern Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Token frequency distributions\n",
    "for i, analysis in enumerate(pattern_analyses):\n",
    "    # Most common tokens\n",
    "    tokens, counts = zip(*analysis['most_common'], strict=False)\n",
    "    axes[i, 0].bar(range(len(tokens)), counts)\n",
    "    axes[i, 0].set_title(f'{analysis[\"name\"]} - Most Common Tokens')\n",
    "    axes[i, 0].set_xlabel('Token Rank')\n",
    "    axes[i, 0].set_ylabel('Frequency')\n",
    "\n",
    "    # Token distribution histogram\n",
    "    all_counts = list(analysis['token_counts'].values())\n",
    "    axes[i, 1].hist(all_counts, bins=20, alpha=0.7)\n",
    "    axes[i, 1].set_title(f'{analysis[\"name\"]} - Token Frequency Distribution')\n",
    "    axes[i, 1].set_xlabel('Token Frequency')\n",
    "    axes[i, 1].set_ylabel('Number of Tokens')\n",
    "    axes[i, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"=== Token Pattern Analysis Summary ===\")\n",
    "for analysis in pattern_analyses:\n",
    "    print(f\"\\n{analysis['name']} Tokenizer:\")\n",
    "    print(f\"  Total tokens: {len(analysis['tokens']):,}\")\n",
    "    print(f\"  Unique tokens: {analysis['unique_tokens']:,}\")\n",
    "    print(f\"  Token diversity: {analysis['unique_tokens']/len(analysis['tokens']):.3f}\")\n",
    "    print(f\"  Most common token appears {analysis['most_common'][0][1]} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970f2f3",
   "metadata": {},
   "source": [
    "## 5. Advanced Tokenization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b13d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate context-aware tokenization\n",
    "def sliding_window_tokenization(tokenizer, sequence, window_size=100, overlap=20):\n",
    "    \"\"\"Tokenize sequence using sliding windows for long sequences.\"\"\"\n",
    "    windows = []\n",
    "    token_windows = []\n",
    "\n",
    "    for start in range(0, len(sequence) - overlap, window_size - overlap):\n",
    "        end = min(start + window_size, len(sequence))\n",
    "        window = sequence[start:end]\n",
    "        tokens = tokenizer.encode(window)\n",
    "\n",
    "        windows.append(window)\n",
    "        token_windows.append(tokens)\n",
    "\n",
    "    return windows, token_windows\n",
    "\n",
    "# Create a long test sequence\n",
    "long_sequence = ''.join(np.random.choice(['A', 'T', 'C', 'G'], size=2000))\n",
    "\n",
    "print(\"=== Sliding Window Tokenization ===\")\n",
    "print(f\"Long sequence length: {len(long_sequence)} characters\")\n",
    "\n",
    "# Apply sliding window tokenization\n",
    "windows, token_windows = sliding_window_tokenization(blt_tokenizer, long_sequence)\n",
    "\n",
    "print(f\"Generated {len(windows)} windows\")\n",
    "print(f\"Window sizes: {[len(w) for w in windows[:5]]}...\")\n",
    "print(f\"Token counts per window: {[len(tw) for tw in token_windows[:5]]}...\")\n",
    "\n",
    "# Visualize tokenization across windows\n",
    "window_lengths = [len(w) for w in windows]\n",
    "token_counts = [len(tw) for tw in token_windows]\n",
    "compression_ratios = [wl/tc for wl, tc in zip(window_lengths, token_counts, strict=False)]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(range(len(windows)), window_lengths, 'b-', label='Characters', alpha=0.7)\n",
    "ax1.plot(range(len(windows)), token_counts, 'r-', label='Tokens', alpha=0.7)\n",
    "ax1.set_xlabel('Window Index')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Characters vs Tokens per Window')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(range(len(windows)), compression_ratios, 'g-', alpha=0.7)\n",
    "ax2.set_xlabel('Window Index')\n",
    "ax2.set_ylabel('Compression Ratio')\n",
    "ax2.set_title('Compression Ratio per Window')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average compression ratio: {np.mean(compression_ratios):.2f}x\")\n",
    "print(f\"Compression ratio std: {np.std(compression_ratios):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a29bae",
   "metadata": {},
   "source": [
    "## 6. Custom Tokenization Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a custom tokenization workflow\n",
    "class AdaptiveTokenizer:\n",
    "    \"\"\"Adaptive tokenizer that chooses the best tokenizer based on sequence characteristics.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizers = {\n",
    "            'blt': BLTTokenizer(vocab_size=4096),\n",
    "            'dna': DNATokenizer(),\n",
    "            'rna': RNATokenizer(),\n",
    "            'protein': ProteinTokenizer()\n",
    "        }\n",
    "\n",
    "    def detect_sequence_type(self, sequence):\n",
    "        \"\"\"Detect the most likely sequence type.\"\"\"\n",
    "        sequence = sequence.upper()\n",
    "\n",
    "        # Count character types\n",
    "        dna_chars = set('ATCG')\n",
    "        rna_chars = set('AUCG')\n",
    "        protein_chars = set('ACDEFGHIKLMNPQRSTVWY')\n",
    "\n",
    "        seq_chars = set(sequence)\n",
    "\n",
    "        # Calculate overlap scores\n",
    "        dna_score = len(seq_chars.intersection(dna_chars)) / len(seq_chars) if seq_chars else 0\n",
    "        rna_score = len(seq_chars.intersection(rna_chars)) / len(seq_chars) if seq_chars else 0\n",
    "        protein_score = len(seq_chars.intersection(protein_chars)) / len(seq_chars) if seq_chars else 0\n",
    "\n",
    "        # Specific checks\n",
    "        has_T = 'T' in seq_chars\n",
    "        has_U = 'U' in seq_chars\n",
    "\n",
    "        if has_U and not has_T and rna_score > 0.9:\n",
    "            return 'rna'\n",
    "        elif has_T and not has_U and dna_score > 0.9:\n",
    "            return 'dna'\n",
    "        elif protein_score > 0.8 and len(seq_chars) > 4:\n",
    "            return 'protein'\n",
    "        else:\n",
    "            return 'blt'  # Default to BLT for ambiguous cases\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        \"\"\"Adaptively encode sequence using the most appropriate tokenizer.\"\"\"\n",
    "        seq_type = self.detect_sequence_type(sequence)\n",
    "        tokenizer = self.tokenizers[seq_type]\n",
    "        return tokenizer.encode(sequence), seq_type\n",
    "\n",
    "    def decode(self, tokens, seq_type):\n",
    "        \"\"\"Decode tokens using the specified tokenizer type.\"\"\"\n",
    "        tokenizer = self.tokenizers[seq_type]\n",
    "        return tokenizer.decode(tokens)\n",
    "\n",
    "# Test adaptive tokenizer\n",
    "adaptive_tokenizer = AdaptiveTokenizer()\n",
    "\n",
    "test_sequences_adaptive = {\n",
    "    'DNA': \"ATCGATCGATCGATCG\",\n",
    "    'RNA': \"AUCGAUCGAUCGAUCG\",\n",
    "    'Protein': \"MKTVRQERLKSIVRIL\",\n",
    "    'Mixed': \"ATCGXYZQWERTY123\"\n",
    "}\n",
    "\n",
    "print(\"=== Adaptive Tokenization Results ===\")\n",
    "for label, sequence in test_sequences_adaptive.items():\n",
    "    tokens, detected_type = adaptive_tokenizer.encode(sequence)\n",
    "    decoded = adaptive_tokenizer.decode(tokens, detected_type)\n",
    "\n",
    "    print(f\"\\n{label} sequence:\")\n",
    "    print(f\"  Input: {sequence}\")\n",
    "    print(f\"  Detected type: {detected_type}\")\n",
    "    print(f\"  Tokens: {len(tokens)} tokens\")\n",
    "    print(f\"  Decoded: {decoded}\")\n",
    "    print(f\"  Perfect match: {sequence == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f29491",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Tokenization Best Practices ===\")\n",
    "print()\n",
    "\n",
    "recommendations = {\n",
    "    \"Sequence Type Selection\": [\n",
    "        \"Use DNATokenizer for pure DNA sequences (A, T, C, G only)\",\n",
    "        \"Use RNATokenizer for RNA sequences (A, U, C, G)\",\n",
    "        \"Use ProteinTokenizer for amino acid sequences\",\n",
    "        \"Use BLTTokenizer for mixed or unknown sequence types\"\n",
    "    ],\n",
    "    \"Performance Optimization\": [\n",
    "        \"Batch process multiple sequences for better efficiency\",\n",
    "        \"Use sliding windows for very long sequences (>10k bp)\",\n",
    "        \"Consider caching tokenized sequences for repeated use\",\n",
    "        \"Profile different tokenizers for your specific use case\"\n",
    "    ],\n",
    "    \"Memory Management\": [\n",
    "        \"Process sequences in chunks for large datasets\",\n",
    "        \"Clear tokenizer caches periodically\",\n",
    "        \"Use appropriate vocab_size for BLT tokenizers\",\n",
    "        \"Monitor memory usage during batch processing\"\n",
    "    ],\n",
    "    \"Quality Assurance\": [\n",
    "        \"Always verify encode/decode round-trip accuracy\",\n",
    "        \"Test tokenizers on representative data samples\",\n",
    "        \"Monitor compression ratios for efficiency\",\n",
    "        \"Validate tokenizer output before model training\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in recommendations.items():\n",
    "    print(f\"**{category}:**\")\n",
    "    for practice in practices:\n",
    "        print(f\"  â€¢ {practice}\")\n",
    "    print()\n",
    "\n",
    "# Summary statistics from this tutorial\n",
    "print(\"=== Tutorial Summary ===\")\n",
    "print(f\"Tokenizers demonstrated: {len(tokenizers) + 1} (including BLT)\")\n",
    "print(f\"Test sequences processed: {len(test_sequences) + len(test_sequences_adaptive)}\")\n",
    "print(f\"Performance benchmarks: {len(benchmark_results)}\")\n",
    "print(\"Visualization plots: 4\")\n",
    "print(\"Advanced techniques: Sliding windows, Adaptive tokenization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916990f0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial provided a comprehensive exploration of tokenization in the Hyena-GLT framework. Key takeaways:\n",
    "\n",
    "1. **BLT Tokenization** offers universal applicability and good compression\n",
    "2. **Specialized Tokenizers** provide optimized performance for specific sequence types\n",
    "3. **Performance Analysis** helps choose the right tokenizer for your use case\n",
    "4. **Advanced Strategies** like sliding windows handle long sequences effectively\n",
    "5. **Adaptive Approaches** can automatically select the best tokenization method\n",
    "\n",
    "In the next tutorial, we'll explore the Striped Hyena architecture and how it processes tokenized genomic sequences."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
