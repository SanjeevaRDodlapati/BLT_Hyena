{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de31fe3",
   "metadata": {},
   "source": [
    "# Model Architecture in Hyena-GLT\n",
    "\n",
    "This notebook provides an in-depth exploration of the Hyena-GLT architecture, combining Byte Latent Transformer (BLT) tokenization with Striped Hyena blocks for efficient genomic sequence modeling.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Architecture Overview](#architecture-overview)\n",
    "2. [BLT Tokenization Component](#blt-tokenization)\n",
    "3. [Hyena Blocks](#hyena-blocks)\n",
    "4. [Model Configuration](#model-configuration)\n",
    "5. [Forward Pass Analysis](#forward-pass)\n",
    "6. [Parameter Efficiency](#parameter-efficiency)\n",
    "7. [Scalability Analysis](#scalability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d77c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path().absolute().parent.parent))\n",
    "\n",
    "from hyena_glt.models.hyena_glt import HyenaGLT, HyenaGLTConfig\n",
    "from hyena_glt.utils.model_utils import count_parameters\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Hyena-GLT Model Architecture Tutorial\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c9f88",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "Hyena-GLT combines two key innovations:\n",
    "- **BLT (Byte Latent Transformer)**: Efficient tokenization that maps variable-length sequences to fixed-size latent representations\n",
    "- **Striped Hyena**: Long-range attention alternative using convolutions and gating mechanisms\n",
    "\n",
    "### Key Components:\n",
    "1. **Tokenization Layer**: BLT-based encoder/decoder\n",
    "2. **Embedding Layer**: Learned embeddings for latent tokens\n",
    "3. **Hyena Blocks**: Stack of Striped Hyena layers\n",
    "4. **Output Layer**: Task-specific heads (classification, generation, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842ee46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic configuration for exploration\n",
    "config = HyenaGLTConfig(\n",
    "    vocab_size=4,  # DNA: A, T, G, C\n",
    "    latent_vocab_size=256,  # BLT latent tokens\n",
    "    d_model=256,\n",
    "    n_layers=4,\n",
    "    sequence_length=1024,\n",
    "    latent_length=64,  # Compressed representation\n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"- Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"- Latent vocabulary size: {config.latent_vocab_size}\")\n",
    "print(f\"- Model dimension: {config.d_model}\")\n",
    "print(f\"- Number of layers: {config.n_layers}\")\n",
    "print(f\"- Sequence length: {config.sequence_length}\")\n",
    "print(f\"- Compressed latent length: {config.latent_length}\")\n",
    "print(f\"- Compression ratio: {config.sequence_length / config.latent_length:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cefd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = HyenaGLT(config)\n",
    "\n",
    "# Analyze model structure\n",
    "print(\"\\nModel Structure:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = count_parameters(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Analyze parameter distribution\n",
    "for name, module in model.named_children():\n",
    "    if hasattr(module, 'parameters'):\n",
    "        module_params = sum(p.numel() for p in module.parameters())\n",
    "        print(f\"- {name}: {module_params:,} parameters ({module_params/total_params*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4d19e",
   "metadata": {},
   "source": [
    "## 2. BLT Tokenization Component\n",
    "\n",
    "The BLT (Byte Latent Transformer) component is responsible for compressing variable-length genomic sequences into fixed-size latent representations. This provides several benefits:\n",
    "\n",
    "### Benefits:\n",
    "1. **Compression**: Long sequences (e.g., 1024 tokens) → Short latents (e.g., 64 tokens)\n",
    "2. **Fixed Length**: Variable input lengths become fixed latent lengths\n",
    "3. **Learned Representation**: The compression is learned end-to-end\n",
    "4. **Efficiency**: Reduces computational complexity in subsequent layers\n",
    "\n",
    "### Architecture Components:\n",
    "- **Encoder**: Maps input sequences to latent space\n",
    "- **Decoder**: Reconstructs sequences from latent representations\n",
    "- **Latent Embeddings**: Learned vocabulary for compressed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b75507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze BLT tokenization behavior\n",
    "def analyze_blt_compression(model, sequence_length=1024):\n",
    "    \"\"\"Analyze how BLT compresses sequences\"\"\"\n",
    "    # Create sample input\n",
    "    batch_size = 2\n",
    "    sample_input = torch.randint(0, config.vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "    print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "    # Get latent representation\n",
    "    with torch.no_grad():\n",
    "        # Forward pass through BLT encoder\n",
    "        embedded = model.embedding(sample_input)  # (batch, seq_len, d_model)\n",
    "        print(f\"Embedded shape: {embedded.shape}\")\n",
    "\n",
    "        # In a real BLT implementation, this would go through encoder\n",
    "        # For demonstration, we'll simulate the compression\n",
    "        compressed_length = config.latent_length\n",
    "\n",
    "        # Simulate compression (in real implementation, this is learned)\n",
    "        compressed = embedded[:, ::sequence_length//compressed_length, :]\n",
    "        print(f\"Compressed shape: {compressed.shape}\")\n",
    "\n",
    "        compression_ratio = sequence_length / compressed_length\n",
    "        print(f\"Compression ratio: {compression_ratio:.1f}x\")\n",
    "\n",
    "        return compressed\n",
    "\n",
    "compressed_repr = analyze_blt_compression(model)\n",
    "\n",
    "# Visualize compression\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Original sequence visualization\n",
    "original_seq = torch.randint(0, 4, (100,))\n",
    "ax1.plot(original_seq.numpy(), 'o-', alpha=0.7)\n",
    "ax1.set_title('Original Sequence (100 tokens)')\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Token ID')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Compressed representation visualization\n",
    "compressed_demo = torch.randn(16)  # Simulated compressed representation\n",
    "ax2.plot(compressed_demo.numpy(), 'o-', color='red', alpha=0.7)\n",
    "ax2.set_title('Compressed Representation (16 latents)')\n",
    "ax2.set_xlabel('Latent Position')\n",
    "ax2.set_ylabel('Latent Value')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCompression efficiency: {100/6.25:.1f}% size reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790d7d53",
   "metadata": {},
   "source": [
    "## 3. Hyena Blocks\n",
    "\n",
    "Hyena blocks replace traditional attention mechanisms with a combination of:\n",
    "- **Convolutions**: For local pattern recognition\n",
    "- **Gating Mechanisms**: For selective information flow\n",
    "- **Global Mixing**: For long-range dependencies\n",
    "\n",
    "### Key Advantages:\n",
    "1. **Linear Complexity**: O(n) vs O(n²) for attention\n",
    "2. **Long Sequences**: Efficient on very long genomic sequences\n",
    "3. **Parallel Training**: Better parallelization than RNNs\n",
    "4. **Memory Efficient**: Lower memory requirements\n",
    "\n",
    "### Hyena Block Components:\n",
    "- **Input Projection**: Linear transformation of inputs\n",
    "- **Convolution Layers**: Local pattern extraction\n",
    "- **Gating**: Element-wise gating for information flow\n",
    "- **Output Projection**: Final linear transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hyena_vs_attention_complexity():\n",
    "    \"\"\"Compare computational complexity of Hyena vs Attention\"\"\"\n",
    "    sequence_lengths = [256, 512, 1024, 2048, 4096, 8192]\n",
    "    d_model = 256\n",
    "\n",
    "    # Attention complexity: O(n²d)\n",
    "    attention_ops = [n**2 * d_model for n in sequence_lengths]\n",
    "\n",
    "    # Hyena complexity: O(nd) + convolution overhead\n",
    "    hyena_ops = [n * d_model * 3 for n in sequence_lengths]  # 3x for conv layers\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(sequence_lengths, attention_ops, 'o-', label='Attention O(n²d)', linewidth=2)\n",
    "    plt.loglog(sequence_lengths, hyena_ops, 's-', label='Hyena O(nd)', linewidth=2)\n",
    "\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Operations (log scale)')\n",
    "    plt.title('Computational Complexity: Hyena vs Attention')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add speedup annotations\n",
    "    for i, n in enumerate(sequence_lengths):\n",
    "        if i % 2 == 0:  # Show every other point\n",
    "            speedup = attention_ops[i] / hyena_ops[i]\n",
    "            plt.annotate(f'{speedup:.1f}x faster',\n",
    "                        xy=(n, hyena_ops[i]),\n",
    "                        xytext=(10, 10),\n",
    "                        textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Complexity Analysis:\")\n",
    "    for i, n in enumerate(sequence_lengths):\n",
    "        speedup = attention_ops[i] / hyena_ops[i]\n",
    "        print(f\"Sequence length {n:4d}: {speedup:5.1f}x speedup with Hyena\")\n",
    "\n",
    "analyze_hyena_vs_attention_complexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb165a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_memory_usage():\n",
    "    \"\"\"Compare memory usage between different architectures\"\"\"\n",
    "    configs = {\n",
    "        'Small': HyenaGLTConfig(d_model=128, n_layers=4, sequence_length=512),\n",
    "        'Medium': HyenaGLTConfig(d_model=256, n_layers=6, sequence_length=1024),\n",
    "        'Large': HyenaGLTConfig(d_model=512, n_layers=8, sequence_length=2048)\n",
    "    }\n",
    "\n",
    "    memory_usage = {}\n",
    "    parameter_counts = {}\n",
    "\n",
    "    for name, cfg in configs.items():\n",
    "        model = HyenaGLT(cfg)\n",
    "        params = count_parameters(model)\n",
    "\n",
    "        # Estimate memory usage (simplified)\n",
    "        # Parameters + activations + gradients\n",
    "        param_memory = params * 4  # 4 bytes per float32\n",
    "        activation_memory = cfg.sequence_length * cfg.d_model * cfg.n_layers * 4\n",
    "        gradient_memory = param_memory  # Same as parameters\n",
    "\n",
    "        total_memory = param_memory + activation_memory + gradient_memory\n",
    "\n",
    "        memory_usage[name] = total_memory / (1024**2)  # Convert to MB\n",
    "        parameter_counts[name] = params\n",
    "\n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Parameter counts\n",
    "    names = list(parameter_counts.keys())\n",
    "    params = [parameter_counts[name]/1e6 for name in names]  # Convert to millions\n",
    "\n",
    "    bars1 = ax1.bar(names, params, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    ax1.set_ylabel('Parameters (Millions)')\n",
    "    ax1.set_title('Model Size Comparison')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, param in zip(bars1, params, strict=False):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{param:.1f}M', ha='center', va='bottom')\n",
    "\n",
    "    # Memory usage\n",
    "    memory = [memory_usage[name] for name in names]\n",
    "    bars2 = ax2.bar(names, memory, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    ax2.set_ylabel('Memory Usage (MB)')\n",
    "    ax2.set_title('Estimated Memory Usage')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, mem in zip(bars2, memory, strict=False):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{mem:.0f}MB', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return memory_usage, parameter_counts\n",
    "\n",
    "memory_stats, param_stats = compare_memory_usage()\n",
    "print(\"\\nDetailed Statistics:\")\n",
    "for name in memory_stats:\n",
    "    print(f\"{name:6s}: {param_stats[name]/1e6:5.1f}M params, {memory_stats[name]:6.0f}MB memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f2edc",
   "metadata": {},
   "source": [
    "## 4. Model Configuration\n",
    "\n",
    "The `HyenaGLTConfig` class provides flexible configuration for different genomic modeling tasks. Key parameters include:\n",
    "\n",
    "### Core Architecture Parameters:\n",
    "- `d_model`: Hidden dimension size\n",
    "- `n_layers`: Number of Hyena blocks\n",
    "- `num_heads`: Number of attention heads (for hybrid models)\n",
    "- `sequence_length`: Maximum input sequence length\n",
    "- `latent_length`: Compressed latent representation length\n",
    "\n",
    "### Tokenization Parameters:\n",
    "- `vocab_size`: Input vocabulary size (4 for DNA, 20 for proteins)\n",
    "- `latent_vocab_size`: Size of BLT latent vocabulary\n",
    "- `compression_ratio`: How much to compress sequences\n",
    "\n",
    "### Task-Specific Parameters:\n",
    "- `num_classes`: Number of output classes for classification\n",
    "- `dropout`: Dropout rate for regularization\n",
    "- `layer_norm_eps`: Layer normalization epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration examples for different genomic tasks\n",
    "\n",
    "# DNA sequence classification (e.g., promoter prediction)\n",
    "dna_config = HyenaGLTConfig(\n",
    "    vocab_size=4,  # A, T, G, C\n",
    "    d_model=256,\n",
    "    n_layers=6,\n",
    "    sequence_length=1000,\n",
    "    latent_length=50,\n",
    "    num_classes=2,  # promoter/non-promoter\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Protein function prediction\n",
    "protein_config = HyenaGLTConfig(\n",
    "    vocab_size=20,  # 20 amino acids\n",
    "    d_model=512,\n",
    "    n_layers=8,\n",
    "    sequence_length=512,\n",
    "    latent_length=32,\n",
    "    num_classes=1000,  # GO term classes\n",
    "    dropout=0.15\n",
    ")\n",
    "\n",
    "# Long genomic sequence modeling (e.g., chromosome regions)\n",
    "long_sequence_config = HyenaGLTConfig(\n",
    "    vocab_size=4,\n",
    "    d_model=384,\n",
    "    n_layers=12,\n",
    "    sequence_length=10000,  # 10kb sequences\n",
    "    latent_length=100,\n",
    "    num_classes=50,  # chromatin state classes\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "configs = {\n",
    "    'DNA Classification': dna_config,\n",
    "    'Protein Function': protein_config,\n",
    "    'Long Sequence': long_sequence_config\n",
    "}\n",
    "\n",
    "print(\"Configuration Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "for name, config in configs.items():\n",
    "    model = HyenaGLT(config)\n",
    "    params = count_parameters(model)\n",
    "    compression = config.sequence_length / config.latent_length\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Input length: {config.sequence_length:,}\")\n",
    "    print(f\"  Latent length: {config.latent_length}\")\n",
    "    print(f\"  Compression: {compression:.1f}x\")\n",
    "    print(f\"  Parameters: {params/1e6:.1f}M\")\n",
    "    print(f\"  Output classes: {config.num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f9e7b",
   "metadata": {},
   "source": [
    "## 5. Forward Pass Analysis\n",
    "\n",
    "Let's trace through a complete forward pass to understand data flow and transformations:\n",
    "\n",
    "### Forward Pass Steps:\n",
    "1. **Input Tokenization**: Raw sequence → Token IDs\n",
    "2. **Embedding**: Token IDs → Dense vectors\n",
    "3. **BLT Encoding**: Sequence → Compressed latents\n",
    "4. **Hyena Processing**: Latents → Contextual representations\n",
    "5. **Output Projection**: Representations → Task outputs\n",
    "\n",
    "### Shape Transformations:\n",
    "- Input: `(batch_size, sequence_length)`\n",
    "- Embedded: `(batch_size, sequence_length, d_model)`\n",
    "- Compressed: `(batch_size, latent_length, d_model)`\n",
    "- Processed: `(batch_size, latent_length, d_model)`\n",
    "- Output: `(batch_size, num_classes)` or `(batch_size, latent_length, vocab_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_forward_pass(model, sequence_length=512, batch_size=2):\n",
    "    \"\"\"Trace the forward pass through the model\"\"\"\n",
    "    print(\"Forward Pass Tracing\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "    # Create sample input\n",
    "    sample_input = torch.randint(0, model.config.vocab_size, (batch_size, sequence_length))\n",
    "    print(f\"1. Input shape: {sample_input.shape}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Step by step forward pass\n",
    "        print(\"\\n2. Embedding Layer:\")\n",
    "        embedded = model.embedding(sample_input)\n",
    "        print(f\"   Embedded shape: {embedded.shape}\")\n",
    "        print(f\"   Memory: {embedded.numel() * 4 / 1024:.1f} KB\")\n",
    "\n",
    "        # Simulate BLT compression (in real implementation, this would be more complex)\n",
    "        print(\"\\n3. BLT Compression:\")\n",
    "        compressed_length = model.config.latent_length\n",
    "        step_size = sequence_length // compressed_length\n",
    "        compressed = embedded[:, ::step_size, :][:, :compressed_length, :]\n",
    "        print(f\"   Compressed shape: {compressed.shape}\")\n",
    "        print(f\"   Compression ratio: {sequence_length / compressed_length:.1f}x\")\n",
    "        print(f\"   Memory reduction: {embedded.numel() / compressed.numel():.1f}x\")\n",
    "\n",
    "        # Process through Hyena layers\n",
    "        print(\"\\n4. Hyena Layers:\")\n",
    "        x = compressed\n",
    "        for i, layer in enumerate(model.hyena_layers):\n",
    "            x = layer(x)\n",
    "            print(f\"   Layer {i+1} output shape: {x.shape}\")\n",
    "\n",
    "        # Output projection\n",
    "        print(\"\\n5. Output Projection:\")\n",
    "        if hasattr(model, 'classifier'):\n",
    "            # Classification task\n",
    "            pooled = x.mean(dim=1)  # Global average pooling\n",
    "            output = model.classifier(pooled)\n",
    "            print(f\"   Pooled shape: {pooled.shape}\")\n",
    "            print(f\"   Final output shape: {output.shape}\")\n",
    "            print(f\"   Task: Classification ({output.shape[-1]} classes)\")\n",
    "        else:\n",
    "            # Generation task\n",
    "            output = model.output_projection(x)\n",
    "            print(f\"   Final output shape: {output.shape}\")\n",
    "            print(f\"   Task: Generation (vocab size {output.shape[-1]})\")\n",
    "\n",
    "    return output\n",
    "\n",
    "# Trace with DNA classification model\n",
    "print(\"\\nTracing DNA Classification Model:\")\n",
    "dna_model = HyenaGLT(dna_config)\n",
    "output = trace_forward_pass(dna_model, sequence_length=1000)\n",
    "\n",
    "# Analyze computational graph\n",
    "print(\"\\nModel summary:\")\n",
    "print(f\"- Total parameters: {count_parameters(dna_model):,}\")\n",
    "print(f\"- Model size: {count_parameters(dna_model) * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"- Compression efficiency: {dna_config.sequence_length / dna_config.latent_length:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a10f8f",
   "metadata": {},
   "source": [
    "## 6. Parameter Efficiency\n",
    "\n",
    "Hyena-GLT achieves parameter efficiency through several design choices:\n",
    "\n",
    "### Efficiency Mechanisms:\n",
    "1. **BLT Compression**: Reduces sequence length before processing\n",
    "2. **Linear Hyena Blocks**: O(n) complexity vs O(n²) attention\n",
    "3. **Shared Parameters**: Reuse weights across sequence positions\n",
    "4. **Efficient Convolutions**: Depthwise separable convolutions\n",
    "\n",
    "### Parameter Distribution Analysis:\n",
    "Let's analyze where parameters are allocated in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_parameter_distribution(model):\n",
    "    \"\"\"Analyze parameter distribution across model components\"\"\"\n",
    "    param_dict = {}\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Leaf modules\n",
    "            module_params = sum(p.numel() for p in module.parameters())\n",
    "            if module_params > 0:\n",
    "                param_dict[name] = module_params\n",
    "\n",
    "    # Group by component type\n",
    "    component_groups = {\n",
    "        'Embedding': {},\n",
    "        'Hyena Layers': {},\n",
    "        'Output': {},\n",
    "        'Other': {}\n",
    "    }\n",
    "\n",
    "    for name, params in param_dict.items():\n",
    "        if 'embedding' in name.lower():\n",
    "            component_groups['Embedding'][name] = params\n",
    "        elif 'hyena' in name.lower() or 'layer' in name.lower():\n",
    "            component_groups['Hyena Layers'][name] = params\n",
    "        elif 'classifier' in name.lower() or 'output' in name.lower():\n",
    "            component_groups['Output'][name] = params\n",
    "        else:\n",
    "            component_groups['Other'][name] = params\n",
    "\n",
    "    # Calculate totals\n",
    "    group_totals = {}\n",
    "    for group, components in component_groups.items():\n",
    "        group_totals[group] = sum(components.values())\n",
    "\n",
    "    total_params = sum(group_totals.values())\n",
    "\n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Pie chart of component distribution\n",
    "    sizes = [group_totals[group] for group in group_totals if group_totals[group] > 0]\n",
    "    labels = [group for group in group_totals if group_totals[group] > 0]\n",
    "    colors = ['skyblue', 'lightcoral', 'lightgreen', 'gold']\n",
    "\n",
    "    wedges, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "                                       colors=colors[:len(sizes)], startangle=90)\n",
    "    ax1.set_title('Parameter Distribution by Component')\n",
    "\n",
    "    # Bar chart of detailed breakdown\n",
    "    all_components = []\n",
    "    all_params = []\n",
    "    all_colors = []\n",
    "    color_map = {'Embedding': 'skyblue', 'Hyena Layers': 'lightcoral',\n",
    "                 'Output': 'lightgreen', 'Other': 'gold'}\n",
    "\n",
    "    for group, components in component_groups.items():\n",
    "        if components:\n",
    "            for comp_name, params in components.items():\n",
    "                # Simplify component names\n",
    "                simple_name = comp_name.split('.')[-1]\n",
    "                all_components.append(f\"{group}\\n{simple_name}\")\n",
    "                all_params.append(params / 1000)  # Convert to thousands\n",
    "                all_colors.append(color_map[group])\n",
    "\n",
    "    if all_components:\n",
    "        bars = ax2.bar(range(len(all_components)), all_params, color=all_colors)\n",
    "        ax2.set_xticks(range(len(all_components)))\n",
    "        ax2.set_xticklabels(all_components, rotation=45, ha='right', fontsize=8)\n",
    "        ax2.set_ylabel('Parameters (Thousands)')\n",
    "        ax2.set_title('Detailed Parameter Breakdown')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, param in zip(bars, all_params, strict=False):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{param:.0f}K', ha='center', va='bottom', fontsize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nParameter Distribution Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    for group, total in group_totals.items():\n",
    "        if total > 0:\n",
    "            percentage = (total / total_params) * 100\n",
    "            print(f\"{group:15s}: {total/1e6:6.2f}M ({percentage:5.1f}%)\")\n",
    "    print(f\"{'Total':15s}: {total_params/1e6:6.2f}M (100.0%)\")\n",
    "\n",
    "    return param_dict, group_totals\n",
    "\n",
    "# Analyze DNA classification model\n",
    "print(\"Parameter Analysis for DNA Classification Model:\")\n",
    "param_breakdown, group_totals = analyze_parameter_distribution(dna_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76cb32",
   "metadata": {},
   "source": [
    "## 7. Scalability Analysis\n",
    "\n",
    "Hyena-GLT is designed to scale efficiently with sequence length and model size. Let's analyze scaling behavior:\n",
    "\n",
    "### Scaling Dimensions:\n",
    "1. **Sequence Length**: How performance scales with longer inputs\n",
    "2. **Model Size**: Impact of increasing model parameters\n",
    "3. **Batch Size**: Training efficiency with larger batches\n",
    "4. **Number of Layers**: Depth vs performance trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe96c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scaling_behavior():\n",
    "    \"\"\"Analyze how model scales with different parameters\"\"\"\n",
    "\n",
    "    # Sequence length scaling\n",
    "    sequence_lengths = [256, 512, 1024, 2048, 4096]\n",
    "    base_config = HyenaGLTConfig(d_model=256, n_layers=4)\n",
    "\n",
    "    seq_scaling_data = {\n",
    "        'length': [],\n",
    "        'parameters': [],\n",
    "        'memory_estimate': [],\n",
    "        'compression_ratio': []\n",
    "    }\n",
    "\n",
    "    print(\"Sequence Length Scaling:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for seq_len in sequence_lengths:\n",
    "        config = HyenaGLTConfig(\n",
    "            d_model=base_config.d_model,\n",
    "            n_layers=base_config.n_layers,\n",
    "            sequence_length=seq_len,\n",
    "            latent_length=max(16, seq_len // 32)  # Adaptive compression\n",
    "        )\n",
    "\n",
    "        model = HyenaGLT(config)\n",
    "        params = count_parameters(model)\n",
    "\n",
    "        # Estimate memory (simplified)\n",
    "        memory_mb = (params * 4 + seq_len * config.d_model * 4 * 2) / (1024**2)\n",
    "        compression = config.sequence_length / config.latent_length\n",
    "\n",
    "        seq_scaling_data['length'].append(seq_len)\n",
    "        seq_scaling_data['parameters'].append(params)\n",
    "        seq_scaling_data['memory_estimate'].append(memory_mb)\n",
    "        seq_scaling_data['compression_ratio'].append(compression)\n",
    "\n",
    "        print(f\"Length {seq_len:4d}: {params/1e6:5.1f}M params, {memory_mb:6.1f}MB, {compression:4.1f}x compression\")\n",
    "\n",
    "    # Model size scaling\n",
    "    model_sizes = [128, 256, 512, 768, 1024]\n",
    "\n",
    "    size_scaling_data = {\n",
    "        'd_model': [],\n",
    "        'parameters': [],\n",
    "        'memory_estimate': []\n",
    "    }\n",
    "\n",
    "    print(\"\\nModel Size Scaling:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for d_model in model_sizes:\n",
    "        config = HyenaGLTConfig(\n",
    "            d_model=d_model,\n",
    "            n_layers=6,\n",
    "            sequence_length=1024,\n",
    "            latent_length=64\n",
    "        )\n",
    "\n",
    "        model = HyenaGLT(config)\n",
    "        params = count_parameters(model)\n",
    "        memory_mb = (params * 4 + 1024 * d_model * 4 * 2) / (1024**2)\n",
    "\n",
    "        size_scaling_data['d_model'].append(d_model)\n",
    "        size_scaling_data['parameters'].append(params)\n",
    "        size_scaling_data['memory_estimate'].append(memory_mb)\n",
    "\n",
    "        print(f\"d_model {d_model:4d}: {params/1e6:6.1f}M params, {memory_mb:6.1f}MB\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Sequence length scaling\n",
    "    ax1.plot(seq_scaling_data['length'], [p/1e6 for p in seq_scaling_data['parameters']],\n",
    "             'o-', color='blue', linewidth=2)\n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Parameters (Millions)')\n",
    "    ax1.set_title('Parameters vs Sequence Length')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log', base=2)\n",
    "\n",
    "    # Memory scaling\n",
    "    ax2.plot(seq_scaling_data['length'], seq_scaling_data['memory_estimate'],\n",
    "             's-', color='red', linewidth=2)\n",
    "    ax2.set_xlabel('Sequence Length')\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.set_title('Memory Usage vs Sequence Length')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log', base=2)\n",
    "\n",
    "    # Model size scaling\n",
    "    ax3.plot(size_scaling_data['d_model'], [p/1e6 for p in size_scaling_data['parameters']],\n",
    "             '^-', color='green', linewidth=2)\n",
    "    ax3.set_xlabel('Model Dimension (d_model)')\n",
    "    ax3.set_ylabel('Parameters (Millions)')\n",
    "    ax3.set_title('Parameters vs Model Size')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Compression efficiency\n",
    "    ax4.plot(seq_scaling_data['length'], seq_scaling_data['compression_ratio'],\n",
    "             'D-', color='purple', linewidth=2)\n",
    "    ax4.set_xlabel('Sequence Length')\n",
    "    ax4.set_ylabel('Compression Ratio')\n",
    "    ax4.set_title('BLT Compression Efficiency')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    ax4.set_xscale('log', base=2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return seq_scaling_data, size_scaling_data\n",
    "\n",
    "seq_data, size_data = analyze_scaling_behavior()\n",
    "\n",
    "# Calculate scaling efficiency\n",
    "print(\"\\nScaling Efficiency Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Parameter scaling with sequence length\n",
    "seq_param_growth = seq_data['parameters'][-1] / seq_data['parameters'][0]\n",
    "seq_length_growth = seq_data['length'][-1] / seq_data['length'][0]\n",
    "print(f\"Sequence length increased {seq_length_growth:.1f}x\")\n",
    "print(f\"Parameters increased {seq_param_growth:.1f}x\")\n",
    "print(f\"Parameter efficiency: {seq_length_growth/seq_param_growth:.2f}\")\n",
    "\n",
    "# Parameter scaling with model size\n",
    "size_param_growth = size_data['parameters'][-1] / size_data['parameters'][0]\n",
    "size_dim_growth = size_data['d_model'][-1] / size_data['d_model'][0]\n",
    "print(f\"\\nModel dimension increased {size_dim_growth:.1f}x\")\n",
    "print(f\"Parameters increased {size_param_growth:.1f}x\")\n",
    "print(f\"Expected quadratic growth: {size_dim_growth**2:.1f}x\")\n",
    "print(f\"Actual vs expected: {size_param_growth/(size_dim_growth**2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c10dc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook explored the Hyena-GLT architecture in detail, covering:\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Efficient Architecture**: Combines BLT compression with Hyena blocks for linear complexity\n",
    "2. **Scalability**: Scales efficiently with sequence length and model size\n",
    "3. **Flexibility**: Configurable for various genomic modeling tasks\n",
    "4. **Parameter Efficiency**: Achieves good performance with fewer parameters than attention-based models\n",
    "\n",
    "### Architecture Benefits:\n",
    "- **Linear Complexity**: O(n) vs O(n²) for attention mechanisms\n",
    "- **Memory Efficiency**: BLT compression reduces memory requirements\n",
    "- **Long Sequences**: Can handle very long genomic sequences efficiently\n",
    "- **Task Adaptability**: Easily configured for different genomic tasks\n",
    "\n",
    "### Next Steps:\n",
    "1. **Training Tutorial**: Learn how to train models with this architecture\n",
    "2. **Advanced Techniques**: Explore fine-tuning and transfer learning\n",
    "3. **Real Applications**: Apply to actual genomic datasets\n",
    "4. **Performance Optimization**: Techniques for improving speed and memory usage\n",
    "\n",
    "### Resources:\n",
    "- [Training Basics Tutorial](04_training_basics.ipynb)\n",
    "- [Fine-tuning Guide](05_fine_tuning.ipynb)\n",
    "- [Example Scripts](../examples/)\n",
    "- [API Documentation](../../docs/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
