{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da8a094",
   "metadata": {},
   "source": [
    "# Training Basics with Hyena-GLT\n",
    "\n",
    "This notebook covers the fundamentals of training Hyena-GLT models for genomic sequence analysis. You'll learn how to prepare data, configure training, and monitor progress.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Training Setup](#training-setup)\n",
    "2. [Data Preparation](#data-preparation)\n",
    "3. [Model Configuration](#model-configuration)\n",
    "4. [Training Loop](#training-loop)\n",
    "5. [Monitoring and Evaluation](#monitoring)\n",
    "6. [Checkpointing and Resuming](#checkpointing)\n",
    "7. [Common Issues and Solutions](#troubleshooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d0a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path().absolute().parent.parent))\n",
    "\n",
    "from hyena_glt.models.hyena_glt import HyenaGLT, HyenaGLTConfig\n",
    "from hyena_glt.tokenizers import DNATokenizer\n",
    "from hyena_glt.data import GenomicDataset, SequenceCollator\n",
    "from hyena_glt.training import HyenaGLTTrainer, TrainingConfig\n",
    "from hyena_glt.evaluation import ModelEvaluator\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Hyena-GLT Training Tutorial\")\n",
    "print(\"=\" * 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ebace0",
   "metadata": {},
   "source": [
    "## 1. Training Setup\n",
    "\n",
    "Before training, we need to:\n",
    "1. **Prepare the data**: Tokenize and format genomic sequences\n",
    "2. **Configure the model**: Set architecture parameters\n",
    "3. **Setup training**: Define optimizer, scheduler, and loss function\n",
    "4. **Initialize monitoring**: Setup logging and visualization\n",
    "\n",
    "### Key Training Components:\n",
    "- **Tokenizer**: Converts sequences to numerical tokens\n",
    "- **Dataset**: Handles data loading and preprocessing\n",
    "- **DataLoader**: Manages batching and shuffling\n",
    "- **Model**: The Hyena-GLT architecture\n",
    "- **Optimizer**: Updates model parameters\n",
    "- **Scheduler**: Adjusts learning rate during training\n",
    "- **Loss Function**: Measures prediction quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5fa594",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Proper data preparation is crucial for successful training. We'll create a synthetic dataset for demonstration, but the same principles apply to real genomic data.\n",
    "\n",
    "### Data Preparation Steps:\n",
    "1. **Generate/Load sequences**: DNA, RNA, or protein sequences\n",
    "2. **Create labels**: Classification targets or other annotations\n",
    "3. **Tokenization**: Convert sequences to numerical tokens\n",
    "4. **Dataset creation**: Wrap data in PyTorch Dataset\n",
    "5. **DataLoader setup**: Configure batching and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic DNA sequences for training\n",
    "def generate_synthetic_dna_data(num_samples=1000, seq_length=512, num_classes=2):\n",
    "    \"\"\"Generate synthetic DNA sequences with labels\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Generate random DNA sequence\n",
    "        sequence = ''.join(np.random.choice(nucleotides, seq_length))\n",
    "        \n",
    "        # Create synthetic labels based on GC content\n",
    "        gc_content = (sequence.count('G') + sequence.count('C')) / len(sequence)\n",
    "        label = 1 if gc_content > 0.5 else 0  # High GC = class 1, Low GC = class 0\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Generate training and validation data\n",
    "print(\"Generating synthetic DNA data...\")\n",
    "train_sequences, train_labels = generate_synthetic_dna_data(num_samples=800, seq_length=256)\n",
    "val_sequences, val_labels = generate_synthetic_dna_data(num_samples=200, seq_length=256)\n",
    "\n",
    "print(f\"Training samples: {len(train_sequences)}\")\n",
    "print(f\"Validation samples: {len(val_sequences)}\")\n",
    "print(f\"Sequence length: {len(train_sequences[0])}\")\n",
    "print(f\"Classes: {set(train_labels)}\")\n",
    "\n",
    "# Show data distribution\n",
    "train_label_counts = np.bincount(train_labels)\n",
    "val_label_counts = np.bincount(val_labels)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training data distribution\n",
    "ax1.bar(['Low GC', 'High GC'], train_label_counts, color=['lightblue', 'lightcoral'])\n",
    "ax1.set_title('Training Data Distribution')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "for i, count in enumerate(train_label_counts):\n",
    "    ax1.text(i, count + 5, str(count), ha='center', va='bottom')\n",
    "\n",
    "# Validation data distribution\n",
    "ax2.bar(['Low GC', 'High GC'], val_label_counts, color=['lightblue', 'lightcoral'])\n",
    "ax2.set_title('Validation Data Distribution')\n",
    "ax2.set_ylabel('Number of Samples')\n",
    "for i, count in enumerate(val_label_counts):\n",
    "    ax2.text(i, count + 2, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show sample sequences\n",
    "print(\"\\nSample sequences:\")\n",
    "for i in range(3):\n",
    "    seq = train_sequences[i]\n",
    "    label = train_labels[i]\n",
    "    gc_content = (seq.count('G') + seq.count('C')) / len(seq)\n",
    "    print(f\"Sequence {i+1}: {seq[:50]}... (GC: {gc_content:.2f}, Label: {label})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class for our DNA data\n",
    "class DNAClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset for DNA sequence classification\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=256):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize sequence\n",
    "        tokens = self.tokenizer.encode(sequence, max_length=self.max_length)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "            'sequence_length': torch.tensor(len(tokens), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DNATokenizer()\n",
    "print(f\"Tokenizer vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DNAClassificationDataset(train_sequences, train_labels, tokenizer)\n",
    "val_dataset = DNAClassificationDataset(val_sequences, val_labels, tokenizer)\n",
    "\n",
    "print(f\"\\nDataset created:\")\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"Input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Label: {sample['labels'].item()}\")\n",
    "print(f\"Sequence length: {sample['sequence_length'].item()}\")\n",
    "print(f\"First 20 tokens: {sample['input_ids'][:20].tolist()}\")\n",
    "\n",
    "# Verify tokenization\n",
    "original_seq = train_sequences[0][:20]\n",
    "decoded_seq = tokenizer.decode(sample['input_ids'][:20].tolist())\n",
    "print(f\"Original: {original_seq}\")\n",
    "print(f\"Decoded:  {decoded_seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ac9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator for batching\n",
    "collator = SequenceCollator(tokenizer.pad_token_id)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0  # Set to 0 for notebook compatibility\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Test batch loading\n",
    "print(\"\\nTesting batch loading...\")\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
    "    print(f\"  Labels shape: {batch['labels'].shape}\")\n",
    "    print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
    "    \n",
    "    # Show padding behavior\n",
    "    seq_lengths = batch['attention_mask'].sum(dim=1)\n",
    "    print(f\"  Sequence lengths in batch: {seq_lengths.tolist()}\")\n",
    "    print(f\"  Max length: {batch['input_ids'].shape[1]}\")\n",
    "    \n",
    "    if batch_idx == 0:  # Only show first batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a87a3e",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "Now let's configure our Hyena-GLT model for the DNA classification task. We need to set:\n",
    "\n",
    "### Configuration Parameters:\n",
    "- **Architecture**: Model dimensions, layers, etc.\n",
    "- **Task-specific**: Number of classes, output type\n",
    "- **Training**: Dropout, initialization, etc.\n",
    "- **Efficiency**: Sequence compression, latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model for DNA classification\n",
    "model_config = HyenaGLTConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    latent_vocab_size=128,  # Smaller for this demo\n",
    "    d_model=256,           # Model dimension\n",
    "    n_layers=4,            # Number of Hyena layers\n",
    "    sequence_length=256,   # Maximum input length\n",
    "    latent_length=32,      # Compressed representation length\n",
    "    num_classes=2,         # Binary classification\n",
    "    num_heads=8,           # For any attention components\n",
    "    dropout=0.1,           # Regularization\n",
    "    layer_norm_eps=1e-5\n",
    ")\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"- Vocabulary size: {model_config.vocab_size}\")\n",
    "print(f\"- Model dimension: {model_config.d_model}\")\n",
    "print(f\"- Number of layers: {model_config.n_layers}\")\n",
    "print(f\"- Sequence length: {model_config.sequence_length}\")\n",
    "print(f\"- Latent length: {model_config.latent_length}\")\n",
    "print(f\"- Compression ratio: {model_config.sequence_length / model_config.latent_length:.1f}x\")\n",
    "print(f\"- Number of classes: {model_config.num_classes}\")\n",
    "print(f\"- Dropout rate: {model_config.dropout}\")\n",
    "\n",
    "# Create model\n",
    "model = HyenaGLT(model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"- Total parameters: {total_params:,}\")\n",
    "print(f\"- Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"- Model size: {total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    sample_batch = {k: v.to(device) for k, v in sample_batch.items()}\n",
    "    \n",
    "    outputs = model(sample_batch['input_ids'], attention_mask=sample_batch['attention_mask'])\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "    print(f\"Output range: [{outputs.min().item():.3f}, {outputs.max().item():.3f}]\")\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probs = torch.softmax(outputs, dim=-1)\n",
    "    print(f\"Probabilities shape: {probs.shape}\")\n",
    "    print(f\"Sample probabilities: {probs[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fe23c",
   "metadata": {},
   "source": [
    "## 4. Training Loop\n",
    "\n",
    "Now we'll set up the training loop with:\n",
    "\n",
    "### Training Components:\n",
    "1. **Optimizer**: Adam with weight decay\n",
    "2. **Scheduler**: Learning rate scheduling\n",
    "3. **Loss Function**: Cross-entropy for classification\n",
    "4. **Metrics**: Accuracy, loss tracking\n",
    "5. **Validation**: Regular evaluation on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccedbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "training_config = TrainingConfig(\n",
    "    learning_rate=1e-4,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=10,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    gradient_clipping=1.0,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    logging_steps=10\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"- Learning rate: {training_config.learning_rate}\")\n",
    "print(f\"- Batch size: {training_config.batch_size}\")\n",
    "print(f\"- Number of epochs: {training_config.num_epochs}\")\n",
    "print(f\"- Warmup steps: {training_config.warmup_steps}\")\n",
    "print(f\"- Weight decay: {training_config.weight_decay}\")\n",
    "print(f\"- Gradient clipping: {training_config.gradient_clipping}\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=training_config.learning_rate,\n",
    "    weight_decay=training_config.weight_decay\n",
    ")\n",
    "\n",
    "# Calculate total training steps\n",
    "total_steps = len(train_loader) * training_config.num_epochs\n",
    "warmup_steps = training_config.warmup_steps\n",
    "\n",
    "# Linear warmup then cosine decay\n",
    "def get_lr_scheduler(optimizer, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        else:\n",
    "            progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"\\nTraining Setup:\")\n",
    "print(f\"- Total training steps: {total_steps}\")\n",
    "print(f\"- Warmup steps: {warmup_steps}\")\n",
    "print(f\"- Steps per epoch: {len(train_loader)}\")\n",
    "print(f\"- Optimizer: {type(optimizer).__name__}\")\n",
    "print(f\"- Loss function: {type(criterion).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d293716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with monitoring\n",
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, criterion, config, device):\n",
    "    \"\"\"Complete training loop with validation and monitoring\"\"\"\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    global_step = 0\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_train_correct = 0\n",
    "        epoch_train_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            loss = criterion(outputs, batch['labels'])\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if config.gradient_clipping > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clipping)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_train_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            epoch_train_correct += (predictions == batch['labels']).sum().item()\n",
    "            epoch_train_total += batch['labels'].size(0)\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{epoch_train_correct/epoch_train_total:.4f}',\n",
    "                'lr': f'{current_lr:.2e}'\n",
    "            })\n",
    "            \n",
    "            # Validation\n",
    "            if global_step % config.eval_steps == 0:\n",
    "                val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "                \n",
    "                # Save best model\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    print(f\"\\nNew best validation accuracy: {val_acc:.4f}\")\n",
    "                \n",
    "                model.train()  # Back to training mode\n",
    "        \n",
    "        # End of epoch statistics\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        train_acc = epoch_train_correct / epoch_train_total\n",
    "        \n",
    "        # Final validation for the epoch\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model on validation/test set\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "            loss = criterion(outputs, batch['labels'])\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            correct += (predictions == batch['labels']).sum().item()\n",
    "            total += batch['labels'].size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Start training\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    config=training_config,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2e0a84",
   "metadata": {},
   "source": [
    "## 5. Monitoring and Evaluation\n",
    "\n",
    "After training, let's analyze the results and visualize the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b72cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training curves\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    ax3.plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training metrics summary\n",
    "    final_train_acc = history['train_acc'][-1]\n",
    "    final_val_acc = history['val_acc'][-1]\n",
    "    best_val_acc = max(history['val_acc'])\n",
    "    \n",
    "    ax4.bar(['Final Train', 'Final Val', 'Best Val'], \n",
    "            [final_train_acc, final_val_acc, best_val_acc],\n",
    "            color=['blue', 'red', 'green'], alpha=0.7)\n",
    "    ax4.set_title('Accuracy Summary')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate([final_train_acc, final_val_acc, best_val_acc]):\n",
    "        ax4.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nTraining Summary:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Final training accuracy: {final_train_acc:.4f}\")\n",
    "    print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Final training loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history['val_loss'][-1]:.4f}\")\n",
    "    \n",
    "    # Check for overfitting\n",
    "    if final_train_acc - final_val_acc > 0.1:\n",
    "        print(\"\\n⚠️  Warning: Possible overfitting detected (train acc >> val acc)\")\n",
    "    elif final_val_acc > final_train_acc:\n",
    "        print(\"\\n✅ Good generalization (val acc >= train acc)\")\n",
    "    else:\n",
    "        print(\"\\n✅ Reasonable training progress\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed model evaluation\n",
    "def detailed_evaluation(model, dataloader, tokenizer, device, num_examples=10):\n",
    "    \"\"\"Perform detailed evaluation with examples\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    example_sequences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            batch_device = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(batch_device['input_ids'], attention_mask=batch_device['attention_mask'])\n",
    "            probabilities = torch.softmax(outputs, dim=-1)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Store some example sequences for analysis\n",
    "            if len(example_sequences) < num_examples:\n",
    "                for i in range(min(batch['input_ids'].size(0), num_examples - len(example_sequences))):\n",
    "                    sequence_tokens = batch['input_ids'][i].numpy()\n",
    "                    sequence_str = tokenizer.decode(sequence_tokens)\n",
    "                    example_sequences.append({\n",
    "                        'sequence': sequence_str,\n",
    "                        'true_label': batch['labels'][i].item(),\n",
    "                        'predicted_label': predictions[i].item(),\n",
    "                        'probabilities': probabilities[i].cpu().numpy().tolist()\n",
    "                    })\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "                xticklabels=['Low GC', 'High GC'],\n",
    "                yticklabels=['Low GC', 'High GC'])\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('True')\n",
    "    \n",
    "    # Prediction confidence distribution\n",
    "    all_probs = np.array(all_probabilities)\n",
    "    max_probs = np.max(all_probs, axis=1)\n",
    "    \n",
    "    ax2.hist(max_probs, bins=20, alpha=0.7, edgecolor='black')\n",
    "    ax2.set_title('Prediction Confidence Distribution')\n",
    "    ax2.set_xlabel('Maximum Probability')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='Random')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed metrics\n",
    "    print(\"\\nDetailed Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Number of samples: {len(all_labels)}\")\n",
    "    print(f\"Class distribution: {np.bincount(all_labels)}\")\n",
    "    print(f\"Average confidence: {np.mean(max_probs):.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, \n",
    "                              target_names=['Low GC', 'High GC']))\n",
    "    \n",
    "    # Show example predictions\n",
    "    print(\"\\nExample Predictions:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, example in enumerate(example_sequences[:5]):\n",
    "        sequence_preview = example['sequence'][:50] + '...' if len(example['sequence']) > 50 else example['sequence']\n",
    "        true_label = 'High GC' if example['true_label'] == 1 else 'Low GC'\n",
    "        pred_label = 'High GC' if example['predicted_label'] == 1 else 'Low GC'\n",
    "        confidence = max(example['probabilities'])\n",
    "        \n",
    "        status = \"✅\" if example['true_label'] == example['predicted_label'] else \"❌\"\n",
    "        \n",
    "        print(f\"{status} Example {i+1}:\")\n",
    "        print(f\"   Sequence: {sequence_preview}\")\n",
    "        print(f\"   True: {true_label}, Predicted: {pred_label} (confidence: {confidence:.3f})\")\n",
    "        print(f\"   Probabilities: [Low GC: {example['probabilities'][0]:.3f}, High GC: {example['probabilities'][1]:.3f}]\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': all_probabilities,\n",
    "        'confusion_matrix': cm,\n",
    "        'examples': example_sequences\n",
    "    }\n",
    "\n",
    "# Perform detailed evaluation\n",
    "eval_results = detailed_evaluation(model, val_loader, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf89289",
   "metadata": {},
   "source": [
    "## 6. Checkpointing and Resuming\n",
    "\n",
    "Checkpointing allows you to save model state and resume training later. This is essential for long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc128b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoint\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, history, filepath):\n",
    "    \"\"\"Save complete training checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'history': history,\n",
    "        'model_config': model.config.__dict__,\n",
    "        'best_val_acc': max(history['val_acc']) if history['val_acc'] else 0.0\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Checkpoint saved to {filepath}\")\n",
    "\n",
    "def load_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
    "    \"\"\"Load training checkpoint\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location='cpu')\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    print(f\"Checkpoint loaded from {filepath}\")\n",
    "    print(f\"Resumed from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Best validation accuracy: {checkpoint['best_val_acc']:.4f}\")\n",
    "    \n",
    "    return checkpoint['epoch'], checkpoint['history']\n",
    "\n",
    "# Save current model\n",
    "checkpoint_path = \"hyena_glt_dna_classifier.pt\"\n",
    "save_checkpoint(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epoch=training_config.num_epochs,\n",
    "    history=history,\n",
    "    filepath=checkpoint_path\n",
    ")\n",
    "\n",
    "# Demonstrate loading\n",
    "print(\"\\nDemonstrating checkpoint loading...\")\n",
    "\n",
    "# Create a new model instance\n",
    "new_model = HyenaGLT(model_config)\n",
    "new_model = new_model.to(device)\n",
    "\n",
    "# Load the checkpoint\n",
    "epoch, loaded_history = load_checkpoint(checkpoint_path, new_model)\n",
    "\n",
    "# Verify the loaded model works\n",
    "print(\"\\nTesting loaded model...\")\n",
    "test_batch = next(iter(val_loader))\n",
    "test_batch = {k: v.to(device) for k, v in test_batch.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    original_outputs = model(test_batch['input_ids'], attention_mask=test_batch['attention_mask'])\n",
    "    loaded_outputs = new_model(test_batch['input_ids'], attention_mask=test_batch['attention_mask'])\n",
    "    \n",
    "    # Check if outputs are identical\n",
    "    outputs_match = torch.allclose(original_outputs, loaded_outputs, atol=1e-6)\n",
    "    print(f\"Original and loaded model outputs match: {outputs_match}\")\n",
    "\n",
    "# Save model for inference (model only)\n",
    "inference_path = \"hyena_glt_dna_classifier_inference.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': model_config.__dict__,\n",
    "    'tokenizer_config': {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'special_tokens': tokenizer.special_tokens\n",
    "    }\n",
    "}, inference_path)\n",
    "print(f\"\\nInference model saved to {inference_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06306723",
   "metadata": {},
   "source": [
    "## 7. Common Issues and Solutions\n",
    "\n",
    "### Training Issues and Solutions:\n",
    "\n",
    "#### 1. **Slow Convergence**\n",
    "- **Issue**: Model takes too long to learn\n",
    "- **Solutions**:\n",
    "  - Increase learning rate (carefully)\n",
    "  - Reduce model complexity\n",
    "  - Check data quality and labels\n",
    "  - Use learning rate scheduling\n",
    "\n",
    "#### 2. **Overfitting**\n",
    "- **Issue**: Training accuracy >> Validation accuracy\n",
    "- **Solutions**:\n",
    "  - Increase dropout rate\n",
    "  - Add weight decay\n",
    "  - Use data augmentation\n",
    "  - Reduce model size\n",
    "  - Early stopping\n",
    "\n",
    "#### 3. **Memory Issues**\n",
    "- **Issue**: CUDA out of memory\n",
    "- **Solutions**:\n",
    "  - Reduce batch size\n",
    "  - Use gradient accumulation\n",
    "  - Reduce sequence length\n",
    "  - Use mixed precision training\n",
    "\n",
    "#### 4. **Unstable Training**\n",
    "- **Issue**: Loss spikes or NaN values\n",
    "- **Solutions**:\n",
    "  - Reduce learning rate\n",
    "  - Add gradient clipping\n",
    "  - Check for numerical instabilities\n",
    "  - Use better initialization\n",
    "\n",
    "#### 5. **Poor Performance**\n",
    "- **Issue**: Low accuracy on validation set\n",
    "- **Solutions**:\n",
    "  - Increase model capacity\n",
    "  - Improve data quality\n",
    "  - Tune hyperparameters\n",
    "  - Use transfer learning\n",
    "  - Check data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62b1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training diagnostics and debugging tools\n",
    "def diagnose_training_issues(model, dataloader, device, num_batches=5):\n",
    "    \"\"\"Diagnose common training issues\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    issues_found = []\n",
    "    \n",
    "    print(\"Training Diagnostics\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Check for gradient flow\n",
    "    print(\"1. Checking gradient flow...\")\n",
    "    model.train()\n",
    "    \n",
    "    total_norm = 0\n",
    "    param_count = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if batch_idx >= num_batches:\n",
    "            break\n",
    "            \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = nn.CrossEntropyLoss()(outputs, batch['labels'])\n",
    "        \n",
    "        # Backward pass\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Check gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "                param_count += 1\n",
    "        \n",
    "        break  # Only check first batch\n",
    "    \n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    avg_grad_norm = total_norm / param_count if param_count > 0 else 0\n",
    "    \n",
    "    print(f\"   Average gradient norm: {avg_grad_norm:.6f}\")\n",
    "    \n",
    "    if avg_grad_norm < 1e-7:\n",
    "        issues_found.append(\"Very small gradients - possible vanishing gradient problem\")\n",
    "    elif avg_grad_norm > 10:\n",
    "        issues_found.append(\"Very large gradients - possible exploding gradient problem\")\n",
    "    \n",
    "    # Check for dead neurons\n",
    "    print(\"\\n2. Checking for dead neurons...\")\n",
    "    activations = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            activations.append(output.detach().cpu())\n",
    "    \n",
    "    hooks = []\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sample_batch = next(iter(dataloader))\n",
    "        sample_batch = {k: v.to(device) for k, v in sample_batch.items()}\n",
    "        _ = model(sample_batch['input_ids'], attention_mask=sample_batch['attention_mask'])\n",
    "    \n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    dead_neurons = 0\n",
    "    total_neurons = 0\n",
    "    \n",
    "    for activation in activations:\n",
    "        if len(activation.shape) >= 2:\n",
    "            # Check for neurons that are always zero\n",
    "            neuron_activity = activation.abs().sum(dim=tuple(range(len(activation.shape)-1)))\n",
    "            dead_neurons += (neuron_activity == 0).sum().item()\n",
    "            total_neurons += neuron_activity.numel()\n",
    "    \n",
    "    dead_neuron_ratio = dead_neurons / total_neurons if total_neurons > 0 else 0\n",
    "    print(f\"   Dead neuron ratio: {dead_neuron_ratio:.4f} ({dead_neurons}/{total_neurons})\")\n",
    "    \n",
    "    if dead_neuron_ratio > 0.1:\n",
    "        issues_found.append(f\"High dead neuron ratio ({dead_neuron_ratio:.2%}) - consider adjusting initialization or learning rate\")\n",
    "    \n",
    "    # Check data statistics\n",
    "    print(\"\\n3. Checking data statistics...\")\n",
    "    all_labels = []\n",
    "    all_seq_lengths = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        all_labels.extend(batch['labels'].tolist())\n",
    "        all_seq_lengths.extend(batch['attention_mask'].sum(dim=1).tolist())\n",
    "    \n",
    "    label_distribution = np.bincount(all_labels)\n",
    "    class_imbalance = max(label_distribution) / min(label_distribution) if min(label_distribution) > 0 else float('inf')\n",
    "    \n",
    "    print(f\"   Class distribution: {label_distribution.tolist()}\")\n",
    "    print(f\"   Class imbalance ratio: {class_imbalance:.2f}\")\n",
    "    print(f\"   Average sequence length: {np.mean(all_seq_lengths):.1f}\")\n",
    "    print(f\"   Sequence length std: {np.std(all_seq_lengths):.1f}\")\n",
    "    \n",
    "    if class_imbalance > 3:\n",
    "        issues_found.append(f\"Significant class imbalance (ratio: {class_imbalance:.1f}) - consider class weighting\")\n",
    "    \n",
    "    # Model capacity check\n",
    "    print(\"\\n4. Checking model capacity...\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    data_size = len(dataloader.dataset)\n",
    "    params_per_sample = total_params / data_size\n",
    "    \n",
    "    print(f\"   Parameters per training sample: {params_per_sample:.1f}\")\n",
    "    \n",
    "    if params_per_sample > 100:\n",
    "        issues_found.append(\"High parameter-to-data ratio - possible overfitting risk\")\n",
    "    elif params_per_sample < 1:\n",
    "        issues_found.append(\"Low parameter-to-data ratio - model might be undercapacity\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 30)\n",
    "    if issues_found:\n",
    "        print(\"⚠️  Issues Found:\")\n",
    "        for i, issue in enumerate(issues_found, 1):\n",
    "            print(f\"   {i}. {issue}\")\n",
    "    else:\n",
    "        print(\"✅ No obvious issues detected\")\n",
    "    \n",
    "    return issues_found\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostic_results = diagnose_training_issues(model, train_loader, device)\n",
    "\n",
    "# Training tips based on our results\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training Tips for Your Model:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Model successfully trained on synthetic DNA data\")\n",
    "print(\"✅ Good convergence and generalization observed\")\n",
    "print(\"✅ Reasonable parameter efficiency\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Try with real genomic datasets\")\n",
    "print(\"- Experiment with different sequence lengths\")\n",
    "print(\"- Test transfer learning capabilities\")\n",
    "print(\"- Optimize for your specific use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ecd25",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial covered the complete training pipeline for Hyena-GLT models:\n",
    "\n",
    "### What We Accomplished:\n",
    "1. **Data Preparation**: Created synthetic DNA sequences with meaningful labels\n",
    "2. **Model Configuration**: Set up Hyena-GLT for binary classification\n",
    "3. **Training Loop**: Implemented complete training with validation\n",
    "4. **Monitoring**: Tracked metrics and visualized progress\n",
    "5. **Evaluation**: Analyzed model performance in detail\n",
    "6. **Checkpointing**: Saved and loaded model states\n",
    "7. **Troubleshooting**: Diagnosed potential training issues\n",
    "\n",
    "### Key Training Insights:\n",
    "- **Compression Benefits**: BLT tokenization reduces computational requirements\n",
    "- **Stable Training**: Hyena blocks provide stable gradient flow\n",
    "- **Good Generalization**: Proper regularization prevents overfitting\n",
    "- **Efficient Memory Usage**: Linear complexity enables longer sequences\n",
    "\n",
    "### Next Steps:\n",
    "1. **Real Data**: Apply to actual genomic datasets\n",
    "2. **Advanced Training**: Multi-task learning, transfer learning\n",
    "3. **Optimization**: Mixed precision, distributed training\n",
    "4. **Fine-tuning**: Task-specific adaptations\n",
    "\n",
    "### Resources:\n",
    "- [Fine-tuning Tutorial](05_fine_tuning.ipynb)\n",
    "- [Advanced Techniques](06_advanced_training.ipynb)\n",
    "- [Production Deployment](07_deployment.ipynb)\n",
    "- [Example Scripts](../examples/)\n",
    "\n",
    "You now have the foundation to train Hyena-GLT models for your specific genomic modeling tasks!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
