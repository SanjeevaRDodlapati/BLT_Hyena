{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b6c4c8",
   "metadata": {},
   "source": [
    "# Fine-tuning Hyena-GLT Models\n",
    "\n",
    "This notebook demonstrates how to fine-tune pre-trained Hyena-GLT models for specific genomic tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand fine-tuning strategies for genomic models\n",
    "- Implement task-specific adaptations\n",
    "- Apply LoRA and other parameter-efficient methods\n",
    "- Monitor and optimize fine-tuning performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe9ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Hyena-GLT imports\n",
    "from hyena_glt.models import HyenaGLT\n",
    "from hyena_glt.tokenizer import GenomicTokenizer\n",
    "from hyena_glt.training import Trainer\n",
    "from hyena_glt.data import GenomicDataset\n",
    "from hyena_glt.utils import (\n",
    "    plot_training_curves,\n",
    "    count_parameters,\n",
    "    validate_model_config\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Fine-tuning environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0857a9",
   "metadata": {},
   "source": [
    "## 1. Fine-tuning Strategies\n",
    "\n",
    "### Full Fine-tuning vs Parameter-Efficient Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95944c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningStrategy:\n",
    "    \"\"\"Base class for fine-tuning strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, model, task_type='classification'):\n",
    "        self.model = model\n",
    "        self.task_type = task_type\n",
    "        \n",
    "    def prepare_model(self):\n",
    "        \"\"\"Prepare model for fine-tuning\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\"Get parameters to optimize\"\"\"\n",
    "        return self.model.parameters()\n",
    "\n",
    "class FullFineTuning(FineTuningStrategy):\n",
    "    \"\"\"Full model fine-tuning\"\"\"\n",
    "    \n",
    "    def prepare_model(self):\n",
    "        # Unfreeze all parameters\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # Add task-specific head\n",
    "        if self.task_type == 'classification':\n",
    "            self.model.classifier = nn.Linear(\n",
    "                self.model.config.hidden_size, \n",
    "                self.model.config.num_classes\n",
    "            )\n",
    "        elif self.task_type == 'regression':\n",
    "            self.model.regressor = nn.Linear(\n",
    "                self.model.config.hidden_size, 1\n",
    "            )\n",
    "            \n",
    "        print(f\"Full fine-tuning prepared. Trainable parameters: {count_parameters(self.model)}\")\n",
    "\n",
    "class LoRAFineTuning(FineTuningStrategy):\n",
    "    \"\"\"LoRA (Low-Rank Adaptation) fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, task_type='classification', rank=16, alpha=32):\n",
    "        super().__init__(model, task_type)\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.lora_modules = {}\n",
    "        \n",
    "    def add_lora_layer(self, module, name):\n",
    "        \"\"\"Add LoRA adaptation to a linear layer\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            in_features = module.in_features\n",
    "            out_features = module.out_features\n",
    "            \n",
    "            # Create LoRA matrices\n",
    "            lora_A = nn.Parameter(torch.randn(self.rank, in_features) * 0.01)\n",
    "            lora_B = nn.Parameter(torch.zeros(out_features, self.rank))\n",
    "            \n",
    "            self.lora_modules[name] = {\n",
    "                'A': lora_A,\n",
    "                'B': lora_B,\n",
    "                'original': module\n",
    "            }\n",
    "            \n",
    "            # Freeze original weights\n",
    "            module.weight.requires_grad = False\n",
    "            if module.bias is not None:\n",
    "                module.bias.requires_grad = False\n",
    "                \n",
    "    def prepare_model(self):\n",
    "        # Freeze base model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Add LoRA to attention and MLP layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            if 'attention' in name or 'mlp' in name:\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    self.add_lora_layer(module, name)\n",
    "                    \n",
    "        # Register LoRA parameters\n",
    "        for name, lora_dict in self.lora_modules.items():\n",
    "            self.model.register_parameter(f'lora_A_{name}', lora_dict['A'])\n",
    "            self.model.register_parameter(f'lora_B_{name}', lora_dict['B'])\n",
    "            \n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"LoRA fine-tuning prepared. Trainable parameters: {trainable_params}\")\n",
    "        \n",
    "    def get_trainable_parameters(self):\n",
    "        return [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "# Example usage\n",
    "config = {\n",
    "    'vocab_size': 4096,\n",
    "    'hidden_size': 512,\n",
    "    'num_layers': 8,\n",
    "    'num_classes': 10\n",
    "}\n",
    "\n",
    "model = HyenaGLT(config)\n",
    "print(f\"Base model parameters: {count_parameters(model)}\")\n",
    "\n",
    "# Compare strategies\n",
    "full_ft = FullFineTuning(model, 'classification')\n",
    "lora_ft = LoRAFineTuning(model, 'classification', rank=16)\n",
    "\n",
    "print(\"\\nFine-tuning strategies comparison:\")\n",
    "print(f\"Full fine-tuning: Updates all {count_parameters(model)} parameters\")\n",
    "print(f\"LoRA (rank {lora_ft.rank}): Updates ~{lora_ft.rank * 2 * config['hidden_size']} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a951e3e0",
   "metadata": {},
   "source": [
    "## 2. Task-Specific Adaptations\n",
    "\n",
    "### Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bfcd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenomicClassificationHead(nn.Module):\n",
    "    \"\"\"Task-specific head for genomic sequence classification\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # Pool sequence representations\n",
    "        pooled = hidden_states.mean(dim=1)  # Average pooling\n",
    "        pooled = self.layer_norm(pooled)\n",
    "        pooled = self.dropout(pooled)\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "class GenomicRegressionHead(nn.Module):\n",
    "    \"\"\"Task-specific head for genomic regression tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, output_dim=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, output_dim)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        pooled = hidden_states.mean(dim=1)\n",
    "        pooled = self.layer_norm(pooled)\n",
    "        return self.regressor(pooled)\n",
    "\n",
    "class MultiTaskHead(nn.Module):\n",
    "    \"\"\"Multi-task learning head\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, task_configs):\n",
    "        super().__init__()\n",
    "        self.task_configs = task_configs\n",
    "        self.shared_layer = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.task_heads = nn.ModuleDict()\n",
    "        for task_name, config in task_configs.items():\n",
    "            if config['type'] == 'classification':\n",
    "                self.task_heads[task_name] = nn.Linear(hidden_size, config['num_classes'])\n",
    "            elif config['type'] == 'regression':\n",
    "                self.task_heads[task_name] = nn.Linear(hidden_size, config.get('output_dim', 1))\n",
    "                \n",
    "    def forward(self, hidden_states, task_name=None):\n",
    "        # Shared representation\n",
    "        pooled = hidden_states.mean(dim=1)\n",
    "        shared = torch.relu(self.shared_layer(pooled))\n",
    "        \n",
    "        if task_name:\n",
    "            return self.task_heads[task_name](shared)\n",
    "        else:\n",
    "            # Return all task outputs\n",
    "            outputs = {}\n",
    "            for name, head in self.task_heads.items():\n",
    "                outputs[name] = head(shared)\n",
    "            return outputs\n",
    "\n",
    "# Example task configurations\n",
    "task_configs = {\n",
    "    'promoter_prediction': {'type': 'classification', 'num_classes': 2},\n",
    "    'expression_level': {'type': 'regression', 'output_dim': 1},\n",
    "    'regulatory_elements': {'type': 'classification', 'num_classes': 5}\n",
    "}\n",
    "\n",
    "# Create task heads\n",
    "hidden_size = 512\n",
    "classification_head = GenomicClassificationHead(hidden_size, num_classes=10)\n",
    "regression_head = GenomicRegressionHead(hidden_size)\n",
    "multitask_head = MultiTaskHead(hidden_size, task_configs)\n",
    "\n",
    "print(\"Task-specific heads created successfully!\")\n",
    "print(f\"Classification head parameters: {count_parameters(classification_head)}\")\n",
    "print(f\"Regression head parameters: {count_parameters(regression_head)}\")\n",
    "print(f\"Multi-task head parameters: {count_parameters(multitask_head)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbb303",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e7885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    \"\"\"Complete fine-tuning pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model, strategy, task_head, device='cuda'):\n",
    "        self.model = model\n",
    "        self.strategy = strategy\n",
    "        self.task_head = task_head\n",
    "        self.device = device\n",
    "        \n",
    "        # Prepare model for fine-tuning\n",
    "        self.strategy.prepare_model()\n",
    "        \n",
    "        # Add task head\n",
    "        self.model.task_head = task_head\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def create_optimizer(self, learning_rate=1e-4, weight_decay=0.01):\n",
    "        \"\"\"Create optimizer for trainable parameters\"\"\"\n",
    "        if hasattr(self.strategy, 'get_trainable_parameters'):\n",
    "            params = self.strategy.get_trainable_parameters()\n",
    "        else:\n",
    "            params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "            \n",
    "        # Add task head parameters\n",
    "        params.extend([p for p in self.task_head.parameters() if p.requires_grad])\n",
    "        \n",
    "        return torch.optim.AdamW(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    def create_scheduler(self, optimizer, num_training_steps):\n",
    "        \"\"\"Create learning rate scheduler\"\"\"\n",
    "        from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "        return CosineAnnealingLR(optimizer, T_max=num_training_steps)\n",
    "    \n",
    "    def train_step(self, batch, optimizer, criterion):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(self.device)\n",
    "        labels = batch['labels'].to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(input_ids)\n",
    "        logits = self.task_head(outputs.last_hidden_state)\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss.item(), logits\n",
    "    \n",
    "    def evaluate_step(self, batch, criterion):\n",
    "        \"\"\"Single evaluation step\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_ids)\n",
    "            logits = self.task_head(outputs.last_hidden_state)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "        return loss.item(), logits\n",
    "    \n",
    "    def fine_tune(self, train_loader, val_loader, num_epochs=5, \n",
    "                  learning_rate=1e-4, save_path=None):\n",
    "        \"\"\"Complete fine-tuning process\"\"\"\n",
    "        \n",
    "        # Setup training components\n",
    "        optimizer = self.create_optimizer(learning_rate)\n",
    "        num_training_steps = len(train_loader) * num_epochs\n",
    "        scheduler = self.create_scheduler(optimizer, num_training_steps)\n",
    "        \n",
    "        # Loss function (assuming classification)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training history\n",
    "        history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss = 0.0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                loss, logits = self.train_step(batch, optimizer, criterion)\n",
    "                train_loss += loss\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                train_total += batch['labels'].size(0)\n",
    "                train_correct += (predicted == batch['labels'].to(self.device)).sum().item()\n",
    "                \n",
    "                scheduler.step()\n",
    "                \n",
    "                if batch_idx % 50 == 0:\n",
    "                    print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            for batch in val_loader:\n",
    "                loss, logits = self.evaluate_step(batch, criterion)\n",
    "                val_loss += loss\n",
    "                \n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += batch['labels'].size(0)\n",
    "                val_correct += (predicted == batch['labels'].to(self.device)).sum().item()\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            train_acc = 100 * train_correct / train_total\n",
    "            val_acc = 100 * val_correct / val_total\n",
    "            \n",
    "            # Store history\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss and save_path:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save({\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'task_head_state_dict': self.task_head.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'val_loss': val_loss\n",
    "                }, save_path)\n",
    "                print(f\"Best model saved to {save_path}\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "print(\"Fine-tuning pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12dbb4b",
   "metadata": {},
   "source": [
    "## 4. Practical Fine-tuning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d45f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic fine-tuning data\n",
    "def create_fine_tuning_data(num_samples=1000, seq_length=512):\n",
    "    \"\"\"Create synthetic genomic data for fine-tuning\"\"\"\n",
    "    \n",
    "    # Generate random DNA sequences\n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Generate sequence\n",
    "        seq = ''.join(np.random.choice(nucleotides, seq_length))\n",
    "        \n",
    "        # Create synthetic label based on GC content\n",
    "        gc_content = (seq.count('G') + seq.count('C')) / len(seq)\n",
    "        label = 1 if gc_content > 0.5 else 0  # High vs low GC content\n",
    "        \n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Create dataset\n",
    "train_sequences, train_labels = create_fine_tuning_data(800, 256)\n",
    "val_sequences, val_labels = create_fine_tuning_data(200, 256)\n",
    "\n",
    "print(f\"Created training data: {len(train_sequences)} samples\")\n",
    "print(f\"Created validation data: {len(val_sequences)} samples\")\n",
    "print(f\"Label distribution - Train: {np.bincount(train_labels)}\")\n",
    "print(f\"Label distribution - Val: {np.bincount(val_labels)}\")\n",
    "\n",
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# GC content distribution\n",
    "train_gc = [(seq.count('G') + seq.count('C')) / len(seq) for seq in train_sequences]\n",
    "val_gc = [(seq.count('G') + seq.count('C')) / len(seq) for seq in val_sequences]\n",
    "\n",
    "axes[0].hist(train_gc, alpha=0.7, label='Train', bins=20)\n",
    "axes[0].hist(val_gc, alpha=0.7, label='Val', bins=20)\n",
    "axes[0].set_xlabel('GC Content')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('GC Content Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Label distribution\n",
    "axes[1].bar(['Low GC', 'High GC'], np.bincount(train_labels), alpha=0.7, label='Train')\n",
    "axes[1].bar(['Low GC', 'High GC'], np.bincount(val_labels), alpha=0.7, label='Val')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Label Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca5052",
   "metadata": {},
   "source": [
    "## 5. Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48afb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fine_tuning_results(history, strategy_name):\n",
    "    \"\"\"Plot fine-tuning training curves\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    axes[0, 0].set_title(f'{strategy_name} - Loss Curves')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy')\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "    axes[0, 1].set_title(f'{strategy_name} - Accuracy Curves')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Loss difference\n",
    "    loss_diff = np.array(history['val_loss']) - np.array(history['train_loss'])\n",
    "    axes[1, 0].plot(epochs, loss_diff, 'g-', linewidth=2)\n",
    "    axes[1, 0].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[1, 0].set_title('Overfitting Analysis (Val Loss - Train Loss)')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss Difference')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Final metrics\n",
    "    final_metrics = {\n",
    "        'Final Train Loss': history['train_loss'][-1],\n",
    "        'Final Val Loss': history['val_loss'][-1],\n",
    "        'Final Train Acc': history['train_acc'][-1],\n",
    "        'Final Val Acc': history['val_acc'][-1],\n",
    "        'Best Val Acc': max(history['val_acc'])\n",
    "    }\n",
    "    \n",
    "    metrics_text = '\\n'.join([f'{k}: {v:.3f}' for k, v in final_metrics.items()])\n",
    "    axes[1, 1].text(0.1, 0.5, metrics_text, fontsize=12, \n",
    "                    verticalalignment='center', fontfamily='monospace')\n",
    "    axes[1, 1].set_title('Final Metrics')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return final_metrics\n",
    "\n",
    "# Simulate training results for demonstration\n",
    "def simulate_training_history(num_epochs=5, strategy='LoRA'):\n",
    "    \"\"\"Simulate realistic training history\"\"\"\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    # Different convergence patterns for different strategies\n",
    "    if strategy == 'LoRA':\n",
    "        # LoRA typically has slower but more stable convergence\n",
    "        base_train_loss, base_val_loss = 1.2, 1.3\n",
    "        base_train_acc, base_val_acc = 60, 58\n",
    "        decay_rate = 0.15\n",
    "    else:  # Full fine-tuning\n",
    "        # Full fine-tuning converges faster but may overfit\n",
    "        base_train_loss, base_val_loss = 1.0, 1.1\n",
    "        base_train_acc, base_val_acc = 65, 62\n",
    "        decay_rate = 0.25\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Exponential decay with noise\n",
    "        noise = np.random.normal(0, 0.02)\n",
    "        \n",
    "        train_loss = base_train_loss * np.exp(-decay_rate * epoch) + noise\n",
    "        val_loss = base_val_loss * np.exp(-decay_rate * epoch * 0.8) + noise * 1.5\n",
    "        \n",
    "        train_acc = 100 - (100 - base_train_acc) * np.exp(-decay_rate * epoch) + noise * 2\n",
    "        val_acc = 100 - (100 - base_val_acc) * np.exp(-decay_rate * epoch * 0.8) + noise * 3\n",
    "        \n",
    "        history['train_loss'].append(max(0.1, train_loss))\n",
    "        history['val_loss'].append(max(0.1, val_loss))\n",
    "        history['train_acc'].append(min(95, max(50, train_acc)))\n",
    "        history['val_acc'].append(min(90, max(45, val_acc)))\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Compare strategies\n",
    "lora_history = simulate_training_history(5, 'LoRA')\n",
    "full_history = simulate_training_history(5, 'Full')\n",
    "\n",
    "print(\"LoRA Fine-tuning Results:\")\n",
    "lora_metrics = plot_fine_tuning_results(lora_history, 'LoRA Fine-tuning')\n",
    "\n",
    "print(\"\\nFull Fine-tuning Results:\")\n",
    "full_metrics = plot_fine_tuning_results(full_history, 'Full Fine-tuning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f550d1",
   "metadata": {},
   "source": [
    "## 6. Best Practices and Tips\n",
    "\n",
    "### Fine-tuning Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningBestPractices:\n",
    "    \"\"\"Collection of fine-tuning best practices\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def learning_rate_recommendations():\n",
    "        \"\"\"Provide learning rate recommendations\"\"\"\n",
    "        recommendations = {\n",
    "            'Full Fine-tuning': {\n",
    "                'range': '1e-5 to 5e-5',\n",
    "                'reasoning': 'Lower LR to preserve pre-trained features',\n",
    "                'typical': 2e-5\n",
    "            },\n",
    "            'LoRA': {\n",
    "                'range': '1e-4 to 1e-3',\n",
    "                'reasoning': 'Higher LR for adaptation layers only',\n",
    "                'typical': 3e-4\n",
    "            },\n",
    "            'Task Head Only': {\n",
    "                'range': '1e-3 to 1e-2',\n",
    "                'reasoning': 'Random initialization requires higher LR',\n",
    "                'typical': 5e-3\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"Learning Rate Recommendations:\")\n",
    "        print(\"=\" * 50)\n",
    "        for strategy, info in recommendations.items():\n",
    "            print(f\"\\n{strategy}:\")\n",
    "            print(f\"  Range: {info['range']}\")\n",
    "            print(f\"  Typical: {info['typical']}\")\n",
    "            print(f\"  Reasoning: {info['reasoning']}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_requirements():\n",
    "        \"\"\"Provide data size recommendations\"\"\"\n",
    "        requirements = {\n",
    "            'Minimum': {\n",
    "                'samples': 1000,\n",
    "                'note': 'For simple binary classification with LoRA'\n",
    "            },\n",
    "            'Recommended': {\n",
    "                'samples': 10000,\n",
    "                'note': 'For robust performance across tasks'\n",
    "            },\n",
    "            'Optimal': {\n",
    "                'samples': 100000,\n",
    "                'note': 'For full fine-tuning and complex tasks'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\nData Size Requirements:\")\n",
    "        print(\"=\" * 50)\n",
    "        for level, info in requirements.items():\n",
    "            print(f\"{level}: {info['samples']:,} samples\")\n",
    "            print(f\"  Note: {info['note']}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def regularization_strategies():\n",
    "        \"\"\"Regularization techniques for fine-tuning\"\"\"\n",
    "        strategies = {\n",
    "            'Dropout': {\n",
    "                'value': 0.1,\n",
    "                'where': 'Task head and attention layers'\n",
    "            },\n",
    "            'Weight Decay': {\n",
    "                'value': 0.01,\n",
    "                'where': 'All trainable parameters'\n",
    "            },\n",
    "            'Gradient Clipping': {\n",
    "                'value': 1.0,\n",
    "                'where': 'Global norm clipping'\n",
    "            },\n",
    "            'Layer Freezing': {\n",
    "                'strategy': 'Gradually unfreeze layers',\n",
    "                'where': 'Start with embeddings, then lower layers'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\nRegularization Strategies:\")\n",
    "        print(\"=\" * 50)\n",
    "        for technique, info in strategies.items():\n",
    "            print(f\"\\n{technique}:\")\n",
    "            if 'value' in info:\n",
    "                print(f\"  Recommended value: {info['value']}\")\n",
    "            if 'strategy' in info:\n",
    "                print(f\"  Strategy: {info['strategy']}\")\n",
    "            print(f\"  Application: {info['where']}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluation_checklist():\n",
    "        \"\"\"Checklist for proper evaluation\"\"\"\n",
    "        checklist = [\n",
    "            \"✓ Hold-out test set (never used during training)\",\n",
    "            \"✓ Stratified sampling for balanced evaluation\",\n",
    "            \"✓ Multiple random seeds for statistical significance\",\n",
    "            \"✓ Cross-validation for small datasets\",\n",
    "            \"✓ Task-specific metrics (not just accuracy)\",\n",
    "            \"✓ Confusion matrix analysis\",\n",
    "            \"✓ Learning curve analysis\",\n",
    "            \"✓ Computational cost assessment\",\n",
    "            \"✓ Comparison with baseline models\",\n",
    "            \"✓ Error analysis and failure cases\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nEvaluation Checklist:\")\n",
    "        print(\"=\" * 50)\n",
    "        for item in checklist:\n",
    "            print(item)\n",
    "\n",
    "# Display best practices\n",
    "practices = FineTuningBestPractices()\n",
    "practices.learning_rate_recommendations()\n",
    "practices.data_requirements()\n",
    "practices.regularization_strategies()\n",
    "practices.evaluation_checklist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429d298",
   "metadata": {},
   "source": [
    "## 7. Advanced Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816fb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradualUnfreezing:\n",
    "    \"\"\"Implement gradual unfreezing strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, model, unfreeze_schedule):\n",
    "        self.model = model\n",
    "        self.unfreeze_schedule = unfreeze_schedule\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "    def update_frozen_layers(self, epoch):\n",
    "        \"\"\"Update which layers are frozen based on epoch\"\"\"\n",
    "        self.current_epoch = epoch\n",
    "        \n",
    "        for layer_group, unfreeze_epoch in self.unfreeze_schedule.items():\n",
    "            if epoch >= unfreeze_epoch:\n",
    "                self._unfreeze_layer_group(layer_group)\n",
    "                print(f\"Epoch {epoch}: Unfroze {layer_group}\")\n",
    "    \n",
    "    def _unfreeze_layer_group(self, layer_group):\n",
    "        \"\"\"Unfreeze specific layer group\"\"\"\n",
    "        if layer_group == 'embeddings':\n",
    "            for param in self.model.embeddings.parameters():\n",
    "                param.requires_grad = True\n",
    "        elif layer_group.startswith('layer_'):\n",
    "            layer_idx = int(layer_group.split('_')[1])\n",
    "            for param in self.model.layers[layer_idx].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "class DiscriminativeLearningRates:\n",
    "    \"\"\"Apply different learning rates to different layer groups\"\"\"\n",
    "    \n",
    "    def __init__(self, model, base_lr=1e-4, decay_factor=0.5):\n",
    "        self.model = model\n",
    "        self.base_lr = base_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        \n",
    "    def create_param_groups(self):\n",
    "        \"\"\"Create parameter groups with different learning rates\"\"\"\n",
    "        param_groups = []\n",
    "        \n",
    "        # Task head - highest learning rate\n",
    "        if hasattr(self.model, 'task_head'):\n",
    "            param_groups.append({\n",
    "                'params': self.model.task_head.parameters(),\n",
    "                'lr': self.base_lr\n",
    "            })\n",
    "        \n",
    "        # Upper layers - medium learning rate\n",
    "        if hasattr(self.model, 'layers'):\n",
    "            num_layers = len(self.model.layers)\n",
    "            upper_layers = self.model.layers[num_layers//2:]\n",
    "            param_groups.append({\n",
    "                'params': [p for layer in upper_layers for p in layer.parameters()],\n",
    "                'lr': self.base_lr * self.decay_factor\n",
    "            })\n",
    "            \n",
    "            # Lower layers - lowest learning rate\n",
    "            lower_layers = self.model.layers[:num_layers//2]\n",
    "            param_groups.append({\n",
    "                'params': [p for layer in lower_layers for p in layer.parameters()],\n",
    "                'lr': self.base_lr * (self.decay_factor ** 2)\n",
    "            })\n",
    "        \n",
    "        # Embeddings - very low learning rate\n",
    "        if hasattr(self.model, 'embeddings'):\n",
    "            param_groups.append({\n",
    "                'params': self.model.embeddings.parameters(),\n",
    "                'lr': self.base_lr * (self.decay_factor ** 3)\n",
    "            })\n",
    "        \n",
    "        return param_groups\n",
    "\n",
    "class CurriculumLearning:\n",
    "    \"\"\"Implement curriculum learning for genomic tasks\"\"\"\n",
    "    \n",
    "    def __init__(self, difficulty_fn, initial_ratio=0.3, final_ratio=1.0):\n",
    "        self.difficulty_fn = difficulty_fn\n",
    "        self.initial_ratio = initial_ratio\n",
    "        self.final_ratio = final_ratio\n",
    "        \n",
    "    def get_curriculum_subset(self, dataset, epoch, total_epochs):\n",
    "        \"\"\"Get subset of data based on curriculum\"\"\"\n",
    "        # Calculate current ratio\n",
    "        progress = epoch / total_epochs\n",
    "        current_ratio = self.initial_ratio + progress * (self.final_ratio - self.initial_ratio)\n",
    "        \n",
    "        # Sort by difficulty\n",
    "        difficulties = [self.difficulty_fn(sample) for sample in dataset]\n",
    "        sorted_indices = np.argsort(difficulties)\n",
    "        \n",
    "        # Select subset\n",
    "        num_samples = int(len(dataset) * current_ratio)\n",
    "        selected_indices = sorted_indices[:num_samples]\n",
    "        \n",
    "        return selected_indices\n",
    "\n",
    "# Example difficulty functions\n",
    "def sequence_length_difficulty(sample):\n",
    "    \"\"\"Difficulty based on sequence length\"\"\"\n",
    "    return len(sample['sequence'])\n",
    "\n",
    "def gc_content_difficulty(sample):\n",
    "    \"\"\"Difficulty based on GC content deviation from 50%\"\"\"\n",
    "    seq = sample['sequence']\n",
    "    gc_content = (seq.count('G') + seq.count('C')) / len(seq)\n",
    "    return abs(gc_content - 0.5)\n",
    "\n",
    "print(\"Advanced fine-tuning techniques implemented!\")\n",
    "print(\"\\nTechniques available:\")\n",
    "print(\"- Gradual Unfreezing: Progressively unfreeze model layers\")\n",
    "print(\"- Discriminative Learning Rates: Different LR for different layers\")\n",
    "print(\"- Curriculum Learning: Train on easier examples first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d14b3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered comprehensive fine-tuning strategies for Hyena-GLT models:\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Fine-tuning Strategies**: Full fine-tuning vs LoRA vs parameter-efficient methods\n",
    "2. **Task Adaptation**: Classification, regression, and multi-task heads\n",
    "3. **Training Pipeline**: Complete implementation with monitoring\n",
    "4. **Best Practices**: Learning rates, regularization, evaluation\n",
    "5. **Advanced Techniques**: Gradual unfreezing, discriminative LR, curriculum learning\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different fine-tuning strategies for your specific task\n",
    "- Implement proper evaluation and comparison frameworks\n",
    "- Consider computational constraints when choosing strategies\n",
    "- Monitor for overfitting and apply appropriate regularization\n",
    "\n",
    "Continue to the next notebook: **07_generation.ipynb** for text generation with Hyena-GLT!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
