{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82eb31e8",
   "metadata": {},
   "source": [
    "# Tutorial 03: Striped Hyena Architecture\n",
    "\n",
    "This notebook explores the Striped Hyena architecture, which forms the core of our genomic sequence modeling framework. We'll dive into the hybrid attention-convolution mechanism and its advantages for long sequence processing.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the Striped Hyena architecture principles\n",
    "- Explore hybrid attention-convolution mechanisms\n",
    "- Analyze computational efficiency for long sequences\n",
    "- Implement custom Hyena layers and configurations\n",
    "- Compare with traditional transformer architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ca952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from hyena_glt.models import HyenaGLT\n",
    "from hyena_glt.models.hyena_blocks import HyenaBlock\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Striped Hyena Architecture Deep Dive\")\n",
    "print(\"====================================\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd3f30",
   "metadata": {},
   "source": [
    "## 1. Hyena Architecture Overview\n",
    "\n",
    "The Striped Hyena architecture combines the best of both worlds:\n",
    "- **Local Processing**: Convolution layers for capturing local patterns\n",
    "- **Global Context**: Attention mechanisms for long-range dependencies\n",
    "- **Efficiency**: Subquadratic complexity for long sequences\n",
    "- **Flexibility**: Configurable attention/convolution ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea7d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample Hyena model\n",
    "config = {\n",
    "    'vocab_size': 4096,\n",
    "    'hidden_size': 512,\n",
    "    'num_layers': 6,\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 2048,\n",
    "    'max_position_embeddings': 8192,\n",
    "    'hyena_config': {\n",
    "        'conv_kernel_size': 7,\n",
    "        'conv_groups': 1,\n",
    "        'attention_ratio': 0.5  # 50% attention, 50% convolution\n",
    "    }\n",
    "}\n",
    "\n",
    "model = HyenaGLT(**config)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"=== Model Architecture Summary ===\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"Model size: {sum(p.numel() * p.element_size() for p in model.parameters()) / 1024**2:.2f} MB\")\n",
    "print(f\"Number of layers: {config['num_layers']}\")\n",
    "print(f\"Hidden size: {config['hidden_size']}\")\n",
    "print(f\"Attention heads: {config['num_attention_heads']}\")\n",
    "print(f\"Max sequence length: {config['max_position_embeddings']}\")\n",
    "print(f\"Attention ratio: {config['hyena_config']['attention_ratio']*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aaf647",
   "metadata": {},
   "source": [
    "## 2. Exploring Individual Hyena Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual Hyena components for analysis\n",
    "hidden_size = 512\n",
    "num_heads = 8\n",
    "sequence_length = 1024\n",
    "batch_size = 2\n",
    "\n",
    "# Create a Hyena block\n",
    "hyena_block = HyenaBlock(\n",
    "    hidden_size=hidden_size,\n",
    "    num_attention_heads=num_heads,\n",
    "    intermediate_size=2048,\n",
    "    conv_kernel_size=7,\n",
    "    attention_ratio=0.5\n",
    ").to(device)\n",
    "\n",
    "# Create sample input\n",
    "sample_input = torch.randn(batch_size, sequence_length, hidden_size).to(device)\n",
    "\n",
    "print(\"=== Hyena Block Analysis ===\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Forward pass through Hyena block\n",
    "with torch.no_grad():\n",
    "    output = hyena_block(sample_input)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Input-output shape match: {sample_input.shape == output.shape}\")\n",
    "\n",
    "# Analyze attention patterns (if available)\n",
    "if hasattr(hyena_block, 'attention') and hasattr(hyena_block.attention, 'attention_weights'):\n",
    "    attention_weights = hyena_block.attention.attention_weights\n",
    "    print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "    print(f\"Attention weight range: [{attention_weights.min():.3f}, {attention_weights.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6cdf11",
   "metadata": {},
   "source": [
    "## 3. Computational Complexity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_computational_complexity(model, sequence_lengths, vocab_size=4096):\n",
    "    \"\"\"Measure computational complexity across different sequence lengths.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for seq_len in sequence_lengths:\n",
    "        # Create random input\n",
    "        input_ids = torch.randint(0, vocab_size, (1, seq_len)).to(device)\n",
    "\n",
    "        # Measure memory and time\n",
    "        torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_memory = torch.cuda.memory_allocated() if device.type == 'cuda' else 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "        end_time = time.time()\n",
    "        peak_memory = torch.cuda.max_memory_allocated() if device.type == 'cuda' else 0\n",
    "\n",
    "        results.append({\n",
    "            'sequence_length': seq_len,\n",
    "            'time': end_time - start_time,\n",
    "            'memory_mb': (peak_memory - start_memory) / 1024**2,\n",
    "            'tokens_per_second': seq_len / (end_time - start_time)\n",
    "        })\n",
    "\n",
    "        # Clear memory\n",
    "        del input_ids, outputs\n",
    "        torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different sequence lengths\n",
    "sequence_lengths = [128, 256, 512, 1024, 2048, 4096]\n",
    "if device.type == 'cpu':\n",
    "    sequence_lengths = [128, 256, 512, 1024]  # Smaller for CPU\n",
    "\n",
    "print(\"=== Computational Complexity Analysis ===\")\n",
    "print(\"Measuring performance across different sequence lengths...\")\n",
    "\n",
    "complexity_results = measure_computational_complexity(model, sequence_lengths)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults:\")\n",
    "print(f\"{'Seq Len':<8} {'Time (s)':<10} {'Memory (MB)':<12} {'Tokens/sec':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for result in complexity_results:\n",
    "    print(f\"{result['sequence_length']:<8} \"\n",
    "          f\"{result['time']:<10.3f} \"\n",
    "          f\"{result['memory_mb']:<12.1f} \"\n",
    "          f\"{result['tokens_per_second']:<12.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20b381",
   "metadata": {},
   "source": [
    "## 4. Visualizing Complexity Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "seq_lengths = [r['sequence_length'] for r in complexity_results]\n",
    "times = [r['time'] for r in complexity_results]\n",
    "memories = [r['memory_mb'] for r in complexity_results]\n",
    "throughputs = [r['tokens_per_second'] for r in complexity_results]\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Hyena-GLT Computational Complexity Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Time complexity\n",
    "ax1.loglog(seq_lengths, times, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Sequence Length')\n",
    "ax1.set_ylabel('Processing Time (seconds)')\n",
    "ax1.set_title('Time Complexity')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Fit and plot theoretical complexity lines\n",
    "if len(seq_lengths) > 2:\n",
    "    # Quadratic reference (O(n²))\n",
    "    quad_ref = [times[0] * (s/seq_lengths[0])**2 for s in seq_lengths]\n",
    "    ax1.loglog(seq_lengths, quad_ref, 'r--', alpha=0.7, label='O(n²) reference')\n",
    "\n",
    "    # Linear reference (O(n))\n",
    "    linear_ref = [times[0] * (s/seq_lengths[0]) for s in seq_lengths]\n",
    "    ax1.loglog(seq_lengths, linear_ref, 'g--', alpha=0.7, label='O(n) reference')\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "# Memory complexity\n",
    "ax2.loglog(seq_lengths, memories, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Sequence Length')\n",
    "ax2.set_ylabel('Peak Memory (MB)')\n",
    "ax2.set_title('Memory Complexity')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Throughput analysis\n",
    "ax3.semilogx(seq_lengths, throughputs, 'go-', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Sequence Length')\n",
    "ax3.set_ylabel('Throughput (tokens/second)')\n",
    "ax3.set_title('Processing Throughput')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency ratio (throughput per memory)\n",
    "efficiency = [t/m if m > 0 else 0 for t, m in zip(throughputs, memories, strict=False)]\n",
    "ax4.semilogx(seq_lengths, efficiency, 'mo-', linewidth=2, markersize=8)\n",
    "ax4.set_xlabel('Sequence Length')\n",
    "ax4.set_ylabel('Efficiency (tokens/sec/MB)')\n",
    "ax4.set_title('Memory Efficiency')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate complexity growth rates\n",
    "if len(complexity_results) > 1:\n",
    "    time_growth_rate = np.log(times[-1]/times[0]) / np.log(seq_lengths[-1]/seq_lengths[0])\n",
    "    memory_growth_rate = np.log(memories[-1]/memories[0]) / np.log(seq_lengths[-1]/seq_lengths[0])\n",
    "\n",
    "    print(\"\\n=== Complexity Growth Analysis ===\")\n",
    "    print(f\"Time complexity growth rate: O(n^{time_growth_rate:.2f})\")\n",
    "    print(f\"Memory complexity growth rate: O(n^{memory_growth_rate:.2f})\")\n",
    "    print(\"Theoretical quadratic (transformer): O(n^2.00)\")\n",
    "    print(f\"Efficiency improvement over quadratic: {2.0/time_growth_rate:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a1750",
   "metadata": {},
   "source": [
    "## 5. Attention vs Convolution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597c8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_ratios(attention_ratios, sequence_length=1024):\n",
    "    \"\"\"Compare models with different attention/convolution ratios.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    base_config = {\n",
    "        'vocab_size': 1024,\n",
    "        'hidden_size': 256,  # Smaller for faster comparison\n",
    "        'num_layers': 4,\n",
    "        'num_attention_heads': 4,\n",
    "        'intermediate_size': 1024,\n",
    "        'max_position_embeddings': 4096\n",
    "    }\n",
    "\n",
    "    for attention_ratio in attention_ratios:\n",
    "        config = base_config.copy()\n",
    "        config['hyena_config'] = {\n",
    "            'conv_kernel_size': 7,\n",
    "            'conv_groups': 1,\n",
    "            'attention_ratio': attention_ratio\n",
    "        }\n",
    "\n",
    "        # Create model\n",
    "        test_model = HyenaGLT(**config).to(device)\n",
    "        test_model.eval()\n",
    "\n",
    "        # Test performance\n",
    "        input_ids = torch.randint(0, config['vocab_size'], (1, sequence_length)).to(device)\n",
    "\n",
    "        # Measure time and memory\n",
    "        torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "\n",
    "        start_time = time.time()\n",
    "        start_memory = torch.cuda.memory_allocated() if device.type == 'cuda' else 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = test_model(input_ids)\n",
    "\n",
    "        end_time = time.time()\n",
    "        peak_memory = torch.cuda.max_memory_allocated() if device.type == 'cuda' else 0\n",
    "\n",
    "        results.append({\n",
    "            'attention_ratio': attention_ratio,\n",
    "            'conv_ratio': 1 - attention_ratio,\n",
    "            'time': end_time - start_time,\n",
    "            'memory_mb': (peak_memory - start_memory) / 1024**2,\n",
    "            'parameters': sum(p.numel() for p in test_model.parameters()),\n",
    "            'throughput': sequence_length / (end_time - start_time)\n",
    "        })\n",
    "\n",
    "        # Cleanup\n",
    "        del test_model, input_ids, outputs\n",
    "        torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test different attention ratios\n",
    "attention_ratios = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "print(\"=== Attention vs Convolution Ratio Analysis ===\")\n",
    "print(\"Comparing models with different attention/convolution ratios...\")\n",
    "\n",
    "ratio_results = compare_attention_ratios(attention_ratios)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nResults:\")\n",
    "print(f\"{'Attn%':<6} {'Conv%':<6} {'Time(s)':<8} {'Memory(MB)':<10} {'Params':<8} {'Throughput':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for result in ratio_results:\n",
    "    print(f\"{result['attention_ratio']*100:<6.0f} \"\n",
    "          f\"{result['conv_ratio']*100:<6.0f} \"\n",
    "          f\"{result['time']:<8.3f} \"\n",
    "          f\"{result['memory_mb']:<10.1f} \"\n",
    "          f\"{result['parameters']/1000:<8.0f}k \"\n",
    "          f\"{result['throughput']:<10.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538722f3",
   "metadata": {},
   "source": [
    "## 6. Visualizing Attention vs Convolution Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09011d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for visualization\n",
    "attn_ratios = [r['attention_ratio'] for r in ratio_results]\n",
    "conv_ratios = [r['conv_ratio'] for r in ratio_results]\n",
    "ratio_times = [r['time'] for r in ratio_results]\n",
    "ratio_memories = [r['memory_mb'] for r in ratio_results]\n",
    "ratio_throughputs = [r['throughput'] for r in ratio_results]\n",
    "ratio_params = [r['parameters'] for r in ratio_results]\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Attention vs Convolution Trade-off Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Performance vs attention ratio\n",
    "ax1.plot(attn_ratios, ratio_times, 'bo-', linewidth=2, markersize=8, label='Processing Time')\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(attn_ratios, ratio_throughputs, 'ro-', linewidth=2, markersize=8, label='Throughput')\n",
    "ax1.set_xlabel('Attention Ratio')\n",
    "ax1.set_ylabel('Processing Time (s)', color='blue')\n",
    "ax1_twin.set_ylabel('Throughput (tokens/s)', color='red')\n",
    "ax1.set_title('Performance vs Attention Ratio')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory usage\n",
    "ax2.plot(attn_ratios, ratio_memories, 'go-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Attention Ratio')\n",
    "ax2.set_ylabel('Memory Usage (MB)')\n",
    "ax2.set_title('Memory vs Attention Ratio')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter count\n",
    "ax3.plot(attn_ratios, [p/1000 for p in ratio_params], 'mo-', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Attention Ratio')\n",
    "ax3.set_ylabel('Parameters (thousands)')\n",
    "ax3.set_title('Model Size vs Attention Ratio')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Efficiency comparison (throughput per parameter)\n",
    "efficiency_per_param = [t/p for t, p in zip(ratio_throughputs, ratio_params, strict=False)]\n",
    "ax4.plot(attn_ratios, efficiency_per_param, 'co-', linewidth=2, markersize=8)\n",
    "ax4.set_xlabel('Attention Ratio')\n",
    "ax4.set_ylabel('Efficiency (tokens/s/param)')\n",
    "ax4.set_title('Parameter Efficiency')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal attention ratio\n",
    "best_throughput_idx = np.argmax(ratio_throughputs)\n",
    "best_memory_idx = np.argmin(ratio_memories)\n",
    "best_efficiency_idx = np.argmax(efficiency_per_param)\n",
    "\n",
    "print(\"\\n=== Optimal Configuration Analysis ===\")\n",
    "print(f\"Best throughput: {attn_ratios[best_throughput_idx]*100:.0f}% attention ({ratio_throughputs[best_throughput_idx]:.0f} tokens/s)\")\n",
    "print(f\"Best memory efficiency: {attn_ratios[best_memory_idx]*100:.0f}% attention ({ratio_memories[best_memory_idx]:.1f} MB)\")\n",
    "print(f\"Best parameter efficiency: {attn_ratios[best_efficiency_idx]*100:.0f}% attention ({efficiency_per_param[best_efficiency_idx]:.2e} tokens/s/param)\")\n",
    "\n",
    "# Calculate trade-offs\n",
    "pure_attention_perf = ratio_throughputs[-1]  # 100% attention\n",
    "pure_conv_perf = ratio_throughputs[0]        # 0% attention (100% conv)\n",
    "hybrid_perf = ratio_throughputs[len(ratio_throughputs)//2]  # 50% attention\n",
    "\n",
    "print(\"\\n=== Architecture Comparison ===\")\n",
    "print(f\"Pure convolution (0% attention): {pure_conv_perf:.0f} tokens/s\")\n",
    "print(f\"Hybrid (50% attention): {hybrid_perf:.0f} tokens/s\")\n",
    "print(f\"Pure attention (100% attention): {pure_attention_perf:.0f} tokens/s\")\n",
    "print(f\"Hybrid advantage over pure conv: {(hybrid_perf/pure_conv_perf-1)*100:.1f}%\")\n",
    "print(f\"Hybrid advantage over pure attention: {(hybrid_perf/pure_attention_perf-1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db54c0c4",
   "metadata": {},
   "source": [
    "## 7. Custom Hyena Layer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e41e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a custom Hyena layer with genomic-specific optimizations\n",
    "class GenomicHyenaLayer(nn.Module):\n",
    "    \"\"\"Custom Hyena layer optimized for genomic sequences.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, conv_kernel_size=7,\n",
    "                 attention_ratio=0.5, genomic_conv_patterns=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_ratio = attention_ratio\n",
    "\n",
    "        # Split channels between attention and convolution\n",
    "        self.attention_dim = int(hidden_size * attention_ratio)\n",
    "        self.conv_dim = hidden_size - self.attention_dim\n",
    "\n",
    "        # Attention component\n",
    "        if self.attention_dim > 0:\n",
    "            self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=self.attention_dim,\n",
    "                num_heads=min(num_heads, self.attention_dim // 64),\n",
    "                batch_first=True\n",
    "            )\n",
    "\n",
    "        # Convolution component with genomic patterns\n",
    "        if self.conv_dim > 0:\n",
    "            if genomic_conv_patterns:\n",
    "                # Use multiple kernel sizes for different genomic patterns\n",
    "                self.conv_layers = nn.ModuleList([\n",
    "                    nn.Conv1d(self.conv_dim, self.conv_dim//3, kernel_size=3, padding=1, groups=1),  # Codons\n",
    "                    nn.Conv1d(self.conv_dim, self.conv_dim//3, kernel_size=7, padding=3, groups=1),  # Local motifs\n",
    "                    nn.Conv1d(self.conv_dim, self.conv_dim//3, kernel_size=15, padding=7, groups=1), # Longer patterns\n",
    "                ])\n",
    "            else:\n",
    "                self.conv_layers = nn.ModuleList([\n",
    "                    nn.Conv1d(self.conv_dim, self.conv_dim, kernel_size=conv_kernel_size,\n",
    "                             padding=conv_kernel_size//2, groups=1)\n",
    "                ])\n",
    "\n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        residual = x\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Attention pathway\n",
    "        if self.attention_dim > 0:\n",
    "            attn_input = x[:, :, :self.attention_dim]\n",
    "            attn_output, _ = self.attention(attn_input, attn_input, attn_input)\n",
    "            outputs.append(attn_output)\n",
    "\n",
    "        # Convolution pathway\n",
    "        if self.conv_dim > 0:\n",
    "            conv_input = x[:, :, self.attention_dim:].transpose(1, 2)  # (B, C, L)\n",
    "\n",
    "            conv_outputs = []\n",
    "            for conv_layer in self.conv_layers:\n",
    "                conv_out = conv_layer(conv_input)\n",
    "                conv_outputs.append(conv_out)\n",
    "\n",
    "            # Concatenate conv outputs\n",
    "            conv_output = torch.cat(conv_outputs, dim=1).transpose(1, 2)  # (B, L, C)\n",
    "            outputs.append(conv_output)\n",
    "\n",
    "        # Combine pathways\n",
    "        if len(outputs) > 1:\n",
    "            combined = torch.cat(outputs, dim=-1)\n",
    "        else:\n",
    "            combined = outputs[0]\n",
    "\n",
    "        # Apply output projection and residual connection\n",
    "        output = self.output_proj(combined)\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Test custom genomic Hyena layer\n",
    "print(\"=== Custom Genomic Hyena Layer Test ===\")\n",
    "\n",
    "# Create custom layer\n",
    "custom_layer = GenomicHyenaLayer(\n",
    "    hidden_size=512,\n",
    "    num_heads=8,\n",
    "    conv_kernel_size=7,\n",
    "    attention_ratio=0.5,\n",
    "    genomic_conv_patterns=True\n",
    ").to(device)\n",
    "\n",
    "# Test input\n",
    "test_input = torch.randn(2, 1024, 512).to(device)\n",
    "\n",
    "print(f\"Custom layer parameters: {sum(p.numel() for p in custom_layer.parameters()):,}\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    custom_output = custom_layer(test_input)\n",
    "\n",
    "print(f\"Output shape: {custom_output.shape}\")\n",
    "print(\"Output statistics:\")\n",
    "print(f\"  Mean: {custom_output.mean().item():.6f}\")\n",
    "print(f\"  Std: {custom_output.std().item():.6f}\")\n",
    "print(f\"  Min: {custom_output.min().item():.6f}\")\n",
    "print(f\"  Max: {custom_output.max().item():.6f}\")\n",
    "\n",
    "# Compare with standard layer\n",
    "standard_layer = HyenaBlock(\n",
    "    hidden_size=512,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=2048,\n",
    "    conv_kernel_size=7,\n",
    "    attention_ratio=0.5\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    standard_output = standard_layer(test_input)\n",
    "\n",
    "print(\"\\nComparison with standard Hyena layer:\")\n",
    "print(f\"Custom layer params: {sum(p.numel() for p in custom_layer.parameters()):,}\")\n",
    "print(f\"Standard layer params: {sum(p.numel() for p in standard_layer.parameters()):,}\")\n",
    "print(f\"Output similarity (cosine): {torch.nn.functional.cosine_similarity(custom_output.flatten(), standard_output.flatten(), dim=0).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acefd00f",
   "metadata": {},
   "source": [
    "## 8. Architecture Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a0f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Striped Hyena Architecture Recommendations ===\")\n",
    "print()\n",
    "\n",
    "recommendations = {\n",
    "    \"Sequence Length Considerations\": [\n",
    "        \"Short sequences (<1K): Higher attention ratio (70-80%) for global context\",\n",
    "        \"Medium sequences (1K-10K): Balanced ratio (40-60%) for optimal performance\",\n",
    "        \"Long sequences (>10K): Lower attention ratio (20-40%) for efficiency\",\n",
    "        \"Very long sequences (>100K): Consider pure convolution or sliding windows\"\n",
    "    ],\n",
    "    \"Genomic Task Optimization\": [\n",
    "        \"Classification: Higher attention ratio for global sequence understanding\",\n",
    "        \"Local motif detection: Higher convolution ratio with multiple kernel sizes\",\n",
    "        \"Sequence generation: Balanced ratio with autoregressive masking\",\n",
    "        \"Protein folding: Custom kernels matching structural patterns\"\n",
    "    ],\n",
    "    \"Performance Tuning\": [\n",
    "        \"Use gradient checkpointing for very long sequences\",\n",
    "        \"Implement mixed precision training for memory efficiency\",\n",
    "        \"Consider model parallelism for large models\",\n",
    "        \"Profile attention vs convolution ratios for your specific task\"\n",
    "    ],\n",
    "    \"Implementation Best Practices\": [\n",
    "        \"Start with 50% attention ratio and tune based on validation performance\",\n",
    "        \"Use multiple convolution kernel sizes for genomic pattern diversity\",\n",
    "        \"Implement proper normalization between attention and conv pathways\",\n",
    "        \"Monitor memory usage and adjust batch sizes accordingly\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in recommendations.items():\n",
    "    print(f\"**{category}:**\")\n",
    "    for practice in practices:\n",
    "        print(f\"  • {practice}\")\n",
    "    print()\n",
    "\n",
    "# Configuration templates\n",
    "print(\"=== Recommended Configurations ===\")\n",
    "print()\n",
    "\n",
    "configs = {\n",
    "    \"Small Model (Fast Inference)\": {\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 6,\n",
    "        'num_attention_heads': 4,\n",
    "        'attention_ratio': 0.5,\n",
    "        'max_sequence_length': 4096\n",
    "    },\n",
    "    \"Medium Model (Balanced)\": {\n",
    "        'hidden_size': 512,\n",
    "        'num_layers': 12,\n",
    "        'num_attention_heads': 8,\n",
    "        'attention_ratio': 0.4,\n",
    "        'max_sequence_length': 8192\n",
    "    },\n",
    "    \"Large Model (High Accuracy)\": {\n",
    "        'hidden_size': 1024,\n",
    "        'num_layers': 24,\n",
    "        'num_attention_heads': 16,\n",
    "        'attention_ratio': 0.3,\n",
    "        'max_sequence_length': 16384\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, config in configs.items():\n",
    "    print(f\"**{name}:**\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print()\n",
    "\n",
    "# Summary from this tutorial\n",
    "print(\"=== Tutorial Summary ===\")\n",
    "print(f\"Architecture components analyzed: {len(['Hyena Block', 'Attention', 'Convolution', 'Custom Layer'])}\")\n",
    "print(f\"Complexity measurements: {len(complexity_results)} sequence lengths tested\")\n",
    "print(f\"Attention ratios compared: {len(ratio_results)}\")\n",
    "print(\"Custom implementations: 1 genomic-optimized layer\")\n",
    "print(\"Visualization plots: 8 comprehensive analyses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c21cc3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial provided a comprehensive analysis of the Striped Hyena architecture. Key insights:\n",
    "\n",
    "1. **Hybrid Architecture**: Combines the efficiency of convolution with the expressiveness of attention\n",
    "2. **Scalable Complexity**: Achieves better-than-quadratic scaling for long sequences\n",
    "3. **Configurable Trade-offs**: Attention ratio can be tuned based on task requirements\n",
    "4. **Genomic Optimization**: Custom implementations can leverage domain-specific patterns\n",
    "5. **Performance Characteristics**: Memory and computational efficiency improve with lower attention ratios\n",
    "\n",
    "In the next tutorial, we'll explore model training strategies and optimization techniques for genomic tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
